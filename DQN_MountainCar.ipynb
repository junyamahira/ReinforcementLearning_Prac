{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_MountainCar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junyamahira/ReinforcementLearning_Prac/blob/master/DQN_MountainCar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BbHY0Gc_tdz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# keras-rlのインストール"
      ]
    },
    {
      "metadata": {
        "id": "ap27433nXCn9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## git clone \n",
        "- これかpipのどちらかを実行すればよい (pipがおすすめ)\n",
        "- keras-rlの中に入り、setup.pyを実行する"
      ]
    },
    {
      "metadata": {
        "id": "xLXt4VtkXLwh",
        "colab_type": "code",
        "outputId": "5d6a19d9-77e5-43b4-e02b-b0cbb9534dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/keras-rl/keras-rl.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-rl'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects:   8% (1/12)   \u001b[K\rremote: Counting objects:  16% (2/12)   \u001b[K\rremote: Counting objects:  25% (3/12)   \u001b[K\rremote: Counting objects:  33% (4/12)   \u001b[K\rremote: Counting objects:  41% (5/12)   \u001b[K\rremote: Counting objects:  50% (6/12)   \u001b[K\rremote: Counting objects:  58% (7/12)   \u001b[K\rremote: Counting objects:  66% (8/12)   \u001b[K\rremote: Counting objects:  75% (9/12)   \u001b[K\rremote: Counting objects:  83% (10/12)   \u001b[K\rremote: Counting objects:  91% (11/12)   \u001b[K\rremote: Counting objects: 100% (12/12)   \u001b[K\rremote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 1710 (delta 3), reused 7 (delta 2), pack-reused 1698\u001b[K\n",
            "Receiving objects: 100% (1710/1710), 1.38 MiB | 22.44 MiB/s, done.\n",
            "Resolving deltas: 100% (1058/1058), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IaD-jZAAXa8S",
        "colab_type": "code",
        "outputId": "92ca32d0-eb5f-4124-f1d2-509fc1d6b118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!ls keras-rl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assets\t\t examples\t    mkdocs.yml\trl\t   tests\n",
            "CONTRIBUTING.md  ISSUE_TEMPLATE.md  pytest.ini\tsetup.cfg  utils\n",
            "docs\t\t LICENSE\t    README.md\tsetup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8cw_CkyZFsd",
        "colab_type": "code",
        "outputId": "be46aaca-71ad-4f27-c88e-7971eeb08fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!cd keras-rl\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keras-rl  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-_L-L0I2a-uQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## pip"
      ]
    },
    {
      "metadata": {
        "id": "DUyHjkEPbBtS",
        "colab_type": "code",
        "outputId": "8e55ace9-99ef-4c54-c165-88282c0b0a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "pip install keras-rl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JUcjN-baSzPe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# その他のimport"
      ]
    },
    {
      "metadata": {
        "id": "ZG-NDV4AQIFS",
        "colab_type": "code",
        "outputId": "50fa3fe7-9f73-4689-a994-16efa2dd4506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "K9I5EaaMHWYK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visiual display"
      ]
    },
    {
      "metadata": {
        "id": "qxGA6tFdNNEa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#?\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdpKHXZ8HVee",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBv7HoHzHXJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#　"
      ]
    },
    {
      "metadata": {
        "id": "DGIdiFuXT1rk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MountainCar-v0\")\n",
        "env = wrappers.Monitor(env, './', force=True)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xsV75LWXtMLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# モデル"
      ]
    },
    {
      "metadata": {
        "id": "vFGewr_orPkl",
        "colab_type": "code",
        "outputId": "65963c4a-59ec-43f8-90fd-2596fe0969ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oFQDZxpvf9PJ",
        "colab_type": "code",
        "outputId": "79aa6ca0-06d5-4148-da4a-9dc8aeaa509b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 643\n",
            "Trainable params: 643\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F_kWc71Kse2-",
        "colab_type": "code",
        "outputId": "c7ddbd24-5992-4f73-a6f6-a572a0938e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5923
        }
      },
      "cell_type": "code",
      "source": [
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "# 方策\n",
        "policy = EpsGreedyQPolicy(eps=0.001)\n",
        "#　各パラメータをここで調整\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions,gamma=0.99, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "# 何設定してん？\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   173/50000: episode: 1, duration: 4.004s, episode steps: 173, steps per second: 43, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.537], loss: 0.174112, mean_absolute_error: 0.700445, mean_q: -0.689521\n",
            "   373/50000: episode: 2, duration: 3.053s, episode steps: 200, steps per second: 66, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.243 [-0.679, 0.025], loss: 0.004197, mean_absolute_error: 1.543724, mean_q: -2.184135\n",
            "   573/50000: episode: 3, duration: 0.694s, episode steps: 200, steps per second: 288, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.725 [0.000, 2.000], mean observation: -0.213 [-0.545, 0.013], loss: 0.011902, mean_absolute_error: 2.517416, mean_q: -3.696577\n",
            "   773/50000: episode: 4, duration: 0.623s, episode steps: 200, steps per second: 321, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 2.000], mean observation: -0.229 [-0.598, 0.017], loss: 0.046947, mean_absolute_error: 3.628202, mean_q: -5.334420\n",
            "   973/50000: episode: 5, duration: 0.600s, episode steps: 200, steps per second: 333, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.243 [-0.808, 0.025], loss: 0.071740, mean_absolute_error: 4.753088, mean_q: -7.013009\n",
            "  1173/50000: episode: 6, duration: 0.602s, episode steps: 200, steps per second: 332, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000], mean observation: -0.231 [-0.764, 0.026], loss: 0.151314, mean_absolute_error: 5.890877, mean_q: -8.690437\n",
            "  1373/50000: episode: 7, duration: 0.612s, episode steps: 200, steps per second: 327, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.254 [-0.824, 0.037], loss: 0.192305, mean_absolute_error: 7.003433, mean_q: -10.334580\n",
            "  1542/50000: episode: 8, duration: 0.513s, episode steps: 169, steps per second: 329, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.544 [0.000, 2.000], mean observation: -0.144 [-0.855, 0.501], loss: 0.337016, mean_absolute_error: 7.995023, mean_q: -11.782211\n",
            "  1742/50000: episode: 9, duration: 2.869s, episode steps: 200, steps per second: 70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.245 [-0.824, 0.020], loss: 0.386769, mean_absolute_error: 8.923615, mean_q: -13.135276\n",
            "  1942/50000: episode: 10, duration: 0.700s, episode steps: 200, steps per second: 286, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.800 [0.000, 2.000], mean observation: -0.182 [-0.852, 0.240], loss: 0.252910, mean_absolute_error: 9.958230, mean_q: -14.743328\n",
            "  2142/50000: episode: 11, duration: 0.608s, episode steps: 200, steps per second: 329, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.715 [0.000, 2.000], mean observation: -0.203 [-0.728, 0.029], loss: 0.490682, mean_absolute_error: 10.919448, mean_q: -16.129978\n",
            "  2342/50000: episode: 12, duration: 0.607s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.247 [-0.939, 0.012], loss: 0.525436, mean_absolute_error: 11.918650, mean_q: -17.617622\n",
            "  2542/50000: episode: 13, duration: 0.619s, episode steps: 200, steps per second: 323, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 2.000], mean observation: -0.231 [-0.881, 0.041], loss: 0.599427, mean_absolute_error: 12.907419, mean_q: -19.069649\n",
            "  2742/50000: episode: 14, duration: 0.614s, episode steps: 200, steps per second: 326, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.775 [0.000, 2.000], mean observation: -0.169 [-0.895, 0.429], loss: 0.816032, mean_absolute_error: 13.834089, mean_q: -20.408808\n",
            "  2942/50000: episode: 15, duration: 0.605s, episode steps: 200, steps per second: 331, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.630 [0.000, 2.000], mean observation: -0.214 [-0.689, 0.015], loss: 0.796859, mean_absolute_error: 14.660967, mean_q: -21.673729\n",
            "  3142/50000: episode: 16, duration: 0.606s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.660 [0.000, 2.000], mean observation: -0.203 [-0.873, 0.023], loss: 0.685307, mean_absolute_error: 15.561733, mean_q: -23.035694\n",
            "  3342/50000: episode: 17, duration: 0.607s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.670 [0.000, 2.000], mean observation: -0.225 [-0.999, 0.042], loss: 1.110367, mean_absolute_error: 16.412630, mean_q: -24.255775\n",
            "  3542/50000: episode: 18, duration: 0.606s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.770 [0.000, 2.000], mean observation: -0.178 [-0.777, 0.120], loss: 1.400989, mean_absolute_error: 17.226204, mean_q: -25.435078\n",
            "  3742/50000: episode: 19, duration: 0.606s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.725 [0.000, 2.000], mean observation: -0.181 [-0.740, 0.050], loss: 1.949248, mean_absolute_error: 17.984491, mean_q: -26.484541\n",
            "  3942/50000: episode: 20, duration: 0.613s, episode steps: 200, steps per second: 326, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.660 [0.000, 2.000], mean observation: -0.213 [-0.660, 0.023], loss: 1.639318, mean_absolute_error: 18.635004, mean_q: -27.508781\n",
            "  4142/50000: episode: 21, duration: 0.604s, episode steps: 200, steps per second: 331, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.700 [0.000, 2.000], mean observation: -0.198 [-0.680, 0.025], loss: 1.379211, mean_absolute_error: 19.419588, mean_q: -28.750122\n",
            "  4342/50000: episode: 22, duration: 0.607s, episode steps: 200, steps per second: 329, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.550 [0.000, 2.000], mean observation: -0.222 [-0.812, 0.036], loss: 2.107300, mean_absolute_error: 20.176508, mean_q: -29.713484\n",
            "  4471/50000: episode: 23, duration: 0.394s, episode steps: 129, steps per second: 328, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.488 [0.000, 2.000], mean observation: -0.169 [-0.880, 0.503], loss: 2.547866, mean_absolute_error: 20.740469, mean_q: -30.616173\n",
            "  4671/50000: episode: 24, duration: 0.611s, episode steps: 200, steps per second: 327, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.485 [0.000, 2.000], mean observation: -0.209 [-0.859, 0.136], loss: 1.897632, mean_absolute_error: 21.217501, mean_q: -31.392656\n",
            "  4871/50000: episode: 25, duration: 0.611s, episode steps: 200, steps per second: 327, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.253 [-0.834, 0.031], loss: 1.935303, mean_absolute_error: 21.908089, mean_q: -32.423141\n",
            "  5071/50000: episode: 26, duration: 0.612s, episode steps: 200, steps per second: 327, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.630 [0.000, 2.000], mean observation: -0.209 [-0.697, 0.026], loss: 1.890050, mean_absolute_error: 22.606550, mean_q: -33.466648\n",
            "  5271/50000: episode: 27, duration: 0.613s, episode steps: 200, steps per second: 326, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.585 [0.000, 2.000], mean observation: -0.193 [-1.006, 0.233], loss: 2.528238, mean_absolute_error: 23.232450, mean_q: -34.383282\n",
            "  5389/50000: episode: 28, duration: 1.802s, episode steps: 118, steps per second: 65, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.602 [0.000, 2.000], mean observation: -0.186 [-0.951, 0.509], loss: 2.427931, mean_absolute_error: 23.686249, mean_q: -35.086277\n",
            "  5574/50000: episode: 29, duration: 0.655s, episode steps: 185, steps per second: 282, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.578 [0.000, 2.000], mean observation: -0.192 [-0.912, 0.513], loss: 2.682391, mean_absolute_error: 24.041441, mean_q: -35.535995\n",
            "  5774/50000: episode: 30, duration: 0.606s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.550 [0.000, 2.000], mean observation: -0.225 [-0.941, 0.208], loss: 3.068554, mean_absolute_error: 24.644861, mean_q: -36.388443\n",
            "  5974/50000: episode: 31, duration: 0.609s, episode steps: 200, steps per second: 328, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 2.000], mean observation: -0.227 [-0.734, 0.025], loss: 3.046602, mean_absolute_error: 25.236206, mean_q: -37.326553\n",
            "  6174/50000: episode: 32, duration: 0.611s, episode steps: 200, steps per second: 327, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.550 [0.000, 2.000], mean observation: -0.193 [-0.903, 0.107], loss: 2.487781, mean_absolute_error: 25.769468, mean_q: -38.170948\n",
            "  6374/50000: episode: 33, duration: 0.609s, episode steps: 200, steps per second: 328, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 2.000], mean observation: -0.206 [-0.739, 0.026], loss: 3.367906, mean_absolute_error: 26.264437, mean_q: -38.863476\n",
            "  6574/50000: episode: 34, duration: 0.601s, episode steps: 200, steps per second: 333, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000], mean observation: -0.251 [-0.785, 0.029], loss: 3.726951, mean_absolute_error: 26.806650, mean_q: -39.635654\n",
            "  6774/50000: episode: 35, duration: 0.608s, episode steps: 200, steps per second: 329, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.640 [1.000, 2.000], mean observation: -0.191 [-0.796, 0.161], loss: 3.371042, mean_absolute_error: 27.270557, mean_q: -40.364510\n",
            "  6974/50000: episode: 36, duration: 0.607s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.435 [0.000, 2.000], mean observation: -0.231 [-0.694, 0.015], loss: 3.051936, mean_absolute_error: 27.792330, mean_q: -41.115498\n",
            "  7174/50000: episode: 37, duration: 0.614s, episode steps: 200, steps per second: 326, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.247 [-0.666, 0.021], loss: 3.286756, mean_absolute_error: 28.246655, mean_q: -41.789154\n",
            "  7374/50000: episode: 38, duration: 0.610s, episode steps: 200, steps per second: 328, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.850 [0.000, 2.000], mean observation: -0.197 [-0.661, 0.023], loss: 2.990859, mean_absolute_error: 28.757710, mean_q: -42.602306\n",
            "  7558/50000: episode: 39, duration: 0.576s, episode steps: 184, steps per second: 320, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.533 [0.000, 2.000], mean observation: -0.224 [-1.093, 0.506], loss: 3.378255, mean_absolute_error: 29.252039, mean_q: -43.280643\n",
            "  7758/50000: episode: 40, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.202 [-0.854, 0.084], loss: 3.910532, mean_absolute_error: 29.601641, mean_q: -43.781971\n",
            "  7958/50000: episode: 41, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.875 [0.000, 2.000], mean observation: -0.183 [-0.654, 0.023], loss: 3.191492, mean_absolute_error: 29.958818, mean_q: -44.335243\n",
            "  8158/50000: episode: 42, duration: 0.679s, episode steps: 200, steps per second: 294, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000], mean observation: -0.220 [-1.101, 0.138], loss: 4.964063, mean_absolute_error: 30.448130, mean_q: -44.942909\n",
            "  8358/50000: episode: 43, duration: 0.678s, episode steps: 200, steps per second: 295, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.735 [0.000, 2.000], mean observation: -0.211 [-0.655, 0.023], loss: 4.975868, mean_absolute_error: 30.785255, mean_q: -45.472431\n",
            "  8558/50000: episode: 44, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.224 [-0.842, 0.152], loss: 3.512646, mean_absolute_error: 31.131163, mean_q: -46.160046\n",
            "  8758/50000: episode: 45, duration: 0.699s, episode steps: 200, steps per second: 286, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000], mean observation: -0.224 [-0.688, 0.023], loss: 3.412370, mean_absolute_error: 31.534327, mean_q: -46.754040\n",
            "  8958/50000: episode: 46, duration: 0.713s, episode steps: 200, steps per second: 280, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 2.000], mean observation: -0.226 [-0.652, 0.016], loss: 5.582429, mean_absolute_error: 31.904980, mean_q: -47.100761\n",
            "  9158/50000: episode: 47, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.570 [0.000, 2.000], mean observation: -0.198 [-0.796, 0.049], loss: 5.084574, mean_absolute_error: 32.200184, mean_q: -47.627430\n",
            "  9298/50000: episode: 48, duration: 0.419s, episode steps: 140, steps per second: 334, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.571 [0.000, 2.000], mean observation: -0.190 [-0.932, 0.507], loss: 4.032854, mean_absolute_error: 32.560310, mean_q: -48.207867\n",
            "  9498/50000: episode: 49, duration: 0.598s, episode steps: 200, steps per second: 335, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000], mean observation: -0.227 [-0.672, 0.024], loss: 4.252864, mean_absolute_error: 32.734951, mean_q: -48.489403\n",
            "  9698/50000: episode: 50, duration: 0.608s, episode steps: 200, steps per second: 329, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 2.000], mean observation: -0.169 [-1.200, 0.310], loss: 4.381291, mean_absolute_error: 33.020973, mean_q: -48.864105\n",
            "  9824/50000: episode: 51, duration: 0.374s, episode steps: 126, steps per second: 337, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.579 [0.000, 2.000], mean observation: -0.155 [-0.875, 0.513], loss: 3.084425, mean_absolute_error: 33.293736, mean_q: -49.334202\n",
            " 10024/50000: episode: 52, duration: 0.595s, episode steps: 200, steps per second: 336, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.228 [-0.702, 0.023], loss: 4.875120, mean_absolute_error: 33.503750, mean_q: -49.555408\n",
            " 10224/50000: episode: 53, duration: 0.593s, episode steps: 200, steps per second: 337, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000], mean observation: -0.265 [-0.691, 0.020], loss: 4.378889, mean_absolute_error: 33.865299, mean_q: -50.159267\n",
            " 10424/50000: episode: 54, duration: 0.593s, episode steps: 200, steps per second: 337, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.052], loss: 4.671082, mean_absolute_error: 34.165913, mean_q: -50.586349\n",
            " 10624/50000: episode: 55, duration: 0.591s, episode steps: 200, steps per second: 339, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000], mean observation: -0.216 [-1.002, 0.506], loss: 3.389343, mean_absolute_error: 34.475243, mean_q: -51.124435\n",
            " 10824/50000: episode: 56, duration: 0.593s, episode steps: 200, steps per second: 337, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000], mean observation: -0.281 [-1.200, 0.028], loss: 5.368696, mean_absolute_error: 34.617416, mean_q: -51.209591\n",
            " 11024/50000: episode: 57, duration: 0.590s, episode steps: 200, steps per second: 339, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.262 [-0.890, 0.022], loss: 6.643625, mean_absolute_error: 34.799736, mean_q: -51.409286\n",
            " 11144/50000: episode: 58, duration: 0.361s, episode steps: 120, steps per second: 333, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.220 [-1.019, 0.504], loss: 5.119659, mean_absolute_error: 35.173157, mean_q: -51.990910\n",
            " 11344/50000: episode: 59, duration: 0.599s, episode steps: 200, steps per second: 334, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.540 [0.000, 2.000], mean observation: -0.220 [-0.698, 0.023], loss: 4.490513, mean_absolute_error: 35.122192, mean_q: -51.991611\n",
            " 11544/50000: episode: 60, duration: 0.597s, episode steps: 200, steps per second: 335, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.263 [-1.200, 0.056], loss: 5.786036, mean_absolute_error: 35.377689, mean_q: -52.300125\n",
            " 11744/50000: episode: 61, duration: 0.594s, episode steps: 200, steps per second: 337, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.620 [0.000, 2.000], mean observation: -0.201 [-0.748, 0.025], loss: 5.700977, mean_absolute_error: 35.668793, mean_q: -52.766064\n",
            " 11944/50000: episode: 62, duration: 0.594s, episode steps: 200, steps per second: 337, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000], mean observation: -0.203 [-0.935, 0.033], loss: 6.176947, mean_absolute_error: 35.836349, mean_q: -53.001064\n",
            " 12144/50000: episode: 63, duration: 0.597s, episode steps: 200, steps per second: 335, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000], mean observation: -0.235 [-0.772, 0.027], loss: 5.512874, mean_absolute_error: 36.008705, mean_q: -53.301952\n",
            " 12344/50000: episode: 64, duration: 0.598s, episode steps: 200, steps per second: 334, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000], mean observation: -0.284 [-1.083, 0.029], loss: 6.320667, mean_absolute_error: 36.115265, mean_q: -53.352371\n",
            " 12544/50000: episode: 65, duration: 3.001s, episode steps: 200, steps per second: 67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.740 [0.000, 2.000], mean observation: -0.293 [-1.180, 0.274], loss: 4.159672, mean_absolute_error: 36.230358, mean_q: -53.656406\n",
            " 12744/50000: episode: 66, duration: 0.699s, episode steps: 200, steps per second: 286, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000], mean observation: -0.273 [-1.200, 0.051], loss: 5.405105, mean_absolute_error: 36.309010, mean_q: -53.709297\n",
            " 12925/50000: episode: 67, duration: 0.544s, episode steps: 181, steps per second: 333, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.431 [0.000, 2.000], mean observation: -0.213 [-1.200, 0.537], loss: 6.768487, mean_absolute_error: 36.700249, mean_q: -54.244274\n",
            " 13120/50000: episode: 68, duration: 0.591s, episode steps: 195, steps per second: 330, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.220 [-1.200, 0.521], loss: 6.175107, mean_absolute_error: 36.748005, mean_q: -54.318394\n",
            " 13284/50000: episode: 69, duration: 0.500s, episode steps: 164, steps per second: 328, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.248 [-1.200, 0.531], loss: 5.028677, mean_absolute_error: 36.798225, mean_q: -54.429188\n",
            " 13484/50000: episode: 70, duration: 0.607s, episode steps: 200, steps per second: 329, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000], mean observation: -0.156 [-0.852, 0.504], loss: 6.781004, mean_absolute_error: 36.975235, mean_q: -54.550957\n",
            " 13684/50000: episode: 71, duration: 0.633s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.101], loss: 6.942064, mean_absolute_error: 36.996346, mean_q: -54.562950\n",
            " 13862/50000: episode: 72, duration: 0.537s, episode steps: 178, steps per second: 331, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.011 [0.000, 2.000], mean observation: -0.229 [-1.200, 0.504], loss: 2.569974, mean_absolute_error: 37.036438, mean_q: -54.882496\n",
            " 13964/50000: episode: 73, duration: 0.314s, episode steps: 102, steps per second: 325, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.078 [0.000, 2.000], mean observation: -0.177 [-0.898, 0.515], loss: 3.959965, mean_absolute_error: 37.054398, mean_q: -54.780052\n",
            " 14164/50000: episode: 74, duration: 0.601s, episode steps: 200, steps per second: 333, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000], mean observation: -0.274 [-0.881, 0.024], loss: 4.758986, mean_absolute_error: 37.219219, mean_q: -55.005314\n",
            " 14295/50000: episode: 75, duration: 0.403s, episode steps: 131, steps per second: 325, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.603 [0.000, 2.000], mean observation: -0.191 [-0.949, 0.506], loss: 7.458239, mean_absolute_error: 37.491615, mean_q: -55.305351\n",
            " 14495/50000: episode: 76, duration: 0.603s, episode steps: 200, steps per second: 332, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.211 [-1.200, 0.131], loss: 5.555298, mean_absolute_error: 37.265327, mean_q: -54.971561\n",
            " 14695/50000: episode: 77, duration: 0.598s, episode steps: 200, steps per second: 335, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000], mean observation: -0.289 [-1.042, 0.040], loss: 7.572034, mean_absolute_error: 37.411118, mean_q: -55.152637\n",
            " 14821/50000: episode: 78, duration: 0.382s, episode steps: 126, steps per second: 330, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.261 [-1.159, 0.537], loss: 5.758380, mean_absolute_error: 37.790268, mean_q: -55.787373\n",
            " 15021/50000: episode: 79, duration: 0.606s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000], mean observation: -0.230 [-0.953, 0.025], loss: 5.445400, mean_absolute_error: 37.737167, mean_q: -55.742344\n",
            " 15137/50000: episode: 80, duration: 0.352s, episode steps: 116, steps per second: 330, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.537], loss: 8.312512, mean_absolute_error: 37.838890, mean_q: -55.717651\n",
            " 15337/50000: episode: 81, duration: 0.597s, episode steps: 200, steps per second: 335, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000], mean observation: -0.243 [-0.955, 0.159], loss: 7.690539, mean_absolute_error: 37.595062, mean_q: -55.337372\n",
            " 15469/50000: episode: 82, duration: 0.397s, episode steps: 132, steps per second: 333, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.170 [-0.888, 0.510], loss: 4.449802, mean_absolute_error: 37.670395, mean_q: -55.632774\n",
            " 15654/50000: episode: 83, duration: 0.557s, episode steps: 185, steps per second: 332, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.171 [-1.200, 0.514], loss: 5.353302, mean_absolute_error: 37.859913, mean_q: -55.786118\n",
            " 15854/50000: episode: 84, duration: 0.614s, episode steps: 200, steps per second: 326, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000], mean observation: -0.261 [-0.860, 0.019], loss: 7.307652, mean_absolute_error: 37.801235, mean_q: -55.644787\n",
            " 15995/50000: episode: 85, duration: 0.426s, episode steps: 141, steps per second: 331, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000], mean observation: -0.257 [-1.200, 0.505], loss: 3.971171, mean_absolute_error: 37.916824, mean_q: -55.931053\n",
            " 16195/50000: episode: 86, duration: 0.604s, episode steps: 200, steps per second: 331, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.750 [0.000, 2.000], mean observation: -0.261 [-0.977, 0.326], loss: 7.128165, mean_absolute_error: 37.892830, mean_q: -55.803520\n",
            " 16380/50000: episode: 87, duration: 0.550s, episode steps: 185, steps per second: 336, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.543], loss: 6.245282, mean_absolute_error: 37.933224, mean_q: -55.846790\n",
            " 16488/50000: episode: 88, duration: 0.326s, episode steps: 108, steps per second: 332, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000], mean observation: -0.180 [-0.948, 0.501], loss: 4.274681, mean_absolute_error: 37.981010, mean_q: -56.016827\n",
            " 16600/50000: episode: 89, duration: 0.341s, episode steps: 112, steps per second: 329, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.098 [0.000, 2.000], mean observation: -0.226 [-0.984, 0.523], loss: 5.626141, mean_absolute_error: 38.027065, mean_q: -56.063843\n",
            " 16800/50000: episode: 90, duration: 0.604s, episode steps: 200, steps per second: 331, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.263 [-1.033, 0.047], loss: 7.471684, mean_absolute_error: 38.078297, mean_q: -55.956154\n",
            " 16988/50000: episode: 91, duration: 0.566s, episode steps: 188, steps per second: 332, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.548 [0.000, 2.000], mean observation: -0.197 [-1.200, 0.537], loss: 5.589637, mean_absolute_error: 38.151299, mean_q: -56.246738\n",
            " 17187/50000: episode: 92, duration: 0.598s, episode steps: 199, steps per second: 333, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.804 [0.000, 2.000], mean observation: -0.271 [-1.058, 0.532], loss: 6.593801, mean_absolute_error: 37.880482, mean_q: -55.731216\n",
            " 17373/50000: episode: 93, duration: 0.562s, episode steps: 186, steps per second: 331, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.212 [-1.200, 0.527], loss: 7.741624, mean_absolute_error: 37.891293, mean_q: -55.759094\n",
            " 17573/50000: episode: 94, duration: 0.596s, episode steps: 200, steps per second: 335, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 2.000], mean observation: -0.215 [-1.200, 0.024], loss: 4.409944, mean_absolute_error: 37.912891, mean_q: -55.904678\n",
            " 17718/50000: episode: 95, duration: 0.444s, episode steps: 145, steps per second: 327, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.532], loss: 7.697193, mean_absolute_error: 38.077145, mean_q: -56.002342\n",
            " 17882/50000: episode: 96, duration: 0.497s, episode steps: 164, steps per second: 330, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.341 [0.000, 2.000], mean observation: -0.221 [-1.061, 0.536], loss: 5.451212, mean_absolute_error: 37.849655, mean_q: -55.814453\n",
            " 18082/50000: episode: 97, duration: 0.608s, episode steps: 200, steps per second: 329, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000], mean observation: -0.167 [-1.200, 0.487], loss: 7.126290, mean_absolute_error: 37.886894, mean_q: -55.770519\n",
            " 18183/50000: episode: 98, duration: 0.316s, episode steps: 101, steps per second: 319, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000], mean observation: -0.205 [-0.940, 0.519], loss: 7.080234, mean_absolute_error: 37.786327, mean_q: -55.573780\n",
            " 18366/50000: episode: 99, duration: 0.570s, episode steps: 183, steps per second: 321, episode reward: -183.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.537], loss: 7.432330, mean_absolute_error: 37.717037, mean_q: -55.538006\n",
            " 18546/50000: episode: 100, duration: 0.569s, episode steps: 180, steps per second: 316, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000], mean observation: -0.150 [-1.200, 0.537], loss: 4.410413, mean_absolute_error: 37.638149, mean_q: -55.523914\n",
            " 18658/50000: episode: 101, duration: 0.344s, episode steps: 112, steps per second: 325, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.258 [-1.125, 0.533], loss: 4.789153, mean_absolute_error: 37.715034, mean_q: -55.634377\n",
            " 18751/50000: episode: 102, duration: 0.288s, episode steps: 93, steps per second: 323, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.182 [-0.890, 0.505], loss: 5.357396, mean_absolute_error: 37.571098, mean_q: -55.441628\n",
            " 18920/50000: episode: 103, duration: 0.515s, episode steps: 169, steps per second: 328, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.537], loss: 3.733965, mean_absolute_error: 37.557137, mean_q: -55.514481\n",
            " 19071/50000: episode: 104, duration: 0.453s, episode steps: 151, steps per second: 333, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000], mean observation: -0.249 [-1.142, 0.524], loss: 8.321772, mean_absolute_error: 37.311199, mean_q: -54.908772\n",
            " 19271/50000: episode: 105, duration: 0.605s, episode steps: 200, steps per second: 331, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.490 [0.000, 2.000], mean observation: -0.185 [-1.200, 0.297], loss: 6.789410, mean_absolute_error: 37.409760, mean_q: -55.151943\n",
            " 19429/50000: episode: 106, duration: 0.475s, episode steps: 158, steps per second: 332, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.230 [-1.200, 0.537], loss: 5.403694, mean_absolute_error: 37.441952, mean_q: -55.303463\n",
            " 19589/50000: episode: 107, duration: 0.492s, episode steps: 160, steps per second: 325, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.537], loss: 4.890769, mean_absolute_error: 37.371037, mean_q: -55.264519\n",
            " 19716/50000: episode: 108, duration: 0.388s, episode steps: 127, steps per second: 327, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000], mean observation: -0.228 [-0.981, 0.524], loss: 4.371248, mean_absolute_error: 37.446651, mean_q: -55.337578\n",
            " 19882/50000: episode: 109, duration: 0.503s, episode steps: 166, steps per second: 330, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000], mean observation: -0.237 [-1.177, 0.511], loss: 5.571266, mean_absolute_error: 37.092525, mean_q: -54.824265\n",
            " 20054/50000: episode: 110, duration: 0.516s, episode steps: 172, steps per second: 333, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000], mean observation: -0.230 [-1.200, 0.537], loss: 3.767062, mean_absolute_error: 37.136978, mean_q: -54.901203\n",
            " 20199/50000: episode: 111, duration: 0.486s, episode steps: 145, steps per second: 299, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000], mean observation: -0.241 [-1.200, 0.537], loss: 5.039743, mean_absolute_error: 36.967087, mean_q: -54.620197\n",
            " 20300/50000: episode: 112, duration: 0.349s, episode steps: 101, steps per second: 289, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000], mean observation: -0.129 [-0.862, 0.513], loss: 2.512657, mean_absolute_error: 36.876854, mean_q: -54.628544\n",
            " 20500/50000: episode: 113, duration: 0.692s, episode steps: 200, steps per second: 289, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.550 [0.000, 2.000], mean observation: -0.254 [-1.200, 0.230], loss: 4.332255, mean_absolute_error: 37.093773, mean_q: -54.881413\n",
            " 20673/50000: episode: 114, duration: 0.595s, episode steps: 173, steps per second: 291, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.636 [0.000, 2.000], mean observation: -0.275 [-0.988, 0.515], loss: 6.643302, mean_absolute_error: 36.833958, mean_q: -54.316010\n",
            " 20873/50000: episode: 115, duration: 0.689s, episode steps: 200, steps per second: 290, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.725 [0.000, 2.000], mean observation: -0.216 [-1.200, 0.332], loss: 5.952458, mean_absolute_error: 36.797817, mean_q: -54.422794\n",
            " 20956/50000: episode: 116, duration: 0.288s, episode steps: 83, steps per second: 288, episode reward: -83.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.181 [0.000, 2.000], mean observation: -0.204 [-0.938, 0.507], loss: 5.222779, mean_absolute_error: 36.788387, mean_q: -54.334518\n",
            " 21041/50000: episode: 117, duration: 0.303s, episode steps: 85, steps per second: 281, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.224 [0.000, 2.000], mean observation: -0.188 [-0.917, 0.516], loss: 4.483483, mean_absolute_error: 36.659199, mean_q: -54.184910\n",
            " 21187/50000: episode: 118, duration: 0.507s, episode steps: 146, steps per second: 288, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.932 [0.000, 2.000], mean observation: -0.238 [-0.939, 0.525], loss: 4.474499, mean_absolute_error: 36.424522, mean_q: -53.926136\n",
            " 21278/50000: episode: 119, duration: 0.324s, episode steps: 91, steps per second: 281, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.187 [-0.896, 0.501], loss: 5.034807, mean_absolute_error: 36.347515, mean_q: -53.736496\n",
            " 21478/50000: episode: 120, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.505 [0.000, 2.000], mean observation: -0.308 [-0.997, 0.134], loss: 6.139604, mean_absolute_error: 36.066696, mean_q: -53.124454\n",
            " 21678/50000: episode: 121, duration: 0.701s, episode steps: 200, steps per second: 285, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.205 [0.000, 2.000], mean observation: -0.332 [-0.921, 0.030], loss: 3.777910, mean_absolute_error: 36.024574, mean_q: -53.312393\n",
            " 21878/50000: episode: 122, duration: 0.696s, episode steps: 200, steps per second: 287, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.370 [0.000, 2.000], mean observation: -0.291 [-1.200, 0.119], loss: 4.178143, mean_absolute_error: 36.027897, mean_q: -53.272549\n",
            " 22041/50000: episode: 123, duration: 0.559s, episode steps: 163, steps per second: 292, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.537], loss: 6.137813, mean_absolute_error: 35.773361, mean_q: -52.851646\n",
            " 22241/50000: episode: 124, duration: 0.688s, episode steps: 200, steps per second: 291, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.195 [0.000, 1.000], mean observation: -0.315 [-0.986, 0.021], loss: 3.967224, mean_absolute_error: 35.694557, mean_q: -52.726307\n",
            " 22331/50000: episode: 125, duration: 0.309s, episode steps: 90, steps per second: 291, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.067 [0.000, 2.000], mean observation: -0.205 [-0.920, 0.514], loss: 7.717114, mean_absolute_error: 36.115971, mean_q: -53.178432\n",
            " 22423/50000: episode: 126, duration: 1.533s, episode steps: 92, steps per second: 60, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.196 [-0.907, 0.505], loss: 2.445627, mean_absolute_error: 35.763374, mean_q: -52.783173\n",
            " 22510/50000: episode: 127, duration: 0.409s, episode steps: 87, steps per second: 213, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.204 [-0.931, 0.503], loss: 5.021122, mean_absolute_error: 35.500492, mean_q: -52.418171\n",
            " 22656/50000: episode: 128, duration: 0.527s, episode steps: 146, steps per second: 277, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.774 [0.000, 2.000], mean observation: -0.275 [-1.040, 0.512], loss: 7.820190, mean_absolute_error: 35.799183, mean_q: -52.712856\n",
            " 22744/50000: episode: 129, duration: 0.286s, episode steps: 88, steps per second: 308, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.215 [-0.941, 0.520], loss: 5.104957, mean_absolute_error: 35.677151, mean_q: -52.669796\n",
            " 22891/50000: episode: 130, duration: 0.447s, episode steps: 147, steps per second: 329, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.061 [0.000, 2.000], mean observation: -0.251 [-1.051, 0.502], loss: 4.189425, mean_absolute_error: 35.691936, mean_q: -52.709267\n",
            " 23091/50000: episode: 131, duration: 0.597s, episode steps: 200, steps per second: 335, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.160 [0.000, 2.000], mean observation: -0.332 [-1.142, 0.022], loss: 3.875313, mean_absolute_error: 35.562832, mean_q: -52.500313\n",
            " 23291/50000: episode: 132, duration: 0.605s, episode steps: 200, steps per second: 331, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.070 [0.000, 1.000], mean observation: -0.332 [-0.743, 0.016], loss: 2.788495, mean_absolute_error: 35.451763, mean_q: -52.409256\n",
            " 23491/50000: episode: 133, duration: 0.604s, episode steps: 200, steps per second: 331, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.375 [0.000, 2.000], mean observation: -0.327 [-1.003, 0.048], loss: 6.055526, mean_absolute_error: 35.704689, mean_q: -52.632099\n",
            " 23691/50000: episode: 134, duration: 0.601s, episode steps: 200, steps per second: 333, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.745 [0.000, 2.000], mean observation: -0.296 [-1.057, 0.149], loss: 5.775032, mean_absolute_error: 35.450829, mean_q: -52.216503\n",
            " 23891/50000: episode: 135, duration: 0.603s, episode steps: 200, steps per second: 331, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.338 [-0.817, 0.012], loss: 3.234416, mean_absolute_error: 35.545647, mean_q: -52.496262\n",
            " 24091/50000: episode: 136, duration: 0.613s, episode steps: 200, steps per second: 326, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.600 [0.000, 2.000], mean observation: -0.288 [-0.831, 0.027], loss: 4.483412, mean_absolute_error: 35.446220, mean_q: -52.233604\n",
            " 24291/50000: episode: 137, duration: 0.609s, episode steps: 200, steps per second: 328, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.334 [-0.738, 0.006], loss: 3.679395, mean_absolute_error: 35.888172, mean_q: -52.974174\n",
            " 24408/50000: episode: 138, duration: 0.354s, episode steps: 117, steps per second: 331, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000], mean observation: -0.256 [-1.187, 0.506], loss: 6.012255, mean_absolute_error: 35.912289, mean_q: -52.922707\n",
            " 24512/50000: episode: 139, duration: 0.322s, episode steps: 104, steps per second: 323, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.154 [0.000, 2.000], mean observation: -0.224 [-0.965, 0.507], loss: 6.173503, mean_absolute_error: 35.605808, mean_q: -52.383053\n",
            " 24600/50000: episode: 140, duration: 0.269s, episode steps: 88, steps per second: 327, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.091 [0.000, 2.000], mean observation: -0.205 [-0.929, 0.525], loss: 6.717441, mean_absolute_error: 35.511307, mean_q: -52.361084\n",
            " 24707/50000: episode: 141, duration: 0.324s, episode steps: 107, steps per second: 331, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000], mean observation: -0.236 [-1.071, 0.503], loss: 3.019366, mean_absolute_error: 35.560131, mean_q: -52.462883\n",
            " 24870/50000: episode: 142, duration: 0.486s, episode steps: 163, steps per second: 335, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.203 [-1.200, 0.537], loss: 4.520889, mean_absolute_error: 35.603405, mean_q: -52.514626\n",
            " 24976/50000: episode: 143, duration: 0.324s, episode steps: 106, steps per second: 327, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.292 [0.000, 2.000], mean observation: -0.128 [-0.869, 0.505], loss: 6.258744, mean_absolute_error: 35.321030, mean_q: -52.039822\n",
            " 25061/50000: episode: 144, duration: 0.261s, episode steps: 85, steps per second: 325, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000], mean observation: -0.176 [-0.903, 0.505], loss: 4.565044, mean_absolute_error: 35.806541, mean_q: -52.802715\n",
            " 25222/50000: episode: 145, duration: 0.487s, episode steps: 161, steps per second: 331, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.537], loss: 5.215185, mean_absolute_error: 35.559647, mean_q: -52.456509\n",
            " 25367/50000: episode: 146, duration: 0.441s, episode steps: 145, steps per second: 328, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000], mean observation: -0.247 [-1.200, 0.537], loss: 5.026359, mean_absolute_error: 35.632381, mean_q: -52.502056\n",
            " 25455/50000: episode: 147, duration: 0.264s, episode steps: 88, steps per second: 333, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.273 [0.000, 2.000], mean observation: -0.169 [-0.892, 0.510], loss: 4.053595, mean_absolute_error: 35.675537, mean_q: -52.633656\n",
            " 25580/50000: episode: 148, duration: 0.378s, episode steps: 125, steps per second: 331, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.664 [0.000, 2.000], mean observation: -0.204 [-1.049, 0.517], loss: 5.718432, mean_absolute_error: 35.319855, mean_q: -51.950500\n",
            " 25762/50000: episode: 149, duration: 0.556s, episode steps: 182, steps per second: 327, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000], mean observation: -0.233 [-1.200, 0.537], loss: 2.065343, mean_absolute_error: 35.512482, mean_q: -52.524384\n",
            " 25875/50000: episode: 150, duration: 0.342s, episode steps: 113, steps per second: 330, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.211 [-0.993, 0.523], loss: 5.765457, mean_absolute_error: 35.759171, mean_q: -52.684834\n",
            " 26020/50000: episode: 151, duration: 0.449s, episode steps: 145, steps per second: 323, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000], mean observation: -0.252 [-1.195, 0.506], loss: 5.809050, mean_absolute_error: 35.489655, mean_q: -52.290962\n",
            " 26202/50000: episode: 152, duration: 0.569s, episode steps: 182, steps per second: 320, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.245 [-1.200, 0.537], loss: 3.944117, mean_absolute_error: 35.432919, mean_q: -52.291634\n",
            " 26375/50000: episode: 153, duration: 0.542s, episode steps: 173, steps per second: 319, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.329 [0.000, 2.000], mean observation: -0.239 [-1.085, 0.511], loss: 3.241985, mean_absolute_error: 35.756413, mean_q: -52.780117\n",
            " 26538/50000: episode: 154, duration: 0.507s, episode steps: 163, steps per second: 321, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000], mean observation: -0.168 [-1.200, 0.537], loss: 4.228080, mean_absolute_error: 35.479378, mean_q: -52.288643\n",
            " 26714/50000: episode: 155, duration: 0.538s, episode steps: 176, steps per second: 327, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000], mean observation: -0.185 [-1.200, 0.537], loss: 3.765454, mean_absolute_error: 35.427158, mean_q: -52.222347\n",
            " 26810/50000: episode: 156, duration: 0.308s, episode steps: 96, steps per second: 312, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.208 [0.000, 2.000], mean observation: -0.162 [-0.877, 0.517], loss: 3.016130, mean_absolute_error: 35.374981, mean_q: -52.275402\n",
            " 27010/50000: episode: 157, duration: 0.642s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.537], loss: 3.932024, mean_absolute_error: 35.160839, mean_q: -51.871845\n",
            " 27125/50000: episode: 158, duration: 0.364s, episode steps: 115, steps per second: 316, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.461 [0.000, 2.000], mean observation: -0.242 [-1.200, 0.537], loss: 5.787102, mean_absolute_error: 35.276222, mean_q: -51.996281\n",
            " 27229/50000: episode: 159, duration: 0.325s, episode steps: 104, steps per second: 320, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.481 [0.000, 2.000], mean observation: -0.156 [-0.880, 0.505], loss: 3.567249, mean_absolute_error: 35.444130, mean_q: -52.350342\n",
            " 27363/50000: episode: 160, duration: 0.407s, episode steps: 134, steps per second: 329, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.269 [0.000, 2.000], mean observation: -0.229 [-1.200, 0.537], loss: 3.795226, mean_absolute_error: 35.385509, mean_q: -52.279694\n",
            " 27512/50000: episode: 161, duration: 0.455s, episode steps: 149, steps per second: 328, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.260 [-1.185, 0.533], loss: 3.867756, mean_absolute_error: 35.022659, mean_q: -51.573097\n",
            " 27712/50000: episode: 162, duration: 0.613s, episode steps: 200, steps per second: 326, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000], mean observation: -0.220 [-1.200, 0.230], loss: 3.615355, mean_absolute_error: 35.217770, mean_q: -51.957069\n",
            " 27840/50000: episode: 163, duration: 0.392s, episode steps: 128, steps per second: 326, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.156 [0.000, 2.000], mean observation: -0.265 [-1.174, 0.541], loss: 4.390257, mean_absolute_error: 35.057533, mean_q: -51.628216\n",
            " 28000/50000: episode: 164, duration: 0.483s, episode steps: 160, steps per second: 331, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000], mean observation: -0.192 [-1.200, 0.537], loss: 3.462721, mean_absolute_error: 35.130428, mean_q: -51.833549\n",
            " 28172/50000: episode: 165, duration: 0.524s, episode steps: 172, steps per second: 328, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.302 [0.000, 2.000], mean observation: -0.178 [-1.200, 0.537], loss: 4.811003, mean_absolute_error: 35.214645, mean_q: -51.812096\n",
            " 28363/50000: episode: 166, duration: 0.574s, episode steps: 191, steps per second: 333, episode reward: -191.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.356 [0.000, 2.000], mean observation: -0.153 [-1.200, 0.537], loss: 3.896299, mean_absolute_error: 34.984585, mean_q: -51.590832\n",
            " 28472/50000: episode: 167, duration: 0.338s, episode steps: 109, steps per second: 323, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.468 [0.000, 2.000], mean observation: -0.090 [-0.852, 0.500], loss: 3.828811, mean_absolute_error: 34.981182, mean_q: -51.539555\n",
            " 28594/50000: episode: 168, duration: 0.368s, episode steps: 122, steps per second: 331, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.131 [0.000, 2.000], mean observation: -0.193 [-0.883, 0.503], loss: 3.558573, mean_absolute_error: 35.194309, mean_q: -51.851830\n",
            " 28696/50000: episode: 169, duration: 0.315s, episode steps: 102, steps per second: 323, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000], mean observation: -0.208 [-0.953, 0.505], loss: 2.556495, mean_absolute_error: 35.054485, mean_q: -51.693260\n",
            " 28783/50000: episode: 170, duration: 0.270s, episode steps: 87, steps per second: 323, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.103 [0.000, 2.000], mean observation: -0.206 [-0.929, 0.514], loss: 3.153430, mean_absolute_error: 35.076332, mean_q: -51.764694\n",
            " 28977/50000: episode: 171, duration: 0.595s, episode steps: 194, steps per second: 326, episode reward: -194.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000], mean observation: -0.137 [-1.200, 0.537], loss: 3.317163, mean_absolute_error: 35.004833, mean_q: -51.540108\n",
            " 29081/50000: episode: 172, duration: 0.324s, episode steps: 104, steps per second: 321, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000], mean observation: -0.117 [-0.858, 0.512], loss: 5.798513, mean_absolute_error: 34.926579, mean_q: -51.262321\n",
            " 29277/50000: episode: 173, duration: 0.597s, episode steps: 196, steps per second: 328, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.388 [0.000, 2.000], mean observation: -0.133 [-1.200, 0.537], loss: 4.417398, mean_absolute_error: 34.724899, mean_q: -51.105797\n",
            " 29370/50000: episode: 174, duration: 0.288s, episode steps: 93, steps per second: 322, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000], mean observation: -0.205 [-0.917, 0.503], loss: 4.312228, mean_absolute_error: 34.602425, mean_q: -50.877594\n",
            " 29485/50000: episode: 175, duration: 0.357s, episode steps: 115, steps per second: 322, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.287 [0.000, 2.000], mean observation: -0.187 [-0.892, 0.504], loss: 2.594470, mean_absolute_error: 34.338280, mean_q: -50.589310\n",
            " 29629/50000: episode: 176, duration: 0.444s, episode steps: 144, steps per second: 324, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.299 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.537], loss: 4.848727, mean_absolute_error: 34.588726, mean_q: -50.857872\n",
            " 29785/50000: episode: 177, duration: 0.478s, episode steps: 156, steps per second: 326, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000], mean observation: -0.202 [-1.200, 0.537], loss: 4.681792, mean_absolute_error: 34.469681, mean_q: -50.701088\n",
            " 29917/50000: episode: 178, duration: 0.399s, episode steps: 132, steps per second: 330, episode reward: -132.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000], mean observation: -0.208 [-0.976, 0.514], loss: 5.997181, mean_absolute_error: 34.639233, mean_q: -50.909626\n",
            " 30072/50000: episode: 179, duration: 0.474s, episode steps: 155, steps per second: 327, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000], mean observation: -0.239 [-1.200, 0.537], loss: 5.312362, mean_absolute_error: 34.430618, mean_q: -50.715038\n",
            " 30197/50000: episode: 180, duration: 0.386s, episode steps: 125, steps per second: 324, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.416 [0.000, 2.000], mean observation: -0.076 [-0.849, 0.508], loss: 2.402682, mean_absolute_error: 34.364300, mean_q: -50.693859\n",
            " 30286/50000: episode: 181, duration: 0.273s, episode steps: 89, steps per second: 326, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.169 [-0.888, 0.501], loss: 1.649163, mean_absolute_error: 34.429577, mean_q: -50.773170\n",
            " 30436/50000: episode: 182, duration: 0.463s, episode steps: 150, steps per second: 324, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.255 [-1.200, 0.537], loss: 2.834408, mean_absolute_error: 34.286900, mean_q: -50.516075\n",
            " 30605/50000: episode: 183, duration: 0.518s, episode steps: 169, steps per second: 326, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.201 [0.000, 2.000], mean observation: -0.206 [-1.200, 0.537], loss: 2.779856, mean_absolute_error: 34.243336, mean_q: -50.440945\n",
            " 30765/50000: episode: 184, duration: 0.484s, episode steps: 160, steps per second: 331, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.181 [0.000, 2.000], mean observation: -0.214 [-1.200, 0.526], loss: 3.727014, mean_absolute_error: 34.122505, mean_q: -50.222000\n",
            " 30906/50000: episode: 185, duration: 0.430s, episode steps: 141, steps per second: 328, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000], mean observation: -0.227 [-1.200, 0.506], loss: 5.130784, mean_absolute_error: 34.155006, mean_q: -50.209251\n",
            " 31052/50000: episode: 186, duration: 0.444s, episode steps: 146, steps per second: 329, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000], mean observation: -0.207 [-1.200, 0.537], loss: 3.375357, mean_absolute_error: 33.778851, mean_q: -49.745049\n",
            " 31218/50000: episode: 187, duration: 0.507s, episode steps: 166, steps per second: 328, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.271 [0.000, 2.000], mean observation: -0.187 [-1.200, 0.537], loss: 2.731279, mean_absolute_error: 33.829075, mean_q: -49.894356\n",
            " 31372/50000: episode: 188, duration: 0.467s, episode steps: 154, steps per second: 330, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000], mean observation: -0.209 [-1.200, 0.537], loss: 3.634926, mean_absolute_error: 33.996372, mean_q: -50.058987\n",
            " 31527/50000: episode: 189, duration: 0.478s, episode steps: 155, steps per second: 325, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.228 [-1.200, 0.537], loss: 2.327073, mean_absolute_error: 33.810577, mean_q: -49.876659\n",
            " 31719/50000: episode: 190, duration: 0.584s, episode steps: 192, steps per second: 329, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.537], loss: 2.472217, mean_absolute_error: 33.963013, mean_q: -50.109711\n",
            " 31834/50000: episode: 191, duration: 0.353s, episode steps: 115, steps per second: 326, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.339 [0.000, 2.000], mean observation: -0.243 [-1.118, 0.542], loss: 4.000197, mean_absolute_error: 33.673588, mean_q: -49.556206\n",
            " 31983/50000: episode: 192, duration: 0.453s, episode steps: 149, steps per second: 329, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.201 [0.000, 2.000], mean observation: -0.236 [-1.200, 0.537], loss: 4.203672, mean_absolute_error: 33.933338, mean_q: -49.977966\n",
            " 32146/50000: episode: 193, duration: 0.500s, episode steps: 163, steps per second: 326, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.092 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.537], loss: 3.496339, mean_absolute_error: 33.536404, mean_q: -49.444042\n",
            " 32243/50000: episode: 194, duration: 0.297s, episode steps: 97, steps per second: 326, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.258 [0.000, 2.000], mean observation: -0.195 [-0.919, 0.509], loss: 1.983238, mean_absolute_error: 33.682575, mean_q: -49.775814\n",
            " 32388/50000: episode: 195, duration: 0.449s, episode steps: 145, steps per second: 323, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.537], loss: 3.752494, mean_absolute_error: 33.436897, mean_q: -49.333412\n",
            " 32492/50000: episode: 196, duration: 0.315s, episode steps: 104, steps per second: 330, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [0.000, 2.000], mean observation: -0.171 [-0.892, 0.517], loss: 2.272765, mean_absolute_error: 33.623070, mean_q: -49.591911\n",
            " 32626/50000: episode: 197, duration: 0.413s, episode steps: 134, steps per second: 324, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000], mean observation: -0.229 [-1.200, 0.537], loss: 4.267094, mean_absolute_error: 33.257446, mean_q: -48.974163\n",
            " 32769/50000: episode: 198, duration: 0.439s, episode steps: 143, steps per second: 326, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.343 [0.000, 2.000], mean observation: -0.247 [-1.126, 0.503], loss: 4.000687, mean_absolute_error: 33.482613, mean_q: -49.377209\n",
            " 32929/50000: episode: 199, duration: 0.484s, episode steps: 160, steps per second: 331, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.038 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.537], loss: 1.648033, mean_absolute_error: 33.476738, mean_q: -49.475609\n",
            " 33074/50000: episode: 200, duration: 0.446s, episode steps: 145, steps per second: 325, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000], mean observation: -0.216 [-1.200, 0.537], loss: 0.836845, mean_absolute_error: 33.430584, mean_q: -49.420246\n",
            " 33220/50000: episode: 201, duration: 0.445s, episode steps: 146, steps per second: 328, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000], mean observation: -0.212 [-1.200, 0.525], loss: 4.048890, mean_absolute_error: 33.238129, mean_q: -48.951538\n",
            " 33345/50000: episode: 202, duration: 0.383s, episode steps: 125, steps per second: 326, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.537], loss: 3.472080, mean_absolute_error: 33.467381, mean_q: -49.379227\n",
            " 33459/50000: episode: 203, duration: 0.362s, episode steps: 114, steps per second: 315, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000], mean observation: -0.222 [-0.976, 0.527], loss: 2.288355, mean_absolute_error: 33.239491, mean_q: -49.090778\n",
            " 33575/50000: episode: 204, duration: 0.354s, episode steps: 116, steps per second: 328, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000], mean observation: -0.222 [-0.992, 0.507], loss: 2.045802, mean_absolute_error: 33.599117, mean_q: -49.583584\n",
            " 33721/50000: episode: 205, duration: 0.445s, episode steps: 146, steps per second: 328, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000], mean observation: -0.215 [-1.200, 0.537], loss: 3.279670, mean_absolute_error: 33.553295, mean_q: -49.436466\n",
            " 33886/50000: episode: 206, duration: 0.504s, episode steps: 165, steps per second: 328, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000], mean observation: -0.215 [-1.200, 0.537], loss: 2.198082, mean_absolute_error: 33.254738, mean_q: -49.123150\n",
            " 34035/50000: episode: 207, duration: 0.452s, episode steps: 149, steps per second: 330, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.537], loss: 2.892000, mean_absolute_error: 33.374096, mean_q: -49.219261\n",
            " 34148/50000: episode: 208, duration: 0.351s, episode steps: 113, steps per second: 322, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.363 [0.000, 2.000], mean observation: -0.236 [-1.200, 0.527], loss: 2.415168, mean_absolute_error: 33.409885, mean_q: -49.269413\n",
            " 34283/50000: episode: 209, duration: 0.415s, episode steps: 135, steps per second: 326, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.037 [0.000, 2.000], mean observation: -0.245 [-0.999, 0.501], loss: 2.557437, mean_absolute_error: 33.252472, mean_q: -49.069103\n",
            " 34403/50000: episode: 210, duration: 0.367s, episode steps: 120, steps per second: 327, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.383 [0.000, 2.000], mean observation: -0.255 [-1.175, 0.507], loss: 2.426646, mean_absolute_error: 33.392025, mean_q: -49.118198\n",
            " 34551/50000: episode: 211, duration: 0.459s, episode steps: 148, steps per second: 323, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000], mean observation: -0.210 [-1.200, 0.537], loss: 4.654037, mean_absolute_error: 33.350521, mean_q: -49.101028\n",
            " 34701/50000: episode: 212, duration: 0.464s, episode steps: 150, steps per second: 324, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000], mean observation: -0.224 [-1.200, 0.537], loss: 0.993329, mean_absolute_error: 33.336029, mean_q: -49.270142\n",
            " 34852/50000: episode: 213, duration: 0.466s, episode steps: 151, steps per second: 324, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.172 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.537], loss: 3.194218, mean_absolute_error: 33.142971, mean_q: -48.868855\n",
            " 34981/50000: episode: 214, duration: 0.391s, episode steps: 129, steps per second: 330, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.519 [0.000, 2.000], mean observation: -0.230 [-1.118, 0.529], loss: 3.918130, mean_absolute_error: 33.316231, mean_q: -48.980145\n",
            " 35096/50000: episode: 215, duration: 0.360s, episode steps: 115, steps per second: 319, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.357 [0.000, 2.000], mean observation: -0.226 [-1.016, 0.516], loss: 4.161349, mean_absolute_error: 33.252354, mean_q: -48.860428\n",
            " 35254/50000: episode: 216, duration: 0.475s, episode steps: 158, steps per second: 332, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000], mean observation: -0.239 [-1.200, 0.536], loss: 2.924690, mean_absolute_error: 32.876209, mean_q: -48.469166\n",
            " 35360/50000: episode: 217, duration: 1.621s, episode steps: 106, steps per second: 65, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.028 [0.000, 2.000], mean observation: -0.182 [-0.933, 0.504], loss: 1.562111, mean_absolute_error: 32.997448, mean_q: -48.656715\n",
            " 35524/50000: episode: 218, duration: 0.595s, episode steps: 164, steps per second: 275, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.049 [0.000, 2.000], mean observation: -0.227 [-1.200, 0.537], loss: 3.469602, mean_absolute_error: 33.142746, mean_q: -48.667034\n",
            " 35616/50000: episode: 219, duration: 0.286s, episode steps: 92, steps per second: 322, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000], mean observation: -0.157 [-0.879, 0.510], loss: 2.437172, mean_absolute_error: 32.693806, mean_q: -48.224678\n",
            " 35758/50000: episode: 220, duration: 0.437s, episode steps: 142, steps per second: 325, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.537], loss: 1.914531, mean_absolute_error: 32.987244, mean_q: -48.668606\n",
            " 35871/50000: episode: 221, duration: 0.351s, episode steps: 113, steps per second: 322, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000], mean observation: -0.106 [-0.853, 0.505], loss: 2.138593, mean_absolute_error: 33.256065, mean_q: -49.074406\n",
            " 36035/50000: episode: 222, duration: 0.511s, episode steps: 164, steps per second: 321, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000], mean observation: -0.252 [-1.200, 0.537], loss: 4.801213, mean_absolute_error: 33.017738, mean_q: -48.464241\n",
            " 36173/50000: episode: 223, duration: 0.426s, episode steps: 138, steps per second: 324, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.537], loss: 2.286289, mean_absolute_error: 32.864529, mean_q: -48.440784\n",
            " 36372/50000: episode: 224, duration: 0.611s, episode steps: 199, steps per second: 326, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.814 [0.000, 2.000], mean observation: -0.273 [-1.200, 0.537], loss: 2.492647, mean_absolute_error: 32.884171, mean_q: -48.421543\n",
            " 36482/50000: episode: 225, duration: 0.347s, episode steps: 110, steps per second: 317, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.364 [0.000, 2.000], mean observation: -0.231 [-1.048, 0.518], loss: 2.467155, mean_absolute_error: 32.963657, mean_q: -48.494095\n",
            " 36581/50000: episode: 226, duration: 0.312s, episode steps: 99, steps per second: 317, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.212 [0.000, 2.000], mean observation: -0.195 [-0.909, 0.502], loss: 0.795461, mean_absolute_error: 32.594700, mean_q: -48.023102\n",
            " 36730/50000: episode: 227, duration: 0.473s, episode steps: 149, steps per second: 315, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.101 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.537], loss: 2.669431, mean_absolute_error: 33.067673, mean_q: -48.635452\n",
            " 36820/50000: episode: 228, duration: 0.300s, episode steps: 90, steps per second: 300, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.233 [0.000, 2.000], mean observation: -0.183 [-0.905, 0.512], loss: 2.620375, mean_absolute_error: 32.925613, mean_q: -48.421577\n",
            " 36938/50000: episode: 229, duration: 0.361s, episode steps: 118, steps per second: 327, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 2.000], mean observation: -0.239 [-1.200, 0.537], loss: 3.253675, mean_absolute_error: 33.157440, mean_q: -48.670788\n",
            " 37088/50000: episode: 230, duration: 0.466s, episode steps: 150, steps per second: 322, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.193 [0.000, 2.000], mean observation: -0.220 [-1.200, 0.537], loss: 2.690191, mean_absolute_error: 32.784725, mean_q: -48.183983\n",
            " 37238/50000: episode: 231, duration: 0.467s, episode steps: 150, steps per second: 321, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.227 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.537], loss: 2.155622, mean_absolute_error: 32.866512, mean_q: -48.310261\n",
            " 37404/50000: episode: 232, duration: 0.519s, episode steps: 166, steps per second: 320, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000], mean observation: -0.245 [-1.200, 0.537], loss: 3.395144, mean_absolute_error: 32.922485, mean_q: -48.352673\n",
            " 37552/50000: episode: 233, duration: 0.454s, episode steps: 148, steps per second: 326, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.939 [0.000, 2.000], mean observation: -0.272 [-1.119, 0.527], loss: 3.352793, mean_absolute_error: 32.623821, mean_q: -47.850574\n",
            " 37712/50000: episode: 234, duration: 0.489s, episode steps: 160, steps per second: 327, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000], mean observation: -0.177 [-1.200, 0.505], loss: 1.378690, mean_absolute_error: 32.594933, mean_q: -47.895706\n",
            " 37827/50000: episode: 235, duration: 0.358s, episode steps: 115, steps per second: 322, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.461 [0.000, 2.000], mean observation: -0.082 [-0.850, 0.500], loss: 1.407419, mean_absolute_error: 32.476273, mean_q: -47.700600\n",
            " 37986/50000: episode: 236, duration: 0.494s, episode steps: 159, steps per second: 322, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.176 [-1.200, 0.537], loss: 0.965173, mean_absolute_error: 32.875217, mean_q: -48.349129\n",
            " 38159/50000: episode: 237, duration: 0.530s, episode steps: 173, steps per second: 327, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000], mean observation: -0.191 [-1.200, 0.537], loss: 2.201546, mean_absolute_error: 32.497036, mean_q: -47.711243\n",
            " 38322/50000: episode: 238, duration: 0.508s, episode steps: 163, steps per second: 321, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.031 [0.000, 2.000], mean observation: -0.233 [-1.200, 0.537], loss: 1.666881, mean_absolute_error: 32.459087, mean_q: -47.648151\n",
            " 38484/50000: episode: 239, duration: 0.499s, episode steps: 162, steps per second: 325, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000], mean observation: -0.190 [-1.200, 0.503], loss: 3.238750, mean_absolute_error: 32.493862, mean_q: -47.627991\n",
            " 38578/50000: episode: 240, duration: 0.303s, episode steps: 94, steps per second: 311, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.309 [0.000, 2.000], mean observation: -0.150 [-0.874, 0.512], loss: 4.552489, mean_absolute_error: 32.556286, mean_q: -47.681198\n",
            " 38737/50000: episode: 241, duration: 0.485s, episode steps: 159, steps per second: 328, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.209 [-1.200, 0.537], loss: 2.074884, mean_absolute_error: 32.560234, mean_q: -47.818481\n",
            " 38891/50000: episode: 242, duration: 0.472s, episode steps: 154, steps per second: 327, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.209 [-1.200, 0.537], loss: 3.081275, mean_absolute_error: 32.361309, mean_q: -47.401196\n",
            " 39062/50000: episode: 243, duration: 0.526s, episode steps: 171, steps per second: 325, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000], mean observation: -0.257 [-1.200, 0.537], loss: 1.826027, mean_absolute_error: 32.423771, mean_q: -47.599239\n",
            " 39156/50000: episode: 244, duration: 0.291s, episode steps: 94, steps per second: 323, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.319 [0.000, 2.000], mean observation: -0.145 [-0.872, 0.512], loss: 1.939755, mean_absolute_error: 32.555977, mean_q: -47.793530\n",
            " 39294/50000: episode: 245, duration: 0.431s, episode steps: 138, steps per second: 320, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.362 [0.000, 2.000], mean observation: -0.231 [-1.200, 0.537], loss: 2.569033, mean_absolute_error: 32.634262, mean_q: -47.866119\n",
            " 39451/50000: episode: 246, duration: 0.490s, episode steps: 157, steps per second: 320, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.172 [0.000, 2.000], mean observation: -0.203 [-1.200, 0.537], loss: 0.484836, mean_absolute_error: 32.422562, mean_q: -47.708267\n",
            " 39594/50000: episode: 247, duration: 0.438s, episode steps: 143, steps per second: 327, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000], mean observation: -0.224 [-1.200, 0.537], loss: 2.539281, mean_absolute_error: 32.198013, mean_q: -47.257946\n",
            " 39681/50000: episode: 248, duration: 0.272s, episode steps: 87, steps per second: 320, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.162 [-0.891, 0.513], loss: 2.160449, mean_absolute_error: 32.228977, mean_q: -47.169991\n",
            " 39836/50000: episode: 249, duration: 0.473s, episode steps: 155, steps per second: 328, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.148 [0.000, 2.000], mean observation: -0.209 [-1.200, 0.537], loss: 2.266199, mean_absolute_error: 32.260414, mean_q: -47.369667\n",
            " 39985/50000: episode: 250, duration: 0.453s, episode steps: 149, steps per second: 329, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.322 [0.000, 2.000], mean observation: -0.225 [-1.007, 0.529], loss: 2.960972, mean_absolute_error: 32.000080, mean_q: -46.964306\n",
            " 40081/50000: episode: 251, duration: 0.302s, episode steps: 96, steps per second: 318, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.479 [0.000, 2.000], mean observation: -0.115 [-0.862, 0.513], loss: 2.042947, mean_absolute_error: 31.916945, mean_q: -46.845806\n",
            " 40186/50000: episode: 252, duration: 0.323s, episode steps: 105, steps per second: 325, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000], mean observation: -0.195 [-0.943, 0.526], loss: 2.420907, mean_absolute_error: 32.762806, mean_q: -48.064243\n",
            " 40301/50000: episode: 253, duration: 0.355s, episode steps: 115, steps per second: 324, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000], mean observation: -0.127 [-0.911, 0.502], loss: 2.262285, mean_absolute_error: 32.081001, mean_q: -47.108936\n",
            " 40386/50000: episode: 254, duration: 0.261s, episode steps: 85, steps per second: 326, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.192 [-0.921, 0.514], loss: 2.447387, mean_absolute_error: 32.162884, mean_q: -47.203655\n",
            " 40541/50000: episode: 255, duration: 0.483s, episode steps: 155, steps per second: 321, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.202 [-1.200, 0.542], loss: 1.993409, mean_absolute_error: 32.346981, mean_q: -47.560738\n",
            " 40680/50000: episode: 256, duration: 0.431s, episode steps: 139, steps per second: 323, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.266 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.537], loss: 2.146798, mean_absolute_error: 32.230171, mean_q: -47.341373\n",
            " 40826/50000: episode: 257, duration: 0.442s, episode steps: 146, steps per second: 330, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000], mean observation: -0.207 [-1.200, 0.537], loss: 1.745540, mean_absolute_error: 31.929306, mean_q: -46.902790\n",
            " 40964/50000: episode: 258, duration: 0.420s, episode steps: 138, steps per second: 328, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.348 [0.000, 2.000], mean observation: -0.228 [-1.200, 0.537], loss: 3.568502, mean_absolute_error: 32.013424, mean_q: -46.882038\n",
            " 41063/50000: episode: 259, duration: 0.316s, episode steps: 99, steps per second: 313, episode reward: -99.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.168 [-0.877, 0.508], loss: 0.854543, mean_absolute_error: 31.820070, mean_q: -46.838375\n",
            " 41197/50000: episode: 260, duration: 0.415s, episode steps: 134, steps per second: 323, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.313 [0.000, 2.000], mean observation: -0.233 [-1.200, 0.537], loss: 2.403310, mean_absolute_error: 31.880510, mean_q: -46.752697\n",
            " 41283/50000: episode: 261, duration: 0.270s, episode steps: 86, steps per second: 319, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000], mean observation: -0.192 [-0.913, 0.503], loss: 0.862459, mean_absolute_error: 32.256420, mean_q: -47.474201\n",
            " 41442/50000: episode: 262, duration: 0.497s, episode steps: 159, steps per second: 320, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.179 [-1.200, 0.537], loss: 1.131724, mean_absolute_error: 31.938488, mean_q: -46.990917\n",
            " 41591/50000: episode: 263, duration: 0.458s, episode steps: 149, steps per second: 326, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.233 [-1.200, 0.524], loss: 2.292506, mean_absolute_error: 32.223492, mean_q: -47.412098\n",
            " 41733/50000: episode: 264, duration: 0.448s, episode steps: 142, steps per second: 317, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.537], loss: 1.883603, mean_absolute_error: 32.358467, mean_q: -47.557961\n",
            " 41880/50000: episode: 265, duration: 0.454s, episode steps: 147, steps per second: 324, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.213 [-1.200, 0.517], loss: 1.196739, mean_absolute_error: 31.879713, mean_q: -46.869324\n",
            " 41969/50000: episode: 266, duration: 0.282s, episode steps: 89, steps per second: 316, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000], mean observation: -0.208 [-0.946, 0.519], loss: 1.857595, mean_absolute_error: 32.097565, mean_q: -47.060574\n",
            " 42107/50000: episode: 267, duration: 0.426s, episode steps: 138, steps per second: 324, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.537], loss: 1.267362, mean_absolute_error: 32.322929, mean_q: -47.554134\n",
            " 42241/50000: episode: 268, duration: 0.410s, episode steps: 134, steps per second: 327, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000], mean observation: -0.231 [-1.200, 0.537], loss: 3.462610, mean_absolute_error: 31.945721, mean_q: -46.837803\n",
            " 42392/50000: episode: 269, duration: 0.466s, episode steps: 151, steps per second: 324, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000], mean observation: -0.218 [-1.200, 0.537], loss: 1.296989, mean_absolute_error: 31.526218, mean_q: -46.278843\n",
            " 42557/50000: episode: 270, duration: 0.507s, episode steps: 165, steps per second: 326, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.157 [-1.200, 0.537], loss: 2.611662, mean_absolute_error: 31.921703, mean_q: -46.805256\n",
            " 42716/50000: episode: 271, duration: 0.486s, episode steps: 159, steps per second: 327, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.217 [-1.200, 0.537], loss: 2.348375, mean_absolute_error: 31.859863, mean_q: -46.814240\n",
            " 42874/50000: episode: 272, duration: 0.482s, episode steps: 158, steps per second: 328, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.987 [0.000, 2.000], mean observation: -0.244 [-1.200, 0.537], loss: 2.262012, mean_absolute_error: 31.871897, mean_q: -46.824577\n",
            " 42988/50000: episode: 273, duration: 0.354s, episode steps: 114, steps per second: 322, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.482 [0.000, 2.000], mean observation: -0.200 [-0.960, 0.517], loss: 2.765028, mean_absolute_error: 31.877319, mean_q: -46.833744\n",
            " 43175/50000: episode: 274, duration: 0.574s, episode steps: 187, steps per second: 326, episode reward: -187.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.172 [-1.200, 0.548], loss: 2.378898, mean_absolute_error: 32.049435, mean_q: -47.068768\n",
            " 43333/50000: episode: 275, duration: 0.486s, episode steps: 158, steps per second: 325, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.177 [0.000, 2.000], mean observation: -0.189 [-1.200, 0.537], loss: 2.756854, mean_absolute_error: 31.994226, mean_q: -47.007584\n",
            " 43485/50000: episode: 276, duration: 0.472s, episode steps: 152, steps per second: 322, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.209 [-1.200, 0.537], loss: 2.323402, mean_absolute_error: 32.104328, mean_q: -47.131840\n",
            " 43674/50000: episode: 277, duration: 0.585s, episode steps: 189, steps per second: 323, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.402 [0.000, 2.000], mean observation: -0.168 [-1.200, 0.537], loss: 2.298790, mean_absolute_error: 31.862558, mean_q: -46.652115\n",
            " 43820/50000: episode: 278, duration: 0.467s, episode steps: 146, steps per second: 313, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000], mean observation: -0.247 [-1.122, 0.505], loss: 2.872388, mean_absolute_error: 31.806702, mean_q: -46.604404\n",
            " 43957/50000: episode: 279, duration: 0.473s, episode steps: 137, steps per second: 290, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.372 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.537], loss: 2.632798, mean_absolute_error: 32.005932, mean_q: -46.883709\n",
            " 44101/50000: episode: 280, duration: 0.463s, episode steps: 144, steps per second: 311, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.236 [-1.200, 0.537], loss: 2.161915, mean_absolute_error: 31.730879, mean_q: -46.554092\n",
            " 44249/50000: episode: 281, duration: 0.462s, episode steps: 148, steps per second: 320, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.236 [-1.200, 0.537], loss: 1.864949, mean_absolute_error: 31.970440, mean_q: -46.893665\n",
            " 44372/50000: episode: 282, duration: 0.376s, episode steps: 123, steps per second: 327, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.455 [0.000, 2.000], mean observation: -0.219 [-1.037, 0.523], loss: 3.269805, mean_absolute_error: 31.866598, mean_q: -46.631950\n",
            " 44512/50000: episode: 283, duration: 0.433s, episode steps: 140, steps per second: 323, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000], mean observation: -0.230 [-1.200, 0.537], loss: 2.016473, mean_absolute_error: 32.037018, mean_q: -46.944233\n",
            " 44664/50000: episode: 284, duration: 0.466s, episode steps: 152, steps per second: 326, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.206 [-1.200, 0.537], loss: 2.564693, mean_absolute_error: 31.625790, mean_q: -46.310780\n",
            " 44753/50000: episode: 285, duration: 0.276s, episode steps: 89, steps per second: 323, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.303 [0.000, 2.000], mean observation: -0.161 [-0.887, 0.517], loss: 1.215647, mean_absolute_error: 31.755121, mean_q: -46.673992\n",
            " 44896/50000: episode: 286, duration: 0.439s, episode steps: 143, steps per second: 326, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000], mean observation: -0.227 [-1.200, 0.537], loss: 1.877105, mean_absolute_error: 31.614880, mean_q: -46.349983\n",
            " 45044/50000: episode: 287, duration: 0.461s, episode steps: 148, steps per second: 321, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000], mean observation: -0.210 [-1.200, 0.540], loss: 1.312834, mean_absolute_error: 31.870121, mean_q: -46.818016\n",
            " 45129/50000: episode: 288, duration: 0.271s, episode steps: 85, steps per second: 314, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000], mean observation: -0.183 [-0.910, 0.507], loss: 2.148305, mean_absolute_error: 31.850830, mean_q: -46.770142\n",
            " 45329/50000: episode: 289, duration: 0.639s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.670 [0.000, 2.000], mean observation: -0.295 [-1.200, 0.062], loss: 2.331200, mean_absolute_error: 31.768400, mean_q: -46.573536\n",
            " 45486/50000: episode: 290, duration: 0.481s, episode steps: 157, steps per second: 326, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.241 [-1.200, 0.537], loss: 1.812621, mean_absolute_error: 31.962019, mean_q: -46.873528\n",
            " 45654/50000: episode: 291, duration: 0.520s, episode steps: 168, steps per second: 323, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.229 [-1.174, 0.519], loss: 2.367550, mean_absolute_error: 32.243011, mean_q: -47.266144\n",
            " 45745/50000: episode: 292, duration: 0.296s, episode steps: 91, steps per second: 308, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.989 [0.000, 2.000], mean observation: -0.220 [-0.948, 0.521], loss: 1.842809, mean_absolute_error: 31.782463, mean_q: -46.555679\n",
            " 45898/50000: episode: 293, duration: 0.499s, episode steps: 153, steps per second: 307, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000], mean observation: -0.192 [-1.200, 0.537], loss: 3.691847, mean_absolute_error: 32.077820, mean_q: -46.938808\n",
            " 46005/50000: episode: 294, duration: 0.359s, episode steps: 107, steps per second: 298, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 2.000], mean observation: -0.206 [-0.968, 0.509], loss: 0.810447, mean_absolute_error: 32.136520, mean_q: -47.200565\n",
            " 46146/50000: episode: 295, duration: 0.430s, episode steps: 141, steps per second: 328, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.539 [0.000, 2.000], mean observation: -0.201 [-0.996, 0.514], loss: 1.942025, mean_absolute_error: 31.872639, mean_q: -46.652699\n",
            " 46311/50000: episode: 296, duration: 0.497s, episode steps: 165, steps per second: 332, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000], mean observation: -0.209 [-1.128, 0.514], loss: 2.741719, mean_absolute_error: 31.940868, mean_q: -46.767002\n",
            " 46470/50000: episode: 297, duration: 0.482s, episode steps: 159, steps per second: 330, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.887 [0.000, 2.000], mean observation: -0.269 [-1.087, 0.523], loss: 2.399971, mean_absolute_error: 31.978098, mean_q: -46.920559\n",
            " 46626/50000: episode: 298, duration: 0.469s, episode steps: 156, steps per second: 333, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.244 [0.000, 2.000], mean observation: -0.178 [-1.200, 0.537], loss: 1.847603, mean_absolute_error: 32.123508, mean_q: -47.194286\n",
            " 46826/50000: episode: 299, duration: 0.606s, episode steps: 200, steps per second: 330, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.705 [0.000, 2.000], mean observation: -0.310 [-1.146, 0.059], loss: 2.976311, mean_absolute_error: 32.060829, mean_q: -46.950558\n",
            " 46984/50000: episode: 300, duration: 0.472s, episode steps: 158, steps per second: 335, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.139 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.537], loss: 0.449001, mean_absolute_error: 32.227726, mean_q: -47.380840\n",
            " 47136/50000: episode: 301, duration: 0.461s, episode steps: 152, steps per second: 330, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.224 [-1.200, 0.537], loss: 2.719812, mean_absolute_error: 32.482666, mean_q: -47.611080\n",
            " 47336/50000: episode: 302, duration: 0.621s, episode steps: 200, steps per second: 322, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.535 [0.000, 2.000], mean observation: -0.187 [-1.200, 0.315], loss: 3.711249, mean_absolute_error: 32.061569, mean_q: -46.781124\n",
            " 47454/50000: episode: 303, duration: 0.359s, episode steps: 118, steps per second: 329, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.508 [0.000, 2.000], mean observation: -0.218 [-1.035, 0.511], loss: 2.263040, mean_absolute_error: 32.145290, mean_q: -47.135044\n",
            " 47549/50000: episode: 304, duration: 0.292s, episode steps: 95, steps per second: 326, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.178 [-0.887, 0.511], loss: 1.526310, mean_absolute_error: 32.341709, mean_q: -47.470528\n",
            " 47696/50000: episode: 305, duration: 0.446s, episode steps: 147, steps per second: 330, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.252 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.526], loss: 2.011599, mean_absolute_error: 32.118855, mean_q: -47.154709\n",
            " 47845/50000: episode: 306, duration: 0.455s, episode steps: 149, steps per second: 328, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.255 [-1.062, 0.514], loss: 1.863795, mean_absolute_error: 32.178341, mean_q: -47.209370\n",
            " 47987/50000: episode: 307, duration: 0.428s, episode steps: 142, steps per second: 332, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.296 [0.000, 2.000], mean observation: -0.234 [-1.200, 0.542], loss: 3.014579, mean_absolute_error: 32.057148, mean_q: -46.921173\n",
            " 48123/50000: episode: 308, duration: 0.423s, episode steps: 136, steps per second: 322, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.309 [0.000, 2.000], mean observation: -0.231 [-1.200, 0.537], loss: 1.509359, mean_absolute_error: 32.160519, mean_q: -47.187840\n",
            " 48268/50000: episode: 309, duration: 0.433s, episode steps: 145, steps per second: 335, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.207 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.540], loss: 2.010397, mean_absolute_error: 32.556782, mean_q: -47.734562\n",
            " 48468/50000: episode: 310, duration: 0.608s, episode steps: 200, steps per second: 329, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000], mean observation: -0.180 [-1.200, 0.405], loss: 2.906212, mean_absolute_error: 32.480045, mean_q: -47.533058\n",
            " 48584/50000: episode: 311, duration: 0.349s, episode steps: 116, steps per second: 332, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.397 [0.000, 2.000], mean observation: -0.234 [-1.073, 0.502], loss: 0.925057, mean_absolute_error: 32.448624, mean_q: -47.622650\n",
            " 48724/50000: episode: 312, duration: 0.425s, episode steps: 140, steps per second: 329, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000], mean observation: -0.224 [-1.200, 0.537], loss: 1.457331, mean_absolute_error: 32.599854, mean_q: -47.760319\n",
            " 48867/50000: episode: 313, duration: 0.433s, episode steps: 143, steps per second: 330, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.537], loss: 2.370157, mean_absolute_error: 32.331921, mean_q: -47.300644\n",
            " 49018/50000: episode: 314, duration: 0.462s, episode steps: 151, steps per second: 327, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.219 [0.000, 2.000], mean observation: -0.250 [-1.200, 0.544], loss: 4.131416, mean_absolute_error: 32.619370, mean_q: -47.688763\n",
            " 49142/50000: episode: 315, duration: 0.378s, episode steps: 124, steps per second: 328, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.331 [0.000, 2.000], mean observation: -0.236 [-1.051, 0.528], loss: 1.811234, mean_absolute_error: 32.409740, mean_q: -47.428444\n",
            " 49275/50000: episode: 316, duration: 0.404s, episode steps: 133, steps per second: 330, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000], mean observation: -0.255 [-1.049, 0.516], loss: 1.856639, mean_absolute_error: 32.467026, mean_q: -47.528290\n",
            " 49460/50000: episode: 317, duration: 0.559s, episode steps: 185, steps per second: 331, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.259 [0.000, 2.000], mean observation: -0.224 [-1.135, 0.537], loss: 1.495571, mean_absolute_error: 32.128510, mean_q: -47.014328\n",
            " 49601/50000: episode: 318, duration: 0.433s, episode steps: 141, steps per second: 326, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.312 [0.000, 2.000], mean observation: -0.217 [-1.200, 0.537], loss: 2.647990, mean_absolute_error: 32.918995, mean_q: -48.093445\n",
            " 49756/50000: episode: 319, duration: 0.471s, episode steps: 155, steps per second: 329, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000], mean observation: -0.239 [-1.200, 0.537], loss: 1.652056, mean_absolute_error: 32.785347, mean_q: -47.937046\n",
            " 49906/50000: episode: 320, duration: 0.455s, episode steps: 150, steps per second: 330, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.537], loss: 4.004577, mean_absolute_error: 32.559120, mean_q: -47.543938\n",
            "done, took 170.018 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "46yjN9D0mhiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 実行"
      ]
    },
    {
      "metadata": {
        "id": "oUcwbR4Q0x80",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## test  \n",
        "ローカル上ではこれでアニメーションも表示される？  \n",
        "colab上では実行してもしなくても同じ"
      ]
    },
    {
      "metadata": {
        "id": "h9Ur8kWrOpv_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes=1, visualize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CAb1ToiLQIVV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 確認\n",
        "- colab上ではこっちを実行\n",
        "- 2回以上実行するとエラーがでる。\n",
        "原因、解決方法は調査中\n",
        "同じような[issue](https://github.com/openai/gym/issues/430)"
      ]
    },
    {
      "metadata": {
        "id": "kFBOmAzxQEII",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "for n_episode in range (3):\n",
        "    #observation = env.reset()\n",
        "    for t in range(100):\n",
        "        plt.imshow(env.render(mode='rgb_array'))\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "        action = env.action_space.sample() # アクションのランダム選択\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            print(\"Episode {} finished after {} timesteps\".format(n_episode+1,t+1))\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bb9nQkyy0feo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ダウンロード  \n",
        "これで学習の途中経過をダウンロードできる "
      ]
    },
    {
      "metadata": {
        "id": "QTavTeUHwZdG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import glob\n",
        "\n",
        "for file in glob.glob(\"openaigym.video.*.mp4\"):\n",
        "  files.download(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bMgPDkNsw7ju",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 参考\n",
        "[Keras Documentation　コールバック](https://keras.io/ja/callbacks/)  \n",
        "[ColaboratoryでOpenAI gym](http://bcl.sci.yamaguchi-u.ac.jp/~jun/ja/blog/180828-openai-colaboratory)"
      ]
    },
    {
      "metadata": {
        "id": "v0l5nywYw9SZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_MountainCar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junyamahira/ReinforcementLearning_Prac/blob/master/DQN_MountainCar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BbHY0Gc_tdz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# keras-rlのインストール"
      ]
    },
    {
      "metadata": {
        "id": "ap27433nXCn9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## git clone \n",
        "- これかpipのどちらかを実行すればよい (pipがおすすめ)\n",
        "- keras-rlの中に入り、setup.pyを実行する"
      ]
    },
    {
      "metadata": {
        "id": "xLXt4VtkXLwh",
        "colab_type": "code",
        "outputId": "52769e99-8b76-47a6-e1b8-3947166fbb41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/keras-rl/keras-rl.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-rl'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 1710 (delta 3), reused 7 (delta 2), pack-reused 1698\u001b[K\n",
            "Receiving objects: 100% (1710/1710), 1.38 MiB | 19.35 MiB/s, done.\n",
            "Resolving deltas: 100% (1056/1056), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IaD-jZAAXa8S",
        "colab_type": "code",
        "outputId": "92ca32d0-eb5f-4124-f1d2-509fc1d6b118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!ls keras-rl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assets\t\t examples\t    mkdocs.yml\trl\t   tests\n",
            "CONTRIBUTING.md  ISSUE_TEMPLATE.md  pytest.ini\tsetup.cfg  utils\n",
            "docs\t\t LICENSE\t    README.md\tsetup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8cw_CkyZFsd",
        "colab_type": "code",
        "outputId": "be46aaca-71ad-4f27-c88e-7971eeb08fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!cd keras-rl\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keras-rl  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-_L-L0I2a-uQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## pip"
      ]
    },
    {
      "metadata": {
        "id": "DUyHjkEPbBtS",
        "colab_type": "code",
        "outputId": "eae63ade-f014-4ea9-a9d9-557f1b95652c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "cell_type": "code",
      "source": [
        "pip install keras-rl"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-rl in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JUcjN-baSzPe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# その他のimport"
      ]
    },
    {
      "metadata": {
        "id": "ZG-NDV4AQIFS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K9I5EaaMHWYK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visiual display"
      ]
    },
    {
      "metadata": {
        "id": "qxGA6tFdNNEa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#?\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdpKHXZ8HVee",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBv7HoHzHXJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#　"
      ]
    },
    {
      "metadata": {
        "id": "DGIdiFuXT1rk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MountainCar-v0\")\n",
        "env = wrappers.Monitor(env, './', force=True)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xsV75LWXtMLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# モデル"
      ]
    },
    {
      "metadata": {
        "id": "vFGewr_orPkl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFQDZxpvf9PJ",
        "colab_type": "code",
        "outputId": "eedca0cf-9de6-4bf3-9a5b-abff23fb7c76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 643\n",
            "Trainable params: 643\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F_kWc71Kse2-",
        "colab_type": "code",
        "outputId": "b1cbcb4e-6e2f-4192-b918-8e09229d4bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4663
        }
      },
      "cell_type": "code",
      "source": [
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "# 方策\n",
        "policy = EpsGreedyQPolicy(eps=0.001)\n",
        "#　各パラメータをここで調整\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions,gamma=0.99, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "# 何設定してん？\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   200/50000: episode: 1, duration: 4.169s, episode steps: 200, steps per second: 48, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.199 [-0.972, 0.321], loss: 0.264469, mean_absolute_error: 0.473223, mean_q: -0.290834\n",
            "   400/50000: episode: 2, duration: 3.041s, episode steps: 200, steps per second: 66, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [1.000, 2.000], mean observation: -0.242 [-0.622, 0.011], loss: 0.008207, mean_absolute_error: 1.312887, mean_q: -1.877084\n",
            "   600/50000: episode: 3, duration: 0.736s, episode steps: 200, steps per second: 272, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [1.000, 2.000], mean observation: -0.212 [-0.697, 0.026], loss: 0.011144, mean_absolute_error: 2.254631, mean_q: -3.331532\n",
            "   800/50000: episode: 4, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [1.000, 2.000], mean observation: -0.231 [-0.535, 0.011], loss: 0.044964, mean_absolute_error: 3.349456, mean_q: -4.943903\n",
            "  1000/50000: episode: 5, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000], mean observation: -0.228 [-0.720, 0.024], loss: 0.090599, mean_absolute_error: 4.469054, mean_q: -6.603441\n",
            "  1200/50000: episode: 6, duration: 0.649s, episode steps: 200, steps per second: 308, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [1.000, 2.000], mean observation: -0.233 [-0.558, 0.012], loss: 0.099081, mean_absolute_error: 5.594741, mean_q: -8.293760\n",
            "  1400/50000: episode: 7, duration: 0.657s, episode steps: 200, steps per second: 305, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [1.000, 2.000], mean observation: -0.219 [-0.810, 0.027], loss: 0.263834, mean_absolute_error: 6.704674, mean_q: -9.902260\n",
            "  1600/50000: episode: 8, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [1.000, 2.000], mean observation: -0.220 [-0.802, 0.035], loss: 0.339241, mean_absolute_error: 7.751396, mean_q: -11.450625\n",
            "  1800/50000: episode: 9, duration: 2.947s, episode steps: 200, steps per second: 68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.248 [-0.828, 0.021], loss: 0.358738, mean_absolute_error: 8.822618, mean_q: -13.036701\n",
            "  2000/50000: episode: 10, duration: 0.754s, episode steps: 200, steps per second: 265, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.495 [0.000, 2.000], mean observation: -0.230 [-0.580, 0.012], loss: 0.432528, mean_absolute_error: 9.860876, mean_q: -14.573743\n",
            "  2200/50000: episode: 11, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.244 [-1.200, 0.248], loss: 0.499548, mean_absolute_error: 10.865774, mean_q: -16.095261\n",
            "  2400/50000: episode: 12, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000], mean observation: -0.238 [-0.708, 0.025], loss: 0.559077, mean_absolute_error: 11.887634, mean_q: -17.620649\n",
            "  2600/50000: episode: 13, duration: 0.658s, episode steps: 200, steps per second: 304, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 2.000], mean observation: -0.210 [-0.723, 0.029], loss: 0.799398, mean_absolute_error: 12.846313, mean_q: -19.048540\n",
            "  2800/50000: episode: 14, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000], mean observation: -0.242 [-0.576, 0.009], loss: 0.780030, mean_absolute_error: 13.783247, mean_q: -20.441526\n",
            "  3000/50000: episode: 15, duration: 0.656s, episode steps: 200, steps per second: 305, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000], mean observation: -0.282 [-1.200, 0.049], loss: 1.348592, mean_absolute_error: 14.568371, mean_q: -21.443657\n",
            "  3200/50000: episode: 16, duration: 0.657s, episode steps: 200, steps per second: 305, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000], mean observation: -0.268 [-0.918, 0.015], loss: 0.894734, mean_absolute_error: 15.477747, mean_q: -22.977837\n",
            "  3400/50000: episode: 17, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000], mean observation: -0.245 [-0.820, 0.036], loss: 1.321203, mean_absolute_error: 16.348804, mean_q: -24.237175\n",
            "  3600/50000: episode: 18, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000], mean observation: -0.269 [-0.811, 0.019], loss: 1.354570, mean_absolute_error: 17.158762, mean_q: -25.476809\n",
            "  3800/50000: episode: 19, duration: 0.656s, episode steps: 200, steps per second: 305, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000], mean observation: -0.228 [-0.792, 0.015], loss: 1.130942, mean_absolute_error: 18.011477, mean_q: -26.769472\n",
            "  4000/50000: episode: 20, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.278 [-1.033, 0.052], loss: 1.263589, mean_absolute_error: 18.829466, mean_q: -28.007164\n",
            "  4200/50000: episode: 21, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.220], loss: 2.139337, mean_absolute_error: 19.642324, mean_q: -29.038565\n",
            "  4400/50000: episode: 22, duration: 0.653s, episode steps: 200, steps per second: 306, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000], mean observation: -0.250 [-0.806, 0.019], loss: 2.318857, mean_absolute_error: 20.352205, mean_q: -30.172239\n",
            "  4600/50000: episode: 23, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000], mean observation: -0.271 [-1.200, 0.438], loss: 1.878290, mean_absolute_error: 21.059746, mean_q: -31.255133\n",
            "  4800/50000: episode: 24, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.248 [-0.716, 0.016], loss: 3.462504, mean_absolute_error: 21.738325, mean_q: -32.132675\n",
            "  5000/50000: episode: 25, duration: 0.661s, episode steps: 200, steps per second: 303, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000], mean observation: -0.281 [-1.200, 0.039], loss: 2.769835, mean_absolute_error: 22.359262, mean_q: -33.102043\n",
            "  5200/50000: episode: 26, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000], mean observation: -0.223 [-0.767, 0.028], loss: 2.429881, mean_absolute_error: 23.010483, mean_q: -34.167709\n",
            "  5400/50000: episode: 27, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000], mean observation: -0.256 [-0.873, 0.032], loss: 2.878025, mean_absolute_error: 23.608923, mean_q: -34.971973\n",
            "  5600/50000: episode: 28, duration: 2.944s, episode steps: 200, steps per second: 68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.225 [-0.998, 0.204], loss: 4.450325, mean_absolute_error: 24.244917, mean_q: -35.879166\n",
            "  5800/50000: episode: 29, duration: 0.762s, episode steps: 200, steps per second: 262, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000], mean observation: -0.264 [-1.010, 0.108], loss: 2.774533, mean_absolute_error: 24.730413, mean_q: -36.685970\n",
            "  6000/50000: episode: 30, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000], mean observation: -0.249 [-0.914, 0.044], loss: 2.715255, mean_absolute_error: 25.352615, mean_q: -37.667145\n",
            "  6200/50000: episode: 31, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.242 [-1.200, 0.236], loss: 3.703198, mean_absolute_error: 25.876419, mean_q: -38.376034\n",
            "  6400/50000: episode: 32, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000], mean observation: -0.292 [-1.156, 0.017], loss: 4.483042, mean_absolute_error: 26.409723, mean_q: -39.159615\n",
            "  6600/50000: episode: 33, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000], mean observation: -0.227 [-1.003, 0.440], loss: 3.271942, mean_absolute_error: 26.926664, mean_q: -40.022770\n",
            "  6800/50000: episode: 34, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000], mean observation: -0.278 [-1.200, 0.031], loss: 4.675241, mean_absolute_error: 27.430803, mean_q: -40.667179\n",
            "  7000/50000: episode: 35, duration: 0.660s, episode steps: 200, steps per second: 303, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000], mean observation: -0.218 [-1.021, 0.088], loss: 4.351493, mean_absolute_error: 27.920942, mean_q: -41.388840\n",
            "  7200/50000: episode: 36, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000], mean observation: -0.161 [-1.200, 0.464], loss: 4.246748, mean_absolute_error: 28.305510, mean_q: -41.949654\n",
            "  7371/50000: episode: 37, duration: 0.568s, episode steps: 171, steps per second: 301, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000], mean observation: -0.251 [-1.084, 0.506], loss: 3.479137, mean_absolute_error: 28.737202, mean_q: -42.680283\n",
            "  7571/50000: episode: 38, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.258 [-0.822, 0.022], loss: 5.044859, mean_absolute_error: 29.236494, mean_q: -43.364101\n",
            "  7680/50000: episode: 39, duration: 0.373s, episode steps: 109, steps per second: 292, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000], mean observation: -0.205 [-0.980, 0.516], loss: 5.399173, mean_absolute_error: 29.508766, mean_q: -43.724010\n",
            "  7880/50000: episode: 40, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000], mean observation: -0.240 [-0.851, 0.023], loss: 4.196890, mean_absolute_error: 29.822002, mean_q: -44.347172\n",
            "  8080/50000: episode: 41, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.750 [0.000, 2.000], mean observation: -0.279 [-0.822, 0.015], loss: 3.536664, mean_absolute_error: 30.300440, mean_q: -45.110455\n",
            "  8280/50000: episode: 42, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000], mean observation: -0.265 [-1.024, 0.043], loss: 4.399347, mean_absolute_error: 30.747082, mean_q: -45.675068\n",
            "  8480/50000: episode: 43, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.255 [-0.669, 0.011], loss: 5.414856, mean_absolute_error: 31.191172, mean_q: -46.293545\n",
            "  8680/50000: episode: 44, duration: 0.674s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000], mean observation: -0.300 [-1.089, 0.030], loss: 3.944353, mean_absolute_error: 31.534712, mean_q: -46.870228\n",
            "  8880/50000: episode: 45, duration: 0.670s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000], mean observation: -0.282 [-1.200, 0.045], loss: 6.453026, mean_absolute_error: 31.963135, mean_q: -47.403980\n",
            "  9080/50000: episode: 46, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.249 [-0.932, 0.033], loss: 5.591299, mean_absolute_error: 32.296036, mean_q: -47.892563\n",
            "  9280/50000: episode: 47, duration: 0.675s, episode steps: 200, steps per second: 296, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000], mean observation: -0.275 [-0.938, 0.025], loss: 5.340531, mean_absolute_error: 32.661728, mean_q: -48.482872\n",
            "  9480/50000: episode: 48, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.249 [-0.856, 0.031], loss: 5.659887, mean_absolute_error: 32.970100, mean_q: -48.998146\n",
            "  9680/50000: episode: 49, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.233 [-0.686, 0.016], loss: 6.623822, mean_absolute_error: 33.283520, mean_q: -49.427467\n",
            "  9880/50000: episode: 50, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000], mean observation: -0.245 [-0.773, 0.032], loss: 4.674743, mean_absolute_error: 33.585037, mean_q: -49.875744\n",
            " 10080/50000: episode: 51, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.230 [-1.200, 0.419], loss: 4.796113, mean_absolute_error: 33.947605, mean_q: -50.564285\n",
            " 10280/50000: episode: 52, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000], mean observation: -0.271 [-1.029, 0.027], loss: 3.762137, mean_absolute_error: 34.379871, mean_q: -51.153759\n",
            " 10456/50000: episode: 53, duration: 0.593s, episode steps: 176, steps per second: 297, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.966 [0.000, 2.000], mean observation: -0.269 [-1.194, 0.521], loss: 6.410531, mean_absolute_error: 34.676426, mean_q: -51.558701\n",
            " 10656/50000: episode: 54, duration: 0.668s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000], mean observation: -0.253 [-0.972, 0.043], loss: 6.791332, mean_absolute_error: 34.953823, mean_q: -51.926605\n",
            " 10856/50000: episode: 55, duration: 0.663s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000], mean observation: -0.295 [-1.068, 0.055], loss: 6.711404, mean_absolute_error: 35.211491, mean_q: -52.282852\n",
            " 11056/50000: episode: 56, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000], mean observation: -0.242 [-0.765, 0.027], loss: 6.526304, mean_absolute_error: 35.444279, mean_q: -52.615509\n",
            " 11256/50000: episode: 57, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000], mean observation: -0.232 [-0.991, 0.054], loss: 6.641700, mean_absolute_error: 35.684307, mean_q: -52.973377\n",
            " 11456/50000: episode: 58, duration: 0.670s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [1.000, 2.000], mean observation: -0.212 [-0.771, 0.022], loss: 5.303106, mean_absolute_error: 35.871815, mean_q: -53.232800\n",
            " 11656/50000: episode: 59, duration: 0.672s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000], mean observation: -0.244 [-0.733, 0.028], loss: 6.584077, mean_absolute_error: 36.166138, mean_q: -53.609097\n",
            " 11856/50000: episode: 60, duration: 0.668s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000], mean observation: -0.288 [-1.200, 0.118], loss: 6.374290, mean_absolute_error: 36.365402, mean_q: -54.060001\n",
            " 12056/50000: episode: 61, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000], mean observation: -0.226 [-0.843, 0.030], loss: 6.437493, mean_absolute_error: 36.612389, mean_q: -54.354580\n",
            " 12256/50000: episode: 62, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.237 [-0.776, 0.024], loss: 6.076293, mean_absolute_error: 36.889149, mean_q: -54.821793\n",
            " 12456/50000: episode: 63, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000], mean observation: -0.239 [-0.860, 0.108], loss: 8.404611, mean_absolute_error: 37.115150, mean_q: -55.029118\n",
            " 12634/50000: episode: 64, duration: 0.602s, episode steps: 178, steps per second: 296, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.416 [0.000, 2.000], mean observation: -0.207 [-0.931, 0.504], loss: 8.020639, mean_absolute_error: 37.274261, mean_q: -55.267666\n",
            " 12834/50000: episode: 65, duration: 2.946s, episode steps: 200, steps per second: 68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 2.000], mean observation: -0.228 [-0.827, 0.025], loss: 8.580289, mean_absolute_error: 37.325390, mean_q: -55.293072\n",
            " 13034/50000: episode: 66, duration: 0.851s, episode steps: 200, steps per second: 235, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.515 [1.000, 2.000], mean observation: -0.207 [-0.883, 0.183], loss: 7.028780, mean_absolute_error: 37.453575, mean_q: -55.562088\n",
            " 13234/50000: episode: 67, duration: 0.741s, episode steps: 200, steps per second: 270, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000], mean observation: -0.273 [-0.948, 0.028], loss: 7.257493, mean_absolute_error: 37.660679, mean_q: -55.786915\n",
            " 13434/50000: episode: 68, duration: 0.740s, episode steps: 200, steps per second: 270, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.254 [-1.157, 0.160], loss: 7.630875, mean_absolute_error: 37.896828, mean_q: -56.282646\n",
            " 13634/50000: episode: 69, duration: 0.744s, episode steps: 200, steps per second: 269, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000], mean observation: -0.223 [-1.020, 0.039], loss: 8.304338, mean_absolute_error: 38.025909, mean_q: -56.468300\n",
            " 13834/50000: episode: 70, duration: 0.738s, episode steps: 200, steps per second: 271, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000], mean observation: -0.250 [-1.200, 0.033], loss: 5.627409, mean_absolute_error: 38.243462, mean_q: -56.881256\n",
            " 14034/50000: episode: 71, duration: 0.742s, episode steps: 200, steps per second: 270, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000], mean observation: -0.278 [-0.889, 0.018], loss: 6.847934, mean_absolute_error: 38.470360, mean_q: -57.169071\n",
            " 14234/50000: episode: 72, duration: 0.742s, episode steps: 200, steps per second: 270, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000], mean observation: -0.284 [-0.797, 0.028], loss: 8.220456, mean_absolute_error: 38.643673, mean_q: -57.360420\n",
            " 14434/50000: episode: 73, duration: 0.693s, episode steps: 200, steps per second: 289, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000], mean observation: -0.250 [-1.200, 0.185], loss: 7.073208, mean_absolute_error: 38.814938, mean_q: -57.671528\n",
            " 14634/50000: episode: 74, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.257 [-1.200, 0.062], loss: 8.909228, mean_absolute_error: 38.983025, mean_q: -57.849232\n",
            " 14834/50000: episode: 75, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.745 [1.000, 2.000], mean observation: -0.209 [-0.586, 0.013], loss: 7.422755, mean_absolute_error: 39.016151, mean_q: -57.886887\n",
            " 15034/50000: episode: 76, duration: 0.698s, episode steps: 200, steps per second: 286, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.390 [0.000, 2.000], mean observation: -0.236 [-0.887, 0.042], loss: 5.597180, mean_absolute_error: 39.202801, mean_q: -58.295609\n",
            " 15234/50000: episode: 77, duration: 0.742s, episode steps: 200, steps per second: 270, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000], mean observation: -0.251 [-0.909, 0.031], loss: 9.919940, mean_absolute_error: 39.244732, mean_q: -58.192764\n",
            " 15434/50000: episode: 78, duration: 0.740s, episode steps: 200, steps per second: 270, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.234 [-0.900, 0.042], loss: 8.991487, mean_absolute_error: 39.394997, mean_q: -58.472549\n",
            " 15634/50000: episode: 79, duration: 0.736s, episode steps: 200, steps per second: 272, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000], mean observation: -0.231 [-0.744, 0.019], loss: 9.101897, mean_absolute_error: 39.435219, mean_q: -58.571983\n",
            " 15834/50000: episode: 80, duration: 0.738s, episode steps: 200, steps per second: 271, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.299], loss: 9.347049, mean_absolute_error: 39.532284, mean_q: -58.647652\n",
            " 15975/50000: episode: 81, duration: 0.524s, episode steps: 141, steps per second: 269, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000], mean observation: -0.253 [-1.200, 0.509], loss: 6.857383, mean_absolute_error: 39.635334, mean_q: -58.868607\n",
            " 16175/50000: episode: 82, duration: 0.734s, episode steps: 200, steps per second: 273, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.265 [-0.844, 0.033], loss: 10.571992, mean_absolute_error: 39.698479, mean_q: -58.901894\n",
            " 16375/50000: episode: 83, duration: 0.737s, episode steps: 200, steps per second: 271, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.237 [-0.816, 0.025], loss: 6.807539, mean_absolute_error: 39.768955, mean_q: -59.146210\n",
            " 16575/50000: episode: 84, duration: 0.744s, episode steps: 200, steps per second: 269, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000], mean observation: -0.243 [-0.976, 0.230], loss: 6.293673, mean_absolute_error: 40.003395, mean_q: -59.543869\n",
            " 16774/50000: episode: 85, duration: 0.730s, episode steps: 199, steps per second: 273, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000], mean observation: -0.250 [-1.200, 0.525], loss: 9.275949, mean_absolute_error: 40.107342, mean_q: -59.453163\n",
            " 16974/50000: episode: 86, duration: 0.735s, episode steps: 200, steps per second: 272, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000], mean observation: -0.235 [-0.830, 0.029], loss: 10.420440, mean_absolute_error: 40.181099, mean_q: -59.581470\n",
            " 17174/50000: episode: 87, duration: 0.735s, episode steps: 200, steps per second: 272, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.495 [0.000, 2.000], mean observation: -0.210 [-0.809, 0.032], loss: 7.537829, mean_absolute_error: 40.187782, mean_q: -59.741734\n",
            " 17374/50000: episode: 88, duration: 0.741s, episode steps: 200, steps per second: 270, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.168], loss: 11.087602, mean_absolute_error: 40.304977, mean_q: -59.803360\n",
            " 17574/50000: episode: 89, duration: 0.746s, episode steps: 200, steps per second: 268, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.216 [-1.021, 0.307], loss: 10.779100, mean_absolute_error: 40.314659, mean_q: -59.695663\n",
            " 17774/50000: episode: 90, duration: 0.711s, episode steps: 200, steps per second: 281, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.720 [0.000, 2.000], mean observation: -0.211 [-0.779, 0.033], loss: 8.354958, mean_absolute_error: 40.247452, mean_q: -59.734230\n",
            " 17974/50000: episode: 91, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000], mean observation: -0.255 [-1.010, 0.071], loss: 7.879617, mean_absolute_error: 40.403744, mean_q: -60.039253\n",
            " 18174/50000: episode: 92, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.258 [-0.971, 0.141], loss: 8.500701, mean_absolute_error: 40.532421, mean_q: -60.247803\n",
            " 18374/50000: episode: 93, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.625 [0.000, 2.000], mean observation: -0.209 [-0.873, 0.024], loss: 10.425900, mean_absolute_error: 40.402653, mean_q: -59.874287\n",
            " 18574/50000: episode: 94, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.201], loss: 10.260589, mean_absolute_error: 40.436649, mean_q: -59.962032\n",
            " 18774/50000: episode: 95, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.430 [0.000, 2.000], mean observation: -0.236 [-0.801, 0.029], loss: 8.304234, mean_absolute_error: 40.472363, mean_q: -60.066040\n",
            " 18974/50000: episode: 96, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000], mean observation: -0.231 [-0.863, 0.034], loss: 6.710110, mean_absolute_error: 40.543240, mean_q: -60.336464\n",
            " 19174/50000: episode: 97, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000], mean observation: -0.227 [-0.910, 0.032], loss: 9.331898, mean_absolute_error: 40.643799, mean_q: -60.309647\n",
            " 19374/50000: episode: 98, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.233 [-1.130, 0.083], loss: 8.863507, mean_absolute_error: 40.714630, mean_q: -60.450718\n",
            " 19574/50000: episode: 99, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.251 [-0.964, 0.125], loss: 8.444583, mean_absolute_error: 40.813602, mean_q: -60.618408\n",
            " 19774/50000: episode: 100, duration: 0.673s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.255 [-0.856, 0.034], loss: 10.286595, mean_absolute_error: 40.858212, mean_q: -60.583858\n",
            " 19974/50000: episode: 101, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.064], loss: 6.816685, mean_absolute_error: 40.963776, mean_q: -60.843014\n",
            " 20174/50000: episode: 102, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000], mean observation: -0.248 [-1.076, 0.209], loss: 8.744537, mean_absolute_error: 41.058887, mean_q: -61.008858\n",
            " 20374/50000: episode: 103, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000], mean observation: -0.229 [-0.621, 0.020], loss: 9.690722, mean_absolute_error: 41.059952, mean_q: -60.817406\n",
            " 20574/50000: episode: 104, duration: 0.661s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.570 [0.000, 2.000], mean observation: -0.270 [-1.005, 0.368], loss: 9.741005, mean_absolute_error: 41.115078, mean_q: -61.041172\n",
            " 20774/50000: episode: 105, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000], mean observation: -0.235 [-1.044, 0.120], loss: 6.839656, mean_absolute_error: 41.172474, mean_q: -61.230358\n",
            " 20974/50000: episode: 106, duration: 0.660s, episode steps: 200, steps per second: 303, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.221 [-0.651, 0.021], loss: 9.364170, mean_absolute_error: 41.301876, mean_q: -61.215050\n",
            " 21174/50000: episode: 107, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000], mean observation: -0.238 [-0.805, 0.024], loss: 6.384614, mean_absolute_error: 41.383198, mean_q: -61.549442\n",
            " 21374/50000: episode: 108, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.725 [1.000, 2.000], mean observation: -0.192 [-0.663, 0.024], loss: 8.765477, mean_absolute_error: 41.405148, mean_q: -61.475433\n",
            " 21574/50000: episode: 109, duration: 0.670s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000], mean observation: -0.278 [-1.200, 0.059], loss: 6.773156, mean_absolute_error: 41.515331, mean_q: -61.718155\n",
            " 21774/50000: episode: 110, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.605 [1.000, 2.000], mean observation: -0.218 [-0.659, 0.015], loss: 12.972512, mean_absolute_error: 41.503250, mean_q: -61.457401\n",
            " 21974/50000: episode: 111, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.545 [1.000, 2.000], mean observation: -0.218 [-0.590, 0.017], loss: 8.993511, mean_absolute_error: 41.359486, mean_q: -61.334606\n",
            " 22174/50000: episode: 112, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.268 [-1.200, 0.034], loss: 5.158388, mean_absolute_error: 41.504532, mean_q: -61.781864\n",
            " 22374/50000: episode: 113, duration: 0.672s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [0.000, 2.000], mean observation: -0.225 [-0.789, 0.015], loss: 9.587852, mean_absolute_error: 41.568966, mean_q: -61.572578\n",
            " 22574/50000: episode: 114, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000], mean observation: -0.237 [-0.726, 0.028], loss: 10.066904, mean_absolute_error: 41.629814, mean_q: -61.767197\n",
            " 22770/50000: episode: 115, duration: 0.654s, episode steps: 196, steps per second: 300, episode reward: -196.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.504], loss: 8.299515, mean_absolute_error: 41.618275, mean_q: -61.835690\n",
            " 22970/50000: episode: 116, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000], mean observation: -0.253 [-0.750, 0.027], loss: 7.766144, mean_absolute_error: 41.760155, mean_q: -62.055283\n",
            " 23170/50000: episode: 117, duration: 0.674s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.695 [0.000, 2.000], mean observation: -0.284 [-0.810, 0.028], loss: 9.186036, mean_absolute_error: 41.753155, mean_q: -61.946739\n",
            " 23314/50000: episode: 118, duration: 0.481s, episode steps: 144, steps per second: 299, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.986 [0.000, 2.000], mean observation: -0.271 [-1.146, 0.522], loss: 7.985129, mean_absolute_error: 41.827286, mean_q: -62.184002\n",
            " 23499/50000: episode: 119, duration: 0.618s, episode steps: 185, steps per second: 299, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.212 [-1.091, 0.511], loss: 10.189411, mean_absolute_error: 41.936031, mean_q: -62.226437\n",
            " 23699/50000: episode: 120, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000], mean observation: -0.291 [-1.088, 0.021], loss: 7.500081, mean_absolute_error: 41.917149, mean_q: -62.191929\n",
            " 23899/50000: episode: 121, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000], mean observation: -0.254 [-1.062, 0.285], loss: 7.269394, mean_absolute_error: 42.039452, mean_q: -62.501060\n",
            " 24099/50000: episode: 122, duration: 0.660s, episode steps: 200, steps per second: 303, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 2.000], mean observation: -0.233 [-0.734, 0.024], loss: 6.808408, mean_absolute_error: 42.146847, mean_q: -62.713398\n",
            " 24299/50000: episode: 123, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.420 [0.000, 2.000], mean observation: -0.236 [-0.695, 0.019], loss: 7.982869, mean_absolute_error: 42.292274, mean_q: -62.733852\n",
            " 24428/50000: episode: 124, duration: 0.432s, episode steps: 129, steps per second: 299, episode reward: -129.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000], mean observation: -0.264 [-1.189, 0.505], loss: 12.981485, mean_absolute_error: 42.224152, mean_q: -62.547737\n",
            " 24628/50000: episode: 125, duration: 0.674s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000], mean observation: -0.293 [-1.200, 0.053], loss: 8.070823, mean_absolute_error: 42.227615, mean_q: -62.742538\n",
            " 24828/50000: episode: 126, duration: 2.948s, episode steps: 200, steps per second: 68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.735 [0.000, 2.000], mean observation: -0.282 [-1.200, 0.140], loss: 8.931476, mean_absolute_error: 42.323887, mean_q: -62.795948\n",
            " 25028/50000: episode: 127, duration: 0.768s, episode steps: 200, steps per second: 260, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000], mean observation: -0.256 [-0.829, 0.035], loss: 6.355799, mean_absolute_error: 42.436440, mean_q: -63.129307\n",
            " 25186/50000: episode: 128, duration: 0.529s, episode steps: 158, steps per second: 299, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.505], loss: 8.307188, mean_absolute_error: 42.462711, mean_q: -63.135361\n",
            " 25386/50000: episode: 129, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.505 [1.000, 2.000], mean observation: -0.222 [-0.656, 0.013], loss: 8.737962, mean_absolute_error: 42.575386, mean_q: -63.156918\n",
            " 25586/50000: episode: 130, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.365 [1.000, 2.000], mean observation: -0.250 [-0.879, 0.041], loss: 9.393983, mean_absolute_error: 42.477039, mean_q: -62.937153\n",
            " 25786/50000: episode: 131, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.765 [0.000, 2.000], mean observation: -0.209 [-0.565, 0.010], loss: 8.903755, mean_absolute_error: 42.481361, mean_q: -63.022587\n",
            " 25986/50000: episode: 132, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000], mean observation: -0.240 [-0.719, 0.015], loss: 7.482205, mean_absolute_error: 42.506611, mean_q: -63.157120\n",
            " 26186/50000: episode: 133, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.720 [0.000, 2.000], mean observation: -0.284 [-1.200, 0.035], loss: 11.140277, mean_absolute_error: 42.527657, mean_q: -63.067333\n",
            " 26386/50000: episode: 134, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.256 [-0.946, 0.038], loss: 8.986475, mean_absolute_error: 42.546249, mean_q: -63.140945\n",
            " 26586/50000: episode: 135, duration: 0.670s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000], mean observation: -0.274 [-1.200, 0.027], loss: 9.128916, mean_absolute_error: 42.522335, mean_q: -63.113007\n",
            " 26786/50000: episode: 136, duration: 0.673s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000], mean observation: -0.267 [-0.966, 0.038], loss: 8.004288, mean_absolute_error: 42.644501, mean_q: -63.378242\n",
            " 26986/50000: episode: 137, duration: 0.659s, episode steps: 200, steps per second: 303, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000], mean observation: -0.275 [-1.200, 0.167], loss: 8.124056, mean_absolute_error: 42.692177, mean_q: -63.375809\n",
            " 27186/50000: episode: 138, duration: 0.655s, episode steps: 200, steps per second: 306, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000], mean observation: -0.249 [-0.761, 0.032], loss: 9.800482, mean_absolute_error: 42.758068, mean_q: -63.473621\n",
            " 27386/50000: episode: 139, duration: 0.673s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.231 [-0.783, 0.022], loss: 9.039825, mean_absolute_error: 42.683632, mean_q: -63.325089\n",
            " 27586/50000: episode: 140, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000], mean observation: -0.244 [-0.803, 0.027], loss: 8.365215, mean_absolute_error: 42.784580, mean_q: -63.534805\n",
            " 27786/50000: episode: 141, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.285 [0.000, 2.000], mean observation: -0.234 [-0.892, 0.031], loss: 11.319921, mean_absolute_error: 42.747971, mean_q: -63.371849\n",
            " 27986/50000: episode: 142, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000], mean observation: -0.225 [-0.777, 0.084], loss: 10.410291, mean_absolute_error: 42.607090, mean_q: -63.205654\n",
            " 28186/50000: episode: 143, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000], mean observation: -0.206 [-0.843, 0.037], loss: 8.724836, mean_absolute_error: 42.605236, mean_q: -63.292637\n",
            " 28386/50000: episode: 144, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.221 [-0.957, 0.113], loss: 8.767089, mean_absolute_error: 42.631538, mean_q: -63.331242\n",
            " 28586/50000: episode: 145, duration: 0.675s, episode steps: 200, steps per second: 296, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 2.000], mean observation: -0.230 [-0.899, 0.031], loss: 6.537992, mean_absolute_error: 42.696190, mean_q: -63.463558\n",
            " 28786/50000: episode: 146, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000], mean observation: -0.232 [-0.819, 0.013], loss: 7.461546, mean_absolute_error: 42.724316, mean_q: -63.367531\n",
            " 28986/50000: episode: 147, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.261 [-1.200, 0.020], loss: 11.211928, mean_absolute_error: 42.653584, mean_q: -63.166683\n",
            " 29186/50000: episode: 148, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [1.000, 2.000], mean observation: -0.223 [-0.617, 0.014], loss: 8.737943, mean_absolute_error: 42.763485, mean_q: -63.480701\n",
            " 29386/50000: episode: 149, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.680 [0.000, 2.000], mean observation: -0.313 [-1.108, 0.038], loss: 9.635704, mean_absolute_error: 42.717209, mean_q: -63.386768\n",
            " 29586/50000: episode: 150, duration: 0.677s, episode steps: 200, steps per second: 296, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.730 [0.000, 2.000], mean observation: -0.308 [-1.200, 0.057], loss: 7.981009, mean_absolute_error: 42.663658, mean_q: -63.405151\n",
            " 29786/50000: episode: 151, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.545 [0.000, 2.000], mean observation: -0.196 [-0.801, 0.036], loss: 10.322305, mean_absolute_error: 42.698238, mean_q: -63.235130\n",
            " 29986/50000: episode: 152, duration: 0.660s, episode steps: 200, steps per second: 303, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000], mean observation: -0.260 [-0.987, 0.060], loss: 6.464822, mean_absolute_error: 42.772457, mean_q: -63.590927\n",
            " 30186/50000: episode: 153, duration: 0.676s, episode steps: 200, steps per second: 296, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000], mean observation: -0.232 [-1.157, 0.323], loss: 8.151704, mean_absolute_error: 42.833038, mean_q: -63.599483\n",
            " 30386/50000: episode: 154, duration: 0.670s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.258 [-0.984, 0.032], loss: 9.968105, mean_absolute_error: 42.862183, mean_q: -63.524364\n",
            " 30586/50000: episode: 155, duration: 0.674s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.505 [0.000, 2.000], mean observation: -0.222 [-1.145, 0.020], loss: 5.487565, mean_absolute_error: 42.783241, mean_q: -63.589951\n",
            " 30786/50000: episode: 156, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.247 [-1.200, 0.233], loss: 10.069098, mean_absolute_error: 42.788486, mean_q: -63.499767\n",
            " 30986/50000: episode: 157, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000], mean observation: -0.258 [-0.807, 0.031], loss: 7.899118, mean_absolute_error: 42.892513, mean_q: -63.693653\n",
            " 31186/50000: episode: 158, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.465 [0.000, 2.000], mean observation: -0.301 [-1.200, 0.038], loss: 9.339774, mean_absolute_error: 42.745064, mean_q: -63.355392\n",
            " 31386/50000: episode: 159, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 2.000], mean observation: -0.239 [-0.854, 0.027], loss: 10.482124, mean_absolute_error: 42.784576, mean_q: -63.468613\n",
            " 31586/50000: episode: 160, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.228], loss: 11.707682, mean_absolute_error: 42.751251, mean_q: -63.329697\n",
            " 31786/50000: episode: 161, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.237 [-0.593, 0.012], loss: 10.240498, mean_absolute_error: 42.639420, mean_q: -63.291016\n",
            " 31986/50000: episode: 162, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.715 [0.000, 2.000], mean observation: -0.286 [-1.063, 0.030], loss: 13.159016, mean_absolute_error: 42.482697, mean_q: -62.866058\n",
            " 32172/50000: episode: 163, duration: 0.622s, episode steps: 186, steps per second: 299, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.244 [-1.200, 0.503], loss: 7.064432, mean_absolute_error: 42.453968, mean_q: -63.075218\n",
            " 32372/50000: episode: 164, duration: 0.673s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000], mean observation: -0.283 [-0.919, 0.021], loss: 9.708910, mean_absolute_error: 42.568325, mean_q: -63.177174\n",
            " 32572/50000: episode: 165, duration: 0.677s, episode steps: 200, steps per second: 295, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.755 [0.000, 2.000], mean observation: -0.239 [-1.200, 0.307], loss: 4.759630, mean_absolute_error: 42.515995, mean_q: -63.228363\n",
            " 32772/50000: episode: 166, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.249 [-0.871, 0.027], loss: 9.759007, mean_absolute_error: 42.697121, mean_q: -63.348946\n",
            " 32972/50000: episode: 167, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000], mean observation: -0.232 [-0.756, 0.017], loss: 7.945380, mean_absolute_error: 42.610214, mean_q: -63.278885\n",
            " 33172/50000: episode: 168, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000], mean observation: -0.261 [-1.016, 0.033], loss: 7.700367, mean_absolute_error: 42.643169, mean_q: -63.305431\n",
            " 33372/50000: episode: 169, duration: 0.666s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.475 [0.000, 2.000], mean observation: -0.340 [-1.142, 0.030], loss: 7.632518, mean_absolute_error: 42.706676, mean_q: -63.421188\n",
            " 33572/50000: episode: 170, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.254 [-0.714, 0.017], loss: 9.296913, mean_absolute_error: 42.733341, mean_q: -63.397903\n",
            " 33772/50000: episode: 171, duration: 0.682s, episode steps: 200, steps per second: 293, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.235 [-1.081, 0.088], loss: 10.878410, mean_absolute_error: 42.708988, mean_q: -63.295456\n",
            " 33972/50000: episode: 172, duration: 0.670s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.240 [0.000, 1.000], mean observation: -0.327 [-1.091, 0.030], loss: 7.433212, mean_absolute_error: 42.764057, mean_q: -63.545254\n",
            " 34172/50000: episode: 173, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.425 [0.000, 2.000], mean observation: -0.323 [-0.941, 0.038], loss: 6.805202, mean_absolute_error: 42.679474, mean_q: -63.410263\n",
            " 34372/50000: episode: 174, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000], mean observation: -0.281 [-1.200, 0.053], loss: 8.187608, mean_absolute_error: 42.743534, mean_q: -63.441895\n",
            " 34572/50000: episode: 175, duration: 0.676s, episode steps: 200, steps per second: 296, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.230 [-0.820, 0.025], loss: 9.782245, mean_absolute_error: 42.841141, mean_q: -63.492031\n",
            " 34772/50000: episode: 176, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000], mean observation: -0.269 [-1.183, 0.023], loss: 7.924414, mean_absolute_error: 42.729198, mean_q: -63.335659\n",
            " 34972/50000: episode: 177, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.253 [-1.192, 0.034], loss: 7.701051, mean_absolute_error: 42.910137, mean_q: -63.758400\n",
            " 35172/50000: episode: 178, duration: 0.668s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 2.000], mean observation: -0.240 [-0.964, 0.038], loss: 7.375408, mean_absolute_error: 42.988400, mean_q: -63.821095\n",
            " 35372/50000: episode: 179, duration: 0.680s, episode steps: 200, steps per second: 294, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.259 [-0.808, 0.014], loss: 10.397479, mean_absolute_error: 42.995060, mean_q: -63.714100\n",
            " 35572/50000: episode: 180, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.590 [0.000, 2.000], mean observation: -0.282 [-1.200, 0.065], loss: 8.936427, mean_absolute_error: 42.885441, mean_q: -63.505280\n",
            " 35772/50000: episode: 181, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000], mean observation: -0.231 [-0.750, 0.017], loss: 10.232148, mean_absolute_error: 42.931767, mean_q: -63.604317\n",
            " 35972/50000: episode: 182, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.233 [-0.941, 0.024], loss: 11.688386, mean_absolute_error: 42.814556, mean_q: -63.365444\n",
            " 36172/50000: episode: 183, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000], mean observation: -0.245 [-1.200, 0.234], loss: 8.244220, mean_absolute_error: 42.652367, mean_q: -63.283703\n",
            " 36372/50000: episode: 184, duration: 0.675s, episode steps: 200, steps per second: 296, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000], mean observation: -0.228 [-0.881, 0.142], loss: 9.864233, mean_absolute_error: 42.608192, mean_q: -63.157684\n",
            " 36572/50000: episode: 185, duration: 0.679s, episode steps: 200, steps per second: 295, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000], mean observation: -0.272 [-0.776, 0.017], loss: 9.570900, mean_absolute_error: 42.685535, mean_q: -63.271236\n",
            " 36772/50000: episode: 186, duration: 0.658s, episode steps: 200, steps per second: 304, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.475 [0.000, 2.000], mean observation: -0.319 [-1.200, 0.036], loss: 10.614021, mean_absolute_error: 42.634144, mean_q: -63.201855\n",
            " 36972/50000: episode: 187, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.615 [0.000, 2.000], mean observation: -0.281 [-1.087, 0.051], loss: 5.828771, mean_absolute_error: 42.627716, mean_q: -63.334267\n",
            " 37172/50000: episode: 188, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000], mean observation: -0.247 [-1.096, 0.088], loss: 10.389121, mean_absolute_error: 42.507393, mean_q: -63.057510\n",
            " 37372/50000: episode: 189, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.660 [0.000, 2.000], mean observation: -0.288 [-1.169, 0.253], loss: 9.048274, mean_absolute_error: 42.520119, mean_q: -63.070858\n",
            " 37572/50000: episode: 190, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000], mean observation: -0.283 [-1.200, 0.178], loss: 6.756853, mean_absolute_error: 42.605190, mean_q: -63.311596\n",
            " 37772/50000: episode: 191, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.290 [0.000, 2.000], mean observation: -0.184 [-0.982, 0.310], loss: 9.510201, mean_absolute_error: 42.616810, mean_q: -63.184628\n",
            " 37972/50000: episode: 192, duration: 0.661s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.247 [-1.200, 0.150], loss: 10.978198, mean_absolute_error: 42.413368, mean_q: -62.761276\n",
            " 38172/50000: episode: 193, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.760 [1.000, 2.000], mean observation: -0.210 [-0.606, 0.019], loss: 7.087687, mean_absolute_error: 42.393368, mean_q: -62.975498\n",
            " 38302/50000: episode: 194, duration: 0.432s, episode steps: 130, steps per second: 301, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.838 [0.000, 2.000], mean observation: -0.225 [-1.082, 0.509], loss: 9.082184, mean_absolute_error: 42.338959, mean_q: -62.760197\n",
            " 38476/50000: episode: 195, duration: 0.588s, episode steps: 174, steps per second: 296, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.759 [0.000, 2.000], mean observation: -0.242 [-1.200, 0.505], loss: 7.776453, mean_absolute_error: 42.380203, mean_q: -62.862988\n",
            " 38676/50000: episode: 196, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.158], loss: 10.022881, mean_absolute_error: 42.290985, mean_q: -62.724754\n",
            " 38876/50000: episode: 197, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.285 [0.000, 2.000], mean observation: -0.303 [-0.806, 0.022], loss: 9.539196, mean_absolute_error: 42.126148, mean_q: -62.440678\n",
            " 39076/50000: episode: 198, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000], mean observation: -0.242 [-1.148, 0.245], loss: 7.741673, mean_absolute_error: 42.070152, mean_q: -62.432236\n",
            " 39276/50000: episode: 199, duration: 0.674s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000], mean observation: -0.304 [-1.200, 0.058], loss: 8.158908, mean_absolute_error: 42.192616, mean_q: -62.501678\n",
            " 39476/50000: episode: 200, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000], mean observation: -0.298 [-1.174, 0.041], loss: 6.492966, mean_absolute_error: 42.240303, mean_q: -62.703026\n",
            " 39676/50000: episode: 201, duration: 0.674s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.228 [-1.108, 0.309], loss: 6.434618, mean_absolute_error: 42.356522, mean_q: -62.889816\n",
            " 39876/50000: episode: 202, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000], mean observation: -0.210 [-0.925, 0.248], loss: 8.410797, mean_absolute_error: 42.423534, mean_q: -62.908138\n",
            " 40076/50000: episode: 203, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.650 [1.000, 2.000], mean observation: -0.215 [-0.916, 0.022], loss: 6.225224, mean_absolute_error: 42.337597, mean_q: -62.876362\n",
            " 40276/50000: episode: 204, duration: 0.670s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.755 [1.000, 2.000], mean observation: -0.204 [-0.603, 0.017], loss: 8.612381, mean_absolute_error: 42.299301, mean_q: -62.701447\n",
            " 40476/50000: episode: 205, duration: 0.674s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.211 [-0.976, 0.158], loss: 11.173790, mean_absolute_error: 42.331856, mean_q: -62.706768\n",
            " 40676/50000: episode: 206, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000], mean observation: -0.219 [-0.979, 0.137], loss: 8.387115, mean_absolute_error: 42.339882, mean_q: -62.856613\n",
            " 40876/50000: episode: 207, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000], mean observation: -0.274 [-1.200, 0.130], loss: 7.710414, mean_absolute_error: 42.370224, mean_q: -62.834976\n",
            " 41074/50000: episode: 208, duration: 0.662s, episode steps: 198, steps per second: 299, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.869 [0.000, 2.000], mean observation: -0.241 [-1.200, 0.522], loss: 7.657154, mean_absolute_error: 42.330067, mean_q: -62.839252\n",
            " 41274/50000: episode: 209, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000], mean observation: -0.287 [-1.166, 0.033], loss: 8.744325, mean_absolute_error: 42.319286, mean_q: -62.744755\n",
            " 41474/50000: episode: 210, duration: 0.674s, episode steps: 200, steps per second: 297, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.252 [-0.898, 0.031], loss: 8.335235, mean_absolute_error: 42.435196, mean_q: -62.935001\n",
            " 41674/50000: episode: 211, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000], mean observation: -0.242 [-1.200, 0.198], loss: 11.180455, mean_absolute_error: 42.304203, mean_q: -62.693378\n",
            " 41874/50000: episode: 212, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.244 [-0.934, 0.034], loss: 9.955413, mean_absolute_error: 42.215500, mean_q: -62.590031\n",
            " 42063/50000: episode: 213, duration: 0.632s, episode steps: 189, steps per second: 299, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.021 [0.000, 2.000], mean observation: -0.194 [-1.074, 0.500], loss: 8.221092, mean_absolute_error: 42.271118, mean_q: -62.710918\n",
            " 42263/50000: episode: 214, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000], mean observation: -0.284 [-0.932, 0.019], loss: 7.292587, mean_absolute_error: 42.362926, mean_q: -62.869293\n",
            " 42463/50000: episode: 215, duration: 0.672s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000], mean observation: -0.289 [-0.980, 0.021], loss: 6.317202, mean_absolute_error: 42.337997, mean_q: -62.898407\n",
            " 42645/50000: episode: 216, duration: 0.610s, episode steps: 182, steps per second: 298, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.132 [0.000, 2.000], mean observation: -0.205 [-1.200, 0.503], loss: 5.468441, mean_absolute_error: 42.542908, mean_q: -63.214619\n",
            " 42845/50000: episode: 217, duration: 3.032s, episode steps: 200, steps per second: 66, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000], mean observation: -0.241 [-1.200, 0.098], loss: 9.493958, mean_absolute_error: 42.531620, mean_q: -63.033859\n",
            " 43045/50000: episode: 218, duration: 0.764s, episode steps: 200, steps per second: 262, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.265 [-1.070, 0.041], loss: 5.258951, mean_absolute_error: 42.567051, mean_q: -63.223427\n",
            " 43245/50000: episode: 219, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000], mean observation: -0.239 [-0.818, 0.025], loss: 10.582071, mean_absolute_error: 42.632950, mean_q: -63.180233\n",
            " 43445/50000: episode: 220, duration: 0.670s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.715 [0.000, 2.000], mean observation: -0.290 [-1.053, 0.021], loss: 7.462174, mean_absolute_error: 42.654385, mean_q: -63.274940\n",
            " 43645/50000: episode: 221, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.243 [-0.822, 0.017], loss: 9.759851, mean_absolute_error: 42.652576, mean_q: -63.244980\n",
            " 43845/50000: episode: 222, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.730 [0.000, 2.000], mean observation: -0.262 [-1.200, 0.182], loss: 8.539242, mean_absolute_error: 42.647694, mean_q: -63.237202\n",
            " 44045/50000: episode: 223, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000], mean observation: -0.300 [-1.168, 0.044], loss: 7.853314, mean_absolute_error: 42.791431, mean_q: -63.500675\n",
            " 44245/50000: episode: 224, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.205 [-1.200, 0.268], loss: 7.178627, mean_absolute_error: 42.707874, mean_q: -63.396503\n",
            " 44445/50000: episode: 225, duration: 0.679s, episode steps: 200, steps per second: 294, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.575 [0.000, 2.000], mean observation: -0.275 [-0.816, 0.032], loss: 8.344945, mean_absolute_error: 42.687550, mean_q: -63.339859\n",
            " 44645/50000: episode: 226, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 2.000], mean observation: -0.217 [-0.688, 0.022], loss: 9.031205, mean_absolute_error: 42.708729, mean_q: -63.349422\n",
            " 44792/50000: episode: 227, duration: 0.489s, episode steps: 147, steps per second: 300, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.048 [0.000, 2.000], mean observation: -0.200 [-1.067, 0.503], loss: 7.412959, mean_absolute_error: 42.677422, mean_q: -63.298920\n",
            " 44992/50000: episode: 228, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000], mean observation: -0.266 [-0.850, 0.021], loss: 12.000233, mean_absolute_error: 42.673981, mean_q: -63.123329\n",
            " 45192/50000: episode: 229, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.750 [0.000, 2.000], mean observation: -0.200 [-1.200, 0.395], loss: 9.171279, mean_absolute_error: 42.502968, mean_q: -63.077091\n",
            " 45392/50000: episode: 230, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.705 [0.000, 2.000], mean observation: -0.261 [-0.724, 0.028], loss: 8.406391, mean_absolute_error: 42.486874, mean_q: -63.042934\n",
            " 45592/50000: episode: 231, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.630 [0.000, 2.000], mean observation: -0.222 [-0.686, 0.013], loss: 8.159739, mean_absolute_error: 42.439354, mean_q: -63.014530\n",
            " 45792/50000: episode: 232, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000], mean observation: -0.292 [-1.200, 0.034], loss: 9.573335, mean_absolute_error: 42.417404, mean_q: -62.897003\n",
            " 45992/50000: episode: 233, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.284 [-1.200, 0.055], loss: 6.211766, mean_absolute_error: 42.441059, mean_q: -63.093250\n",
            " 46192/50000: episode: 234, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000], mean observation: -0.286 [-1.182, 0.111], loss: 8.376471, mean_absolute_error: 42.586971, mean_q: -63.168549\n",
            " 46376/50000: episode: 235, duration: 0.612s, episode steps: 184, steps per second: 301, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.092 [0.000, 2.000], mean observation: -0.238 [-1.188, 0.505], loss: 8.200684, mean_absolute_error: 42.633762, mean_q: -63.278988\n",
            " 46576/50000: episode: 236, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.180 [0.000, 2.000], mean observation: -0.318 [-0.827, 0.013], loss: 6.689529, mean_absolute_error: 42.563301, mean_q: -63.197350\n",
            " 46727/50000: episode: 237, duration: 0.502s, episode steps: 151, steps per second: 301, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.881 [0.000, 2.000], mean observation: -0.269 [-1.200, 0.504], loss: 10.036544, mean_absolute_error: 42.421371, mean_q: -62.820869\n",
            " 46927/50000: episode: 238, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.241 [-1.200, 0.120], loss: 9.165191, mean_absolute_error: 42.517139, mean_q: -62.998383\n",
            " 47127/50000: episode: 239, duration: 0.671s, episode steps: 200, steps per second: 298, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.243 [-1.200, 0.236], loss: 6.951638, mean_absolute_error: 42.480186, mean_q: -63.126884\n",
            " 47327/50000: episode: 240, duration: 0.668s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.310 [0.000, 2.000], mean observation: -0.316 [-1.200, 0.035], loss: 8.768567, mean_absolute_error: 42.489994, mean_q: -63.006893\n",
            " 47527/50000: episode: 241, duration: 0.665s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.520 [0.000, 2.000], mean observation: -0.286 [-1.200, 0.059], loss: 6.639121, mean_absolute_error: 42.383785, mean_q: -62.964767\n",
            " 47727/50000: episode: 242, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.247 [-0.932, 0.024], loss: 10.065023, mean_absolute_error: 42.363491, mean_q: -62.802696\n",
            " 47927/50000: episode: 243, duration: 0.681s, episode steps: 200, steps per second: 294, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000], mean observation: -0.260 [-0.963, 0.024], loss: 9.346580, mean_absolute_error: 42.315899, mean_q: -62.789356\n",
            " 48104/50000: episode: 244, duration: 0.593s, episode steps: 177, steps per second: 298, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.582 [0.000, 2.000], mean observation: -0.302 [-1.194, 0.504], loss: 10.671309, mean_absolute_error: 42.353062, mean_q: -62.756763\n",
            " 48304/50000: episode: 245, duration: 0.675s, episode steps: 200, steps per second: 296, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 2.000], mean observation: -0.227 [-0.784, 0.025], loss: 8.432452, mean_absolute_error: 42.215378, mean_q: -62.546017\n",
            " 48504/50000: episode: 246, duration: 0.675s, episode steps: 200, steps per second: 296, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000], mean observation: -0.260 [-1.013, 0.140], loss: 8.688944, mean_absolute_error: 42.359898, mean_q: -62.853195\n",
            " 48704/50000: episode: 247, duration: 0.662s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.220 [-0.984, 0.100], loss: 8.393069, mean_absolute_error: 42.262012, mean_q: -62.737518\n",
            " 48904/50000: episode: 248, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.338], loss: 10.411612, mean_absolute_error: 42.227814, mean_q: -62.613976\n",
            " 49104/50000: episode: 249, duration: 0.667s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000], mean observation: -0.225 [-0.798, 0.038], loss: 7.524013, mean_absolute_error: 42.320179, mean_q: -62.790985\n",
            " 49304/50000: episode: 250, duration: 0.666s, episode steps: 200, steps per second: 300, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000], mean observation: -0.264 [-1.200, 0.142], loss: 9.309267, mean_absolute_error: 42.279510, mean_q: -62.691666\n",
            " 49504/50000: episode: 251, duration: 0.663s, episode steps: 200, steps per second: 302, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.575 [0.000, 2.000], mean observation: -0.200 [-1.200, 0.190], loss: 13.961621, mean_absolute_error: 42.102283, mean_q: -62.275719\n",
            " 49623/50000: episode: 252, duration: 0.398s, episode steps: 119, steps per second: 299, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.328 [0.000, 2.000], mean observation: -0.232 [-1.200, 0.538], loss: 12.342926, mean_absolute_error: 41.884701, mean_q: -61.968002\n",
            " 49823/50000: episode: 253, duration: 0.669s, episode steps: 200, steps per second: 299, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000], mean observation: -0.244 [-0.802, 0.012], loss: 6.717562, mean_absolute_error: 41.825199, mean_q: -62.096188\n",
            "done, took 186.667 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "46yjN9D0mhiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 実行"
      ]
    },
    {
      "metadata": {
        "id": "oUcwbR4Q0x80",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## test  \n",
        "ローカル上ではこれでアニメーションも表示される？  \n",
        "colab上では実行してもしなくても同じ"
      ]
    },
    {
      "metadata": {
        "id": "h9Ur8kWrOpv_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes=1, visualize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bb9nQkyy0feo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ダウンロード  \n",
        "これで学習の途中経過をダウンロードできる"
      ]
    },
    {
      "metadata": {
        "id": "QTavTeUHwZdG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import glob\n",
        "\n",
        "for file in glob.glob(\"openaigym.video.*.mp4\"):\n",
        "  files.download(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bMgPDkNsw7ju",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 参考\n",
        "色々役に立ちそうなkerasのcallback[https://keras.io/ja/callbacks/]"
      ]
    },
    {
      "metadata": {
        "id": "v0l5nywYw9SZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
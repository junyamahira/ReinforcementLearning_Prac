{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_MountainCar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junyamahira/ReinforcementLearning_Prac/blob/master/DQN_MountainCar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ap27433nXCn9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# git clone \n",
        "- これかpipのどちらかを実行すればよい\n",
        "- keras-rlの中に入り、setup.pyを実行する"
      ]
    },
    {
      "metadata": {
        "id": "xLXt4VtkXLwh",
        "colab_type": "code",
        "outputId": "52769e99-8b76-47a6-e1b8-3947166fbb41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/keras-rl/keras-rl.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-rl'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 1710 (delta 3), reused 7 (delta 2), pack-reused 1698\u001b[K\n",
            "Receiving objects: 100% (1710/1710), 1.38 MiB | 19.35 MiB/s, done.\n",
            "Resolving deltas: 100% (1056/1056), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IaD-jZAAXa8S",
        "colab_type": "code",
        "outputId": "92ca32d0-eb5f-4124-f1d2-509fc1d6b118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!ls keras-rl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assets\t\t examples\t    mkdocs.yml\trl\t   tests\n",
            "CONTRIBUTING.md  ISSUE_TEMPLATE.md  pytest.ini\tsetup.cfg  utils\n",
            "docs\t\t LICENSE\t    README.md\tsetup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8cw_CkyZFsd",
        "colab_type": "code",
        "outputId": "be46aaca-71ad-4f27-c88e-7971eeb08fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!cd keras-rl\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keras-rl  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-_L-L0I2a-uQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# pip"
      ]
    },
    {
      "metadata": {
        "id": "DUyHjkEPbBtS",
        "colab_type": "code",
        "outputId": "b50b2ad7-f08f-4148-d972-caa7e6d64a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "pip install keras-rl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\u001b[K    100% |████████████████████████████████| 40kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.2.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.9)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JUcjN-baSzPe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# import"
      ]
    },
    {
      "metadata": {
        "id": "ZG-NDV4AQIFS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5d589e1-9212-4a1c-e043-5b09cd80934e"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "import gym\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGIdiFuXT1rk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MountainCar-v0\")\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xsV75LWXtMLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# モデル"
      ]
    },
    {
      "metadata": {
        "id": "vFGewr_orPkl",
        "colab_type": "code",
        "outputId": "370aa9ea-a6f2-4818-8a20-3a4bfbece079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F_kWc71Kse2-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5689
        },
        "outputId": "32bb62a2-a680-471c-def0-f14952fe07b4"
      },
      "cell_type": "code",
      "source": [
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "\n",
        "policy = EpsGreedyQPolicy(eps=0.001)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions,gamma=0.99, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   200/50000: episode: 1, duration: 1.430s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000], mean observation: -0.233 [-1.200, 0.092], loss: 0.200101, mean_absolute_error: 0.638835, mean_q: -0.618114\n",
            "   400/50000: episode: 2, duration: 0.611s, episode steps: 200, steps per second: 327, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.770 [0.000, 2.000], mean observation: -0.275 [-0.704, 0.018], loss: 0.002689, mean_absolute_error: 1.501911, mean_q: -2.210117\n",
            "   600/50000: episode: 3, duration: 0.597s, episode steps: 200, steps per second: 335, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.730 [0.000, 2.000], mean observation: -0.284 [-0.737, 0.023], loss: 0.017980, mean_absolute_error: 2.541771, mean_q: -3.754195\n",
            "   800/50000: episode: 4, duration: 0.568s, episode steps: 200, steps per second: 352, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.228 [-0.985, 0.056], loss: 0.032359, mean_absolute_error: 3.672266, mean_q: -5.431519\n",
            "  1000/50000: episode: 5, duration: 0.568s, episode steps: 200, steps per second: 352, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000], mean observation: -0.267 [-0.739, 0.024], loss: 0.089432, mean_absolute_error: 4.813484, mean_q: -7.109255\n",
            "  1200/50000: episode: 6, duration: 0.569s, episode steps: 200, steps per second: 352, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000], mean observation: -0.245 [-0.949, 0.047], loss: 0.145844, mean_absolute_error: 5.940130, mean_q: -8.773721\n",
            "  1400/50000: episode: 7, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000], mean observation: -0.242 [-0.676, 0.018], loss: 0.244290, mean_absolute_error: 7.028189, mean_q: -10.359668\n",
            "  1600/50000: episode: 8, duration: 0.571s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000], mean observation: -0.266 [-0.816, 0.030], loss: 0.290718, mean_absolute_error: 8.103985, mean_q: -11.990530\n",
            "  1800/50000: episode: 9, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.735 [0.000, 2.000], mean observation: -0.277 [-0.909, 0.038], loss: 0.418597, mean_absolute_error: 9.157326, mean_q: -13.491738\n",
            "  1977/50000: episode: 10, duration: 0.508s, episode steps: 177, steps per second: 349, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.525], loss: 0.454227, mean_absolute_error: 10.108831, mean_q: -14.949539\n",
            "  2177/50000: episode: 11, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000], mean observation: -0.250 [-0.800, 0.029], loss: 0.581317, mean_absolute_error: 11.070327, mean_q: -16.357857\n",
            "  2377/50000: episode: 12, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.258 [-0.798, 0.026], loss: 0.602920, mean_absolute_error: 12.044407, mean_q: -17.840326\n",
            "  2577/50000: episode: 13, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000], mean observation: -0.282 [-0.832, 0.036], loss: 0.808192, mean_absolute_error: 12.993628, mean_q: -19.231865\n",
            "  2777/50000: episode: 14, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.755 [0.000, 2.000], mean observation: -0.271 [-0.883, 0.028], loss: 1.042596, mean_absolute_error: 13.926940, mean_q: -20.642447\n",
            "  2977/50000: episode: 15, duration: 0.586s, episode steps: 200, steps per second: 341, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000], mean observation: -0.259 [-1.040, 0.042], loss: 1.348500, mean_absolute_error: 14.794070, mean_q: -21.858652\n",
            "  3177/50000: episode: 16, duration: 0.577s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.480 [0.000, 2.000], mean observation: -0.206 [-0.832, 0.039], loss: 1.744003, mean_absolute_error: 15.528245, mean_q: -22.891357\n",
            "  3377/50000: episode: 17, duration: 0.576s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.215 [-1.200, 0.238], loss: 1.247039, mean_absolute_error: 16.356291, mean_q: -24.239008\n",
            "  3557/50000: episode: 18, duration: 0.517s, episode steps: 180, steps per second: 348, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000], mean observation: -0.209 [-1.001, 0.508], loss: 1.526819, mean_absolute_error: 17.138144, mean_q: -25.362993\n",
            "  3757/50000: episode: 19, duration: 0.579s, episode steps: 200, steps per second: 345, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000], mean observation: -0.258 [-1.015, 0.050], loss: 1.648241, mean_absolute_error: 17.871885, mean_q: -26.443184\n",
            "  3957/50000: episode: 20, duration: 0.572s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000], mean observation: -0.266 [-0.775, 0.013], loss: 1.976740, mean_absolute_error: 18.656454, mean_q: -27.592239\n",
            "  4141/50000: episode: 21, duration: 0.530s, episode steps: 184, steps per second: 347, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.272 [0.000, 2.000], mean observation: -0.198 [-1.200, 0.510], loss: 2.005175, mean_absolute_error: 19.346621, mean_q: -28.615967\n",
            "  4341/50000: episode: 22, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000], mean observation: -0.255 [-0.742, 0.030], loss: 1.791910, mean_absolute_error: 20.011377, mean_q: -29.667608\n",
            "  4510/50000: episode: 23, duration: 0.489s, episode steps: 169, steps per second: 346, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.278 [0.000, 2.000], mean observation: -0.224 [-1.200, 0.534], loss: 2.318456, mean_absolute_error: 20.588377, mean_q: -30.422014\n",
            "  4710/50000: episode: 24, duration: 0.572s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.237 [-0.840, 0.038], loss: 2.657496, mean_absolute_error: 21.179136, mean_q: -31.332253\n",
            "  4880/50000: episode: 25, duration: 0.490s, episode steps: 170, steps per second: 347, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.216 [-1.092, 0.501], loss: 1.985934, mean_absolute_error: 21.841131, mean_q: -32.332119\n",
            "  5080/50000: episode: 26, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.450 [0.000, 2.000], mean observation: -0.227 [-0.865, 0.029], loss: 2.578224, mean_absolute_error: 22.338591, mean_q: -33.063404\n",
            "  5280/50000: episode: 27, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000], mean observation: -0.265 [-0.746, 0.031], loss: 2.824425, mean_absolute_error: 23.008818, mean_q: -34.075169\n",
            "  5480/50000: episode: 28, duration: 0.570s, episode steps: 200, steps per second: 351, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000], mean observation: -0.195 [-1.107, 0.321], loss: 3.297313, mean_absolute_error: 23.582016, mean_q: -34.824318\n",
            "  5638/50000: episode: 29, duration: 0.454s, episode steps: 158, steps per second: 348, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.051 [0.000, 2.000], mean observation: -0.214 [-1.053, 0.507], loss: 1.877514, mean_absolute_error: 24.014856, mean_q: -35.546276\n",
            "  5838/50000: episode: 30, duration: 0.591s, episode steps: 200, steps per second: 338, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.355 [0.000, 2.000], mean observation: -0.215 [-0.970, 0.046], loss: 2.366959, mean_absolute_error: 24.571377, mean_q: -36.394623\n",
            "  6038/50000: episode: 31, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.249 [-1.021, 0.040], loss: 3.341477, mean_absolute_error: 25.101946, mean_q: -37.108501\n",
            "  6238/50000: episode: 32, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000], mean observation: -0.229 [-0.985, 0.122], loss: 3.164332, mean_absolute_error: 25.596113, mean_q: -37.818153\n",
            "  6431/50000: episode: 33, duration: 0.555s, episode steps: 193, steps per second: 348, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.301 [0.000, 2.000], mean observation: -0.202 [-1.024, 0.503], loss: 3.217314, mean_absolute_error: 26.088511, mean_q: -38.504822\n",
            "  6631/50000: episode: 34, duration: 0.570s, episode steps: 200, steps per second: 351, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.465 [0.000, 2.000], mean observation: -0.195 [-0.856, 0.135], loss: 1.764485, mean_absolute_error: 26.413677, mean_q: -39.129787\n",
            "  6831/50000: episode: 35, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000], mean observation: -0.239 [-0.929, 0.039], loss: 2.960867, mean_absolute_error: 26.966419, mean_q: -39.864155\n",
            "  7031/50000: episode: 36, duration: 0.567s, episode steps: 200, steps per second: 353, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000], mean observation: -0.231 [-0.647, 0.022], loss: 3.414288, mean_absolute_error: 27.503633, mean_q: -40.645519\n",
            "  7231/50000: episode: 37, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000], mean observation: -0.252 [-0.807, 0.018], loss: 3.621176, mean_absolute_error: 28.001846, mean_q: -41.392685\n",
            "  7431/50000: episode: 38, duration: 0.574s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.264 [-0.910, 0.042], loss: 3.456342, mean_absolute_error: 28.318359, mean_q: -41.810841\n",
            "  7631/50000: episode: 39, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000], mean observation: -0.224 [-1.200, 0.226], loss: 3.325820, mean_absolute_error: 28.801420, mean_q: -42.557301\n",
            "  7831/50000: episode: 40, duration: 0.570s, episode steps: 200, steps per second: 351, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000], mean observation: -0.239 [-0.651, 0.023], loss: 2.429714, mean_absolute_error: 29.301550, mean_q: -43.314747\n",
            "  8002/50000: episode: 41, duration: 0.486s, episode steps: 171, steps per second: 352, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.263 [0.000, 2.000], mean observation: -0.197 [-1.200, 0.504], loss: 3.561138, mean_absolute_error: 29.689728, mean_q: -43.825733\n",
            "  8202/50000: episode: 42, duration: 0.574s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 2.000], mean observation: -0.246 [-0.720, 0.023], loss: 3.228719, mean_absolute_error: 29.963707, mean_q: -44.284054\n",
            "  8402/50000: episode: 43, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000], mean observation: -0.234 [-0.940, 0.033], loss: 3.735707, mean_absolute_error: 30.304546, mean_q: -44.763092\n",
            "  8602/50000: episode: 44, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.520 [0.000, 2.000], mean observation: -0.228 [-0.866, 0.038], loss: 4.189311, mean_absolute_error: 30.621250, mean_q: -45.172832\n",
            "  8802/50000: episode: 45, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000], mean observation: -0.255 [-0.790, 0.031], loss: 4.053092, mean_absolute_error: 31.166119, mean_q: -46.107304\n",
            "  9002/50000: episode: 46, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.440 [0.000, 2.000], mean observation: -0.225 [-0.959, 0.035], loss: 3.752812, mean_absolute_error: 31.386314, mean_q: -46.341110\n",
            "  9202/50000: episode: 47, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000], mean observation: -0.253 [-0.851, 0.024], loss: 6.254263, mean_absolute_error: 31.693960, mean_q: -46.719967\n",
            "  9402/50000: episode: 48, duration: 0.571s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.410 [0.000, 2.000], mean observation: -0.241 [-0.963, 0.042], loss: 3.421829, mean_absolute_error: 32.082394, mean_q: -47.482311\n",
            "  9602/50000: episode: 49, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 2.000], mean observation: -0.246 [-1.069, 0.068], loss: 4.955098, mean_absolute_error: 32.426216, mean_q: -47.936680\n",
            "  9802/50000: episode: 50, duration: 0.585s, episode steps: 200, steps per second: 342, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.530 [0.000, 2.000], mean observation: -0.222 [-1.094, 0.369], loss: 4.955162, mean_absolute_error: 32.676426, mean_q: -48.221138\n",
            " 10002/50000: episode: 51, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.255 [-0.833, 0.025], loss: 3.912536, mean_absolute_error: 33.037472, mean_q: -48.884586\n",
            " 10202/50000: episode: 52, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000], mean observation: -0.237 [-0.805, 0.026], loss: 4.189075, mean_absolute_error: 33.471188, mean_q: -49.535995\n",
            " 10402/50000: episode: 53, duration: 0.579s, episode steps: 200, steps per second: 346, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000], mean observation: -0.243 [-0.674, 0.013], loss: 6.518003, mean_absolute_error: 33.705658, mean_q: -49.754425\n",
            " 10602/50000: episode: 54, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.760 [0.000, 2.000], mean observation: -0.177 [-0.789, 0.113], loss: 4.661745, mean_absolute_error: 33.781593, mean_q: -49.947613\n",
            " 10802/50000: episode: 55, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.209 [-0.864, 0.215], loss: 7.673247, mean_absolute_error: 33.997879, mean_q: -50.140350\n",
            " 11002/50000: episode: 56, duration: 0.577s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.171 [-1.200, 0.373], loss: 6.221045, mean_absolute_error: 34.165077, mean_q: -50.488415\n",
            " 11202/50000: episode: 57, duration: 0.574s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.179], loss: 4.504457, mean_absolute_error: 34.472240, mean_q: -51.016701\n",
            " 11402/50000: episode: 58, duration: 0.574s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.405 [0.000, 2.000], mean observation: -0.234 [-0.672, 0.020], loss: 5.785335, mean_absolute_error: 34.592670, mean_q: -51.169315\n",
            " 11601/50000: episode: 59, duration: 0.570s, episode steps: 199, steps per second: 349, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000], mean observation: -0.211 [-1.087, 0.525], loss: 4.214635, mean_absolute_error: 34.841076, mean_q: -51.557682\n",
            " 11801/50000: episode: 60, duration: 0.580s, episode steps: 200, steps per second: 345, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000], mean observation: -0.207 [-0.917, 0.041], loss: 5.191108, mean_absolute_error: 35.108017, mean_q: -51.900028\n",
            " 12001/50000: episode: 61, duration: 0.570s, episode steps: 200, steps per second: 351, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000], mean observation: -0.268 [-0.714, 0.018], loss: 5.553503, mean_absolute_error: 35.389565, mean_q: -52.306202\n",
            " 12201/50000: episode: 62, duration: 0.572s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.229 [-0.843, 0.151], loss: 4.995423, mean_absolute_error: 35.550591, mean_q: -52.554131\n",
            " 12401/50000: episode: 63, duration: 0.570s, episode steps: 200, steps per second: 351, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000], mean observation: -0.283 [-0.799, 0.016], loss: 6.234340, mean_absolute_error: 35.704453, mean_q: -52.716759\n",
            " 12601/50000: episode: 64, duration: 0.577s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000], mean observation: -0.249 [-0.726, 0.019], loss: 4.435873, mean_absolute_error: 36.030388, mean_q: -53.277676\n",
            " 12801/50000: episode: 65, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.215 [-0.773, 0.110], loss: 3.396319, mean_absolute_error: 36.221802, mean_q: -53.622921\n",
            " 13001/50000: episode: 66, duration: 0.571s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.395 [0.000, 2.000], mean observation: -0.218 [-0.730, 0.025], loss: 8.358595, mean_absolute_error: 36.446617, mean_q: -53.757172\n",
            " 13191/50000: episode: 67, duration: 0.545s, episode steps: 190, steps per second: 349, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.837 [0.000, 2.000], mean observation: -0.267 [-1.020, 0.510], loss: 6.279155, mean_absolute_error: 36.427605, mean_q: -53.744408\n",
            " 13348/50000: episode: 68, duration: 0.450s, episode steps: 157, steps per second: 349, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.580 [0.000, 2.000], mean observation: -0.184 [-0.941, 0.508], loss: 4.386993, mean_absolute_error: 36.566601, mean_q: -54.093204\n",
            " 13548/50000: episode: 69, duration: 0.584s, episode steps: 200, steps per second: 342, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.245 [0.000, 2.000], mean observation: -0.244 [-0.644, 0.020], loss: 7.419196, mean_absolute_error: 36.648891, mean_q: -54.086349\n",
            " 13692/50000: episode: 70, duration: 0.414s, episode steps: 144, steps per second: 348, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.194 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.523], loss: 5.662934, mean_absolute_error: 36.685722, mean_q: -54.201691\n",
            " 13892/50000: episode: 71, duration: 0.577s, episode steps: 200, steps per second: 346, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.685 [0.000, 2.000], mean observation: -0.190 [-0.732, 0.027], loss: 7.918818, mean_absolute_error: 36.853382, mean_q: -54.353138\n",
            " 14085/50000: episode: 72, duration: 0.553s, episode steps: 193, steps per second: 349, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000], mean observation: -0.198 [-0.928, 0.507], loss: 5.589266, mean_absolute_error: 36.799885, mean_q: -54.319626\n",
            " 14269/50000: episode: 73, duration: 0.534s, episode steps: 184, steps per second: 344, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000], mean observation: -0.211 [-0.930, 0.521], loss: 6.798637, mean_absolute_error: 36.938202, mean_q: -54.457699\n",
            " 14455/50000: episode: 74, duration: 0.531s, episode steps: 186, steps per second: 350, episode reward: -186.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.124 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.514], loss: 4.196996, mean_absolute_error: 36.893372, mean_q: -54.589993\n",
            " 14614/50000: episode: 75, duration: 0.482s, episode steps: 159, steps per second: 330, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.231 [-1.003, 0.501], loss: 8.255678, mean_absolute_error: 36.967125, mean_q: -54.532345\n",
            " 14814/50000: episode: 76, duration: 0.598s, episode steps: 200, steps per second: 334, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000], mean observation: -0.240 [-0.728, 0.027], loss: 6.089247, mean_absolute_error: 37.043655, mean_q: -54.652061\n",
            " 14976/50000: episode: 77, duration: 0.467s, episode steps: 162, steps per second: 347, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.370 [0.000, 2.000], mean observation: -0.214 [-1.096, 0.530], loss: 4.766608, mean_absolute_error: 37.074493, mean_q: -54.817318\n",
            " 15103/50000: episode: 78, duration: 0.367s, episode steps: 127, steps per second: 346, episode reward: -127.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.291 [0.000, 2.000], mean observation: -0.241 [-1.146, 0.519], loss: 7.145968, mean_absolute_error: 37.238636, mean_q: -54.913071\n",
            " 15268/50000: episode: 79, duration: 0.477s, episode steps: 165, steps per second: 346, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000], mean observation: -0.276 [-1.175, 0.519], loss: 7.166128, mean_absolute_error: 37.197662, mean_q: -54.845657\n",
            " 15365/50000: episode: 80, duration: 0.284s, episode steps: 97, steps per second: 342, episode reward: -97.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.141 [-0.871, 0.506], loss: 5.699423, mean_absolute_error: 37.043808, mean_q: -54.603985\n",
            " 15557/50000: episode: 81, duration: 0.550s, episode steps: 192, steps per second: 349, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.974 [0.000, 2.000], mean observation: -0.251 [-1.059, 0.524], loss: 6.354733, mean_absolute_error: 37.222549, mean_q: -54.954617\n",
            " 15750/50000: episode: 82, duration: 0.553s, episode steps: 193, steps per second: 349, episode reward: -193.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000], mean observation: -0.257 [-1.022, 0.501], loss: 5.398926, mean_absolute_error: 37.214172, mean_q: -54.923737\n",
            " 15950/50000: episode: 83, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.760 [0.000, 2.000], mean observation: -0.260 [-0.838, 0.074], loss: 4.955064, mean_absolute_error: 37.091785, mean_q: -54.735329\n",
            " 16140/50000: episode: 84, duration: 0.544s, episode steps: 190, steps per second: 349, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000], mean observation: -0.256 [-1.060, 0.520], loss: 5.349069, mean_absolute_error: 37.146183, mean_q: -54.841576\n",
            " 16309/50000: episode: 85, duration: 0.485s, episode steps: 169, steps per second: 349, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.456 [0.000, 2.000], mean observation: -0.175 [-0.909, 0.521], loss: 5.653275, mean_absolute_error: 37.311378, mean_q: -54.944820\n",
            " 16509/50000: episode: 86, duration: 0.578s, episode steps: 200, steps per second: 346, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.720 [0.000, 2.000], mean observation: -0.261 [-0.923, 0.029], loss: 6.365369, mean_absolute_error: 37.229137, mean_q: -54.806007\n",
            " 16637/50000: episode: 87, duration: 0.377s, episode steps: 128, steps per second: 340, episode reward: -128.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.320 [0.000, 2.000], mean observation: -0.244 [-1.108, 0.505], loss: 6.750388, mean_absolute_error: 37.252396, mean_q: -54.930069\n",
            " 16837/50000: episode: 88, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.770 [0.000, 2.000], mean observation: -0.202 [-0.563, 0.010], loss: 5.237485, mean_absolute_error: 37.359859, mean_q: -55.018585\n",
            " 17037/50000: episode: 89, duration: 0.574s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.540 [0.000, 2.000], mean observation: -0.307 [-1.093, 0.031], loss: 6.512651, mean_absolute_error: 37.328968, mean_q: -54.963837\n",
            " 17195/50000: episode: 90, duration: 0.455s, episode steps: 158, steps per second: 347, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.475 [0.000, 2.000], mean observation: -0.060 [-0.902, 0.509], loss: 5.870505, mean_absolute_error: 37.564800, mean_q: -55.355679\n",
            " 17362/50000: episode: 91, duration: 0.483s, episode steps: 167, steps per second: 346, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.018 [0.000, 2.000], mean observation: -0.264 [-1.200, 0.537], loss: 7.775086, mean_absolute_error: 37.338020, mean_q: -54.912189\n",
            " 17540/50000: episode: 92, duration: 0.508s, episode steps: 178, steps per second: 350, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.263 [-1.200, 0.537], loss: 6.286909, mean_absolute_error: 37.308609, mean_q: -55.016636\n",
            " 17740/50000: episode: 93, duration: 0.574s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.595 [0.000, 2.000], mean observation: -0.287 [-1.042, 0.419], loss: 5.545007, mean_absolute_error: 37.418159, mean_q: -55.175411\n",
            " 17940/50000: episode: 94, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000], mean observation: -0.259 [-0.766, 0.012], loss: 8.487616, mean_absolute_error: 37.324608, mean_q: -54.944424\n",
            " 18140/50000: episode: 95, duration: 0.574s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.240 [0.000, 2.000], mean observation: -0.325 [-0.873, 0.018], loss: 6.610188, mean_absolute_error: 37.273632, mean_q: -54.952389\n",
            " 18314/50000: episode: 96, duration: 0.500s, episode steps: 174, steps per second: 348, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.023 [0.000, 2.000], mean observation: -0.229 [-1.200, 0.537], loss: 6.999677, mean_absolute_error: 37.308022, mean_q: -54.975563\n",
            " 18487/50000: episode: 97, duration: 0.501s, episode steps: 173, steps per second: 345, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.231 [0.000, 2.000], mean observation: -0.221 [-0.993, 0.509], loss: 6.809001, mean_absolute_error: 37.317547, mean_q: -54.992825\n",
            " 18644/50000: episode: 98, duration: 0.453s, episode steps: 157, steps per second: 347, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000], mean observation: -0.230 [-1.200, 0.517], loss: 4.863678, mean_absolute_error: 37.393185, mean_q: -55.147202\n",
            " 18844/50000: episode: 99, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.470 [0.000, 2.000], mean observation: -0.298 [-0.745, 0.017], loss: 6.350968, mean_absolute_error: 37.315083, mean_q: -54.998035\n",
            " 19022/50000: episode: 100, duration: 0.514s, episode steps: 178, steps per second: 347, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.261 [-1.150, 0.503], loss: 4.460745, mean_absolute_error: 37.593140, mean_q: -55.476749\n",
            " 19222/50000: episode: 101, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.185 [0.000, 2.000], mean observation: -0.319 [-0.947, 0.021], loss: 4.996299, mean_absolute_error: 37.330479, mean_q: -55.020752\n",
            " 19327/50000: episode: 102, duration: 0.304s, episode steps: 105, steps per second: 346, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.314 [0.000, 2.000], mean observation: -0.230 [-1.037, 0.507], loss: 5.186604, mean_absolute_error: 37.559742, mean_q: -55.415951\n",
            " 19472/50000: episode: 103, duration: 0.420s, episode steps: 145, steps per second: 345, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.014 [0.000, 2.000], mean observation: -0.185 [-0.970, 0.503], loss: 6.258902, mean_absolute_error: 37.534760, mean_q: -55.351967\n",
            " 19639/50000: episode: 104, duration: 0.479s, episode steps: 167, steps per second: 348, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.102 [0.000, 2.000], mean observation: -0.241 [-1.006, 0.510], loss: 7.571444, mean_absolute_error: 37.530064, mean_q: -55.219990\n",
            " 19788/50000: episode: 105, duration: 0.431s, episode steps: 149, steps per second: 346, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.168 [0.000, 2.000], mean observation: -0.206 [-0.921, 0.509], loss: 7.806642, mean_absolute_error: 37.553867, mean_q: -55.380173\n",
            " 19906/50000: episode: 106, duration: 0.346s, episode steps: 118, steps per second: 341, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.381 [0.000, 2.000], mean observation: -0.248 [-1.141, 0.507], loss: 4.732207, mean_absolute_error: 37.478184, mean_q: -55.277328\n",
            " 20106/50000: episode: 107, duration: 0.573s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.555 [0.000, 2.000], mean observation: -0.299 [-0.915, 0.037], loss: 7.062250, mean_absolute_error: 37.568596, mean_q: -55.360519\n",
            " 20259/50000: episode: 108, duration: 0.438s, episode steps: 153, steps per second: 349, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.928 [0.000, 2.000], mean observation: -0.265 [-1.069, 0.528], loss: 6.371315, mean_absolute_error: 37.541103, mean_q: -55.218201\n",
            " 20426/50000: episode: 109, duration: 0.483s, episode steps: 167, steps per second: 346, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.749 [0.000, 2.000], mean observation: -0.287 [-1.200, 0.506], loss: 4.632385, mean_absolute_error: 37.424828, mean_q: -55.196095\n",
            " 20578/50000: episode: 110, duration: 0.439s, episode steps: 152, steps per second: 346, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.730 [0.000, 2.000], mean observation: -0.293 [-1.144, 0.546], loss: 5.885558, mean_absolute_error: 37.305344, mean_q: -54.898590\n",
            " 20740/50000: episode: 111, duration: 0.467s, episode steps: 162, steps per second: 347, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000], mean observation: -0.285 [-1.118, 0.527], loss: 5.369725, mean_absolute_error: 37.334839, mean_q: -55.066998\n",
            " 20940/50000: episode: 112, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.110 [0.000, 1.000], mean observation: -0.348 [-1.122, 0.030], loss: 5.082380, mean_absolute_error: 36.914257, mean_q: -54.249062\n",
            " 21098/50000: episode: 113, duration: 0.456s, episode steps: 158, steps per second: 346, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000], mean observation: -0.282 [-1.200, 0.537], loss: 7.129067, mean_absolute_error: 37.032139, mean_q: -54.475449\n",
            " 21298/50000: episode: 114, duration: 0.576s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.341 [-0.854, 0.015], loss: 3.094656, mean_absolute_error: 36.988159, mean_q: -54.574596\n",
            " 21461/50000: episode: 115, duration: 0.473s, episode steps: 163, steps per second: 344, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.031 [0.000, 2.000], mean observation: -0.208 [-1.200, 0.509], loss: 4.010952, mean_absolute_error: 37.054100, mean_q: -54.612167\n",
            " 21661/50000: episode: 116, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.385 [0.000, 2.000], mean observation: -0.312 [-1.200, 0.029], loss: 6.287267, mean_absolute_error: 36.741428, mean_q: -54.062466\n",
            " 21833/50000: episode: 117, duration: 0.493s, episode steps: 172, steps per second: 349, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.240 [-1.200, 0.537], loss: 4.714657, mean_absolute_error: 37.011246, mean_q: -54.524265\n",
            " 22033/50000: episode: 118, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.336 [-0.784, 0.010], loss: 6.550395, mean_absolute_error: 36.579628, mean_q: -53.749649\n",
            " 22233/50000: episode: 119, duration: 0.577s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.175 [0.000, 1.000], mean observation: -0.322 [-0.991, 0.025], loss: 4.711190, mean_absolute_error: 36.539005, mean_q: -53.743176\n",
            " 22433/50000: episode: 120, duration: 0.572s, episode steps: 200, steps per second: 349, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.105 [0.000, 1.000], mean observation: -0.327 [-0.808, 0.012], loss: 2.395483, mean_absolute_error: 36.546612, mean_q: -53.878807\n",
            " 22590/50000: episode: 121, duration: 0.450s, episode steps: 157, steps per second: 349, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.650 [0.000, 2.000], mean observation: -0.297 [-1.124, 0.537], loss: 5.647020, mean_absolute_error: 36.518112, mean_q: -53.729828\n",
            " 22770/50000: episode: 122, duration: 0.520s, episode steps: 180, steps per second: 346, episode reward: -180.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.606 [0.000, 2.000], mean observation: -0.291 [-1.067, 0.520], loss: 5.151494, mean_absolute_error: 36.424526, mean_q: -53.677242\n",
            " 22921/50000: episode: 123, duration: 0.432s, episode steps: 151, steps per second: 350, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.702 [0.000, 2.000], mean observation: -0.296 [-1.200, 0.520], loss: 8.240197, mean_absolute_error: 36.380085, mean_q: -53.430756\n",
            " 23118/50000: episode: 124, duration: 0.565s, episode steps: 197, steps per second: 349, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.827 [0.000, 2.000], mean observation: -0.279 [-1.200, 0.537], loss: 5.946724, mean_absolute_error: 36.300869, mean_q: -53.402821\n",
            " 23236/50000: episode: 125, duration: 0.342s, episode steps: 118, steps per second: 345, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.265 [-1.078, 0.513], loss: 5.952303, mean_absolute_error: 36.052715, mean_q: -52.981285\n",
            " 23436/50000: episode: 126, duration: 0.569s, episode steps: 200, steps per second: 351, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.320 [0.000, 2.000], mean observation: -0.290 [-1.200, 0.111], loss: 4.540039, mean_absolute_error: 35.901302, mean_q: -52.947708\n",
            " 23636/50000: episode: 127, duration: 0.576s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.260 [0.000, 2.000], mean observation: -0.343 [-1.065, 0.044], loss: 4.706560, mean_absolute_error: 36.061569, mean_q: -53.089462\n",
            " 23791/50000: episode: 128, duration: 0.460s, episode steps: 155, steps per second: 337, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.690 [0.000, 2.000], mean observation: -0.271 [-1.023, 0.518], loss: 5.409301, mean_absolute_error: 36.040607, mean_q: -53.012505\n",
            " 23991/50000: episode: 129, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.250 [0.000, 2.000], mean observation: -0.329 [-0.850, 0.028], loss: 5.308681, mean_absolute_error: 35.819199, mean_q: -52.659496\n",
            " 24191/50000: episode: 130, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.315 [0.000, 2.000], mean observation: -0.312 [-0.714, 0.011], loss: 2.966362, mean_absolute_error: 35.937618, mean_q: -53.026669\n",
            " 24391/50000: episode: 131, duration: 0.578s, episode steps: 200, steps per second: 346, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.470 [0.000, 2.000], mean observation: -0.313 [-0.916, 0.043], loss: 4.613573, mean_absolute_error: 36.107059, mean_q: -53.161121\n",
            " 24546/50000: episode: 132, duration: 0.447s, episode steps: 155, steps per second: 347, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.665 [0.000, 2.000], mean observation: -0.297 [-1.200, 0.537], loss: 5.859930, mean_absolute_error: 36.170837, mean_q: -53.224602\n",
            " 24709/50000: episode: 133, duration: 0.471s, episode steps: 163, steps per second: 346, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000], mean observation: -0.257 [-1.200, 0.537], loss: 4.031658, mean_absolute_error: 35.958241, mean_q: -52.986172\n",
            " 24802/50000: episode: 134, duration: 0.269s, episode steps: 93, steps per second: 346, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.183 [0.000, 2.000], mean observation: -0.175 [-0.900, 0.511], loss: 4.516469, mean_absolute_error: 35.841064, mean_q: -52.823002\n",
            " 24950/50000: episode: 135, duration: 0.428s, episode steps: 148, steps per second: 346, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.838 [0.000, 2.000], mean observation: -0.278 [-1.107, 0.529], loss: 3.864839, mean_absolute_error: 36.041405, mean_q: -53.092026\n",
            " 25122/50000: episode: 136, duration: 0.496s, episode steps: 172, steps per second: 347, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.198 [0.000, 2.000], mean observation: -0.247 [-1.200, 0.537], loss: 7.931696, mean_absolute_error: 35.988861, mean_q: -52.941326\n",
            " 25322/50000: episode: 137, duration: 0.574s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.190 [0.000, 1.000], mean observation: -0.315 [-0.780, 0.011], loss: 3.433047, mean_absolute_error: 36.119835, mean_q: -53.360939\n",
            " 25455/50000: episode: 138, duration: 0.385s, episode steps: 133, steps per second: 345, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.188 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.537], loss: 5.702447, mean_absolute_error: 36.251869, mean_q: -53.401035\n",
            " 25655/50000: episode: 139, duration: 0.581s, episode steps: 200, steps per second: 344, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.055 [0.000, 1.000], mean observation: -0.334 [-0.918, 0.012], loss: 4.349531, mean_absolute_error: 36.070354, mean_q: -53.204865\n",
            " 25781/50000: episode: 140, duration: 0.366s, episode steps: 126, steps per second: 344, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000], mean observation: -0.240 [-0.981, 0.531], loss: 5.237336, mean_absolute_error: 35.693039, mean_q: -52.546543\n",
            " 25869/50000: episode: 141, duration: 0.255s, episode steps: 88, steps per second: 345, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.023 [0.000, 2.000], mean observation: -0.221 [-0.945, 0.510], loss: 2.567268, mean_absolute_error: 35.711987, mean_q: -52.753052\n",
            " 25955/50000: episode: 142, duration: 0.252s, episode steps: 86, steps per second: 341, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000], mean observation: -0.222 [-0.954, 0.514], loss: 2.827159, mean_absolute_error: 36.063850, mean_q: -53.305035\n",
            " 26155/50000: episode: 143, duration: 0.581s, episode steps: 200, steps per second: 344, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.640 [0.000, 2.000], mean observation: -0.277 [-0.943, 0.405], loss: 5.308827, mean_absolute_error: 36.006702, mean_q: -53.106613\n",
            " 26298/50000: episode: 144, duration: 0.414s, episode steps: 143, steps per second: 345, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.266 [-1.166, 0.526], loss: 4.520566, mean_absolute_error: 36.292953, mean_q: -53.494850\n",
            " 26457/50000: episode: 145, duration: 0.458s, episode steps: 159, steps per second: 347, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.277 [0.000, 2.000], mean observation: -0.229 [-1.200, 0.537], loss: 5.116827, mean_absolute_error: 36.178833, mean_q: -53.358078\n",
            " 26600/50000: episode: 146, duration: 0.410s, episode steps: 143, steps per second: 349, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000], mean observation: -0.240 [-1.200, 0.537], loss: 6.911371, mean_absolute_error: 36.086033, mean_q: -53.174763\n",
            " 26776/50000: episode: 147, duration: 0.508s, episode steps: 176, steps per second: 346, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000], mean observation: -0.286 [-1.155, 0.527], loss: 4.924494, mean_absolute_error: 36.006577, mean_q: -53.153915\n",
            " 26976/50000: episode: 148, duration: 0.576s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.755 [0.000, 2.000], mean observation: -0.240 [-0.824, 0.143], loss: 6.021397, mean_absolute_error: 36.132534, mean_q: -53.264893\n",
            " 27141/50000: episode: 149, duration: 0.480s, episode steps: 165, steps per second: 344, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000], mean observation: -0.240 [-1.200, 0.525], loss: 6.931960, mean_absolute_error: 36.062626, mean_q: -53.174351\n",
            " 27304/50000: episode: 150, duration: 0.469s, episode steps: 163, steps per second: 347, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.537], loss: 3.826471, mean_absolute_error: 35.966789, mean_q: -53.082588\n",
            " 27454/50000: episode: 151, duration: 0.433s, episode steps: 150, steps per second: 346, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.773 [0.000, 2.000], mean observation: -0.271 [-1.039, 0.527], loss: 6.668631, mean_absolute_error: 35.992908, mean_q: -52.973953\n",
            " 27608/50000: episode: 152, duration: 0.444s, episode steps: 154, steps per second: 347, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.818 [0.000, 2.000], mean observation: -0.257 [-0.990, 0.502], loss: 4.878146, mean_absolute_error: 36.205429, mean_q: -53.420937\n",
            " 27784/50000: episode: 153, duration: 0.521s, episode steps: 176, steps per second: 338, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.011 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.519], loss: 2.795544, mean_absolute_error: 36.263004, mean_q: -53.573132\n",
            " 27955/50000: episode: 154, duration: 0.528s, episode steps: 171, steps per second: 324, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.813 [0.000, 2.000], mean observation: -0.274 [-1.200, 0.513], loss: 4.282392, mean_absolute_error: 36.160969, mean_q: -53.320232\n",
            " 28121/50000: episode: 155, duration: 0.516s, episode steps: 166, steps per second: 322, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.729 [0.000, 2.000], mean observation: -0.265 [-1.200, 0.521], loss: 5.591938, mean_absolute_error: 36.131672, mean_q: -53.223362\n",
            " 28214/50000: episode: 156, duration: 0.289s, episode steps: 93, steps per second: 322, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.011 [0.000, 2.000], mean observation: -0.209 [-0.929, 0.512], loss: 5.392176, mean_absolute_error: 35.872849, mean_q: -52.904747\n",
            " 28322/50000: episode: 157, duration: 0.338s, episode steps: 108, steps per second: 319, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000], mean observation: -0.222 [-0.959, 0.511], loss: 3.006094, mean_absolute_error: 36.153507, mean_q: -53.334122\n",
            " 28465/50000: episode: 158, duration: 0.439s, episode steps: 143, steps per second: 325, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000], mean observation: -0.260 [-1.200, 0.513], loss: 4.960481, mean_absolute_error: 36.137383, mean_q: -53.237228\n",
            " 28627/50000: episode: 159, duration: 0.494s, episode steps: 162, steps per second: 328, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000], mean observation: -0.231 [-1.200, 0.540], loss: 5.596450, mean_absolute_error: 36.241867, mean_q: -53.329567\n",
            " 28786/50000: episode: 160, duration: 0.489s, episode steps: 159, steps per second: 325, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.874 [0.000, 2.000], mean observation: -0.285 [-1.148, 0.526], loss: 3.087069, mean_absolute_error: 36.188709, mean_q: -53.379913\n",
            " 28949/50000: episode: 161, duration: 0.503s, episode steps: 163, steps per second: 324, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.742 [0.000, 2.000], mean observation: -0.286 [-1.107, 0.535], loss: 4.446953, mean_absolute_error: 36.116928, mean_q: -53.199902\n",
            " 29126/50000: episode: 162, duration: 0.546s, episode steps: 177, steps per second: 324, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.068 [0.000, 2.000], mean observation: -0.232 [-1.200, 0.505], loss: 3.609319, mean_absolute_error: 36.143879, mean_q: -53.289936\n",
            " 29321/50000: episode: 163, duration: 0.596s, episode steps: 195, steps per second: 327, episode reward: -195.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.831 [0.000, 2.000], mean observation: -0.270 [-1.200, 0.505], loss: 4.536903, mean_absolute_error: 36.051216, mean_q: -52.970249\n",
            " 29498/50000: episode: 164, duration: 0.534s, episode steps: 177, steps per second: 331, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.949 [0.000, 2.000], mean observation: -0.209 [-1.200, 0.505], loss: 5.526089, mean_absolute_error: 35.893467, mean_q: -52.654385\n",
            " 29589/50000: episode: 165, duration: 0.263s, episode steps: 91, steps per second: 346, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.099 [0.000, 2.000], mean observation: -0.195 [-0.908, 0.517], loss: 3.828997, mean_absolute_error: 35.901485, mean_q: -52.820942\n",
            " 29789/50000: episode: 166, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000], mean observation: -0.255 [-1.200, 0.196], loss: 4.105865, mean_absolute_error: 35.937313, mean_q: -52.841084\n",
            " 29901/50000: episode: 167, duration: 0.329s, episode steps: 112, steps per second: 340, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.141 [-0.879, 0.513], loss: 1.923713, mean_absolute_error: 36.152332, mean_q: -53.269024\n",
            " 30070/50000: episode: 168, duration: 0.485s, episode steps: 169, steps per second: 349, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000], mean observation: -0.220 [-1.200, 0.541], loss: 6.103244, mean_absolute_error: 35.736378, mean_q: -52.494499\n",
            " 30246/50000: episode: 169, duration: 0.506s, episode steps: 176, steps per second: 348, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.989 [0.000, 2.000], mean observation: -0.240 [-1.200, 0.521], loss: 4.264547, mean_absolute_error: 35.588638, mean_q: -52.262325\n",
            " 30417/50000: episode: 170, duration: 0.492s, episode steps: 171, steps per second: 348, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.117 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.521], loss: 2.630705, mean_absolute_error: 35.668053, mean_q: -52.473888\n",
            " 30585/50000: episode: 171, duration: 0.483s, episode steps: 168, steps per second: 348, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000], mean observation: -0.210 [-1.200, 0.543], loss: 5.538678, mean_absolute_error: 35.557655, mean_q: -52.170013\n",
            " 30746/50000: episode: 172, duration: 0.460s, episode steps: 161, steps per second: 350, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.963 [0.000, 2.000], mean observation: -0.260 [-1.200, 0.537], loss: 6.418477, mean_absolute_error: 35.508926, mean_q: -51.994102\n",
            " 30868/50000: episode: 173, duration: 0.353s, episode steps: 122, steps per second: 345, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.393 [0.000, 2.000], mean observation: -0.105 [-0.864, 0.501], loss: 5.752618, mean_absolute_error: 35.620148, mean_q: -52.252193\n",
            " 31006/50000: episode: 174, duration: 0.397s, episode steps: 138, steps per second: 347, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.245 [-1.200, 0.531], loss: 4.650890, mean_absolute_error: 35.164368, mean_q: -51.553059\n",
            " 31161/50000: episode: 175, duration: 0.457s, episode steps: 155, steps per second: 339, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.968 [0.000, 2.000], mean observation: -0.264 [-1.200, 0.537], loss: 5.609788, mean_absolute_error: 35.213924, mean_q: -51.555874\n",
            " 31343/50000: episode: 176, duration: 0.526s, episode steps: 182, steps per second: 346, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.874 [0.000, 2.000], mean observation: -0.272 [-1.200, 0.541], loss: 2.705137, mean_absolute_error: 35.272190, mean_q: -51.884182\n",
            " 31443/50000: episode: 177, duration: 0.291s, episode steps: 100, steps per second: 343, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000], mean observation: -0.193 [-0.924, 0.505], loss: 2.131629, mean_absolute_error: 35.048805, mean_q: -51.475906\n",
            " 31601/50000: episode: 178, duration: 0.457s, episode steps: 158, steps per second: 346, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.522], loss: 2.078766, mean_absolute_error: 34.916183, mean_q: -51.297276\n",
            " 31694/50000: episode: 179, duration: 0.271s, episode steps: 93, steps per second: 343, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.043 [0.000, 2.000], mean observation: -0.179 [-0.905, 0.505], loss: 3.061816, mean_absolute_error: 35.020031, mean_q: -51.452808\n",
            " 31836/50000: episode: 180, duration: 0.413s, episode steps: 142, steps per second: 344, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.042 [0.000, 2.000], mean observation: -0.253 [-1.200, 0.526], loss: 4.809834, mean_absolute_error: 35.111851, mean_q: -51.541466\n",
            " 31994/50000: episode: 181, duration: 0.460s, episode steps: 158, steps per second: 344, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.521], loss: 2.023307, mean_absolute_error: 34.892815, mean_q: -51.314541\n",
            " 32171/50000: episode: 182, duration: 0.510s, episode steps: 177, steps per second: 347, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.243 [0.000, 2.000], mean observation: -0.227 [-0.959, 0.500], loss: 6.097913, mean_absolute_error: 35.104691, mean_q: -51.391705\n",
            " 32295/50000: episode: 183, duration: 0.359s, episode steps: 124, steps per second: 345, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000], mean observation: -0.231 [-1.052, 0.504], loss: 4.189592, mean_absolute_error: 35.315708, mean_q: -51.799183\n",
            " 32386/50000: episode: 184, duration: 0.264s, episode steps: 91, steps per second: 344, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000], mean observation: -0.205 [-0.935, 0.506], loss: 3.338313, mean_absolute_error: 34.975967, mean_q: -51.398754\n",
            " 32563/50000: episode: 185, duration: 0.519s, episode steps: 177, steps per second: 341, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.073 [0.000, 2.000], mean observation: -0.164 [-1.200, 0.530], loss: 2.936216, mean_absolute_error: 34.615208, mean_q: -50.851540\n",
            " 32738/50000: episode: 186, duration: 0.507s, episode steps: 175, steps per second: 345, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.069 [0.000, 2.000], mean observation: -0.170 [-1.200, 0.537], loss: 3.167716, mean_absolute_error: 34.876019, mean_q: -51.185127\n",
            " 32861/50000: episode: 187, duration: 0.361s, episode steps: 123, steps per second: 341, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.374 [0.000, 2.000], mean observation: -0.184 [-0.946, 0.502], loss: 2.911277, mean_absolute_error: 34.731705, mean_q: -50.988937\n",
            " 32985/50000: episode: 188, duration: 0.358s, episode steps: 124, steps per second: 347, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.379 [0.000, 2.000], mean observation: -0.226 [-1.057, 0.517], loss: 2.427407, mean_absolute_error: 34.745373, mean_q: -51.009003\n",
            " 33170/50000: episode: 189, duration: 0.533s, episode steps: 185, steps per second: 347, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.135 [-1.200, 0.515], loss: 3.523602, mean_absolute_error: 35.005257, mean_q: -51.334892\n",
            " 33367/50000: episode: 190, duration: 0.567s, episode steps: 197, steps per second: 347, episode reward: -197.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000], mean observation: -0.107 [-1.200, 0.525], loss: 3.182446, mean_absolute_error: 34.331364, mean_q: -50.338207\n",
            " 33530/50000: episode: 191, duration: 0.467s, episode steps: 163, steps per second: 349, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000], mean observation: -0.213 [-1.200, 0.525], loss: 3.774956, mean_absolute_error: 34.606136, mean_q: -50.722149\n",
            " 33653/50000: episode: 192, duration: 0.358s, episode steps: 123, steps per second: 344, episode reward: -123.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.268 [0.000, 2.000], mean observation: -0.248 [-1.200, 0.531], loss: 5.693626, mean_absolute_error: 34.368973, mean_q: -50.333481\n",
            " 33772/50000: episode: 193, duration: 0.345s, episode steps: 119, steps per second: 345, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000], mean observation: -0.243 [-1.200, 0.522], loss: 2.095694, mean_absolute_error: 34.365826, mean_q: -50.518280\n",
            " 33972/50000: episode: 194, duration: 0.577s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000], mean observation: -0.195 [-1.200, 0.385], loss: 2.856483, mean_absolute_error: 34.307499, mean_q: -50.369667\n",
            " 34086/50000: episode: 195, duration: 0.344s, episode steps: 114, steps per second: 332, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.531], loss: 3.003274, mean_absolute_error: 34.200378, mean_q: -50.246677\n",
            " 34261/50000: episode: 196, duration: 0.502s, episode steps: 175, steps per second: 349, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.017 [0.000, 2.000], mean observation: -0.151 [-1.200, 0.527], loss: 2.648438, mean_absolute_error: 34.255917, mean_q: -50.339725\n",
            " 34396/50000: episode: 197, duration: 0.393s, episode steps: 135, steps per second: 344, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.163 [0.000, 2.000], mean observation: -0.233 [-1.200, 0.507], loss: 4.812231, mean_absolute_error: 34.207840, mean_q: -50.155518\n",
            " 34529/50000: episode: 198, duration: 0.384s, episode steps: 133, steps per second: 347, episode reward: -133.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.270 [-1.197, 0.517], loss: 3.138247, mean_absolute_error: 33.960060, mean_q: -49.821743\n",
            " 34682/50000: episode: 199, duration: 0.442s, episode steps: 153, steps per second: 346, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.928 [0.000, 2.000], mean observation: -0.240 [-1.200, 0.541], loss: 1.192591, mean_absolute_error: 33.980858, mean_q: -50.028622\n",
            " 34792/50000: episode: 200, duration: 0.321s, episode steps: 110, steps per second: 343, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.282 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.508], loss: 1.570514, mean_absolute_error: 33.860485, mean_q: -49.765385\n",
            " 34992/50000: episode: 201, duration: 0.582s, episode steps: 200, steps per second: 344, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000], mean observation: -0.187 [-1.200, 0.400], loss: 5.770100, mean_absolute_error: 33.988121, mean_q: -49.844166\n",
            " 35123/50000: episode: 202, duration: 0.382s, episode steps: 131, steps per second: 343, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.230 [-1.200, 0.502], loss: 4.305812, mean_absolute_error: 33.731518, mean_q: -49.545795\n",
            " 35241/50000: episode: 203, duration: 0.351s, episode steps: 118, steps per second: 336, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.458 [0.000, 2.000], mean observation: -0.220 [-1.055, 0.519], loss: 2.433479, mean_absolute_error: 34.101803, mean_q: -50.103916\n",
            " 35357/50000: episode: 204, duration: 0.340s, episode steps: 116, steps per second: 341, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.052 [0.000, 2.000], mean observation: -0.195 [-0.946, 0.500], loss: 1.494878, mean_absolute_error: 33.969395, mean_q: -50.014370\n",
            " 35503/50000: episode: 205, duration: 0.442s, episode steps: 146, steps per second: 331, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.952 [0.000, 2.000], mean observation: -0.231 [-1.200, 0.504], loss: 2.897445, mean_absolute_error: 33.733925, mean_q: -49.523613\n",
            " 35671/50000: episode: 206, duration: 0.480s, episode steps: 168, steps per second: 350, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.976 [0.000, 2.000], mean observation: -0.181 [-1.200, 0.506], loss: 2.310657, mean_absolute_error: 33.983540, mean_q: -49.975273\n",
            " 35801/50000: episode: 207, duration: 0.373s, episode steps: 130, steps per second: 349, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.054 [0.000, 2.000], mean observation: -0.263 [-1.111, 0.529], loss: 4.359028, mean_absolute_error: 33.982841, mean_q: -49.823994\n",
            " 35951/50000: episode: 208, duration: 0.432s, episode steps: 150, steps per second: 347, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.517], loss: 2.828948, mean_absolute_error: 33.963951, mean_q: -49.911926\n",
            " 36101/50000: episode: 209, duration: 0.434s, episode steps: 150, steps per second: 345, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000], mean observation: -0.216 [-1.200, 0.509], loss: 3.329819, mean_absolute_error: 33.839439, mean_q: -49.692219\n",
            " 36221/50000: episode: 210, duration: 0.349s, episode steps: 120, steps per second: 344, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000], mean observation: -0.253 [-1.152, 0.514], loss: 2.178667, mean_absolute_error: 33.742023, mean_q: -49.570847\n",
            " 36382/50000: episode: 211, duration: 0.464s, episode steps: 161, steps per second: 347, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000], mean observation: -0.190 [-1.200, 0.522], loss: 2.787043, mean_absolute_error: 33.646832, mean_q: -49.372562\n",
            " 36533/50000: episode: 212, duration: 0.438s, episode steps: 151, steps per second: 345, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.113 [0.000, 2.000], mean observation: -0.256 [-1.159, 0.511], loss: 4.175264, mean_absolute_error: 33.625092, mean_q: -49.263344\n",
            " 36674/50000: episode: 213, duration: 0.409s, episode steps: 141, steps per second: 345, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.021 [0.000, 2.000], mean observation: -0.266 [-1.200, 0.531], loss: 2.200164, mean_absolute_error: 33.571495, mean_q: -49.374268\n",
            " 36793/50000: episode: 214, duration: 0.348s, episode steps: 119, steps per second: 342, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.218 [0.000, 2.000], mean observation: -0.217 [-1.047, 0.512], loss: 2.765856, mean_absolute_error: 33.811050, mean_q: -49.737015\n",
            " 36913/50000: episode: 215, duration: 0.358s, episode steps: 120, steps per second: 335, episode reward: -120.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.525 [0.000, 2.000], mean observation: -0.202 [-0.978, 0.515], loss: 3.379853, mean_absolute_error: 33.418732, mean_q: -49.096207\n",
            " 37034/50000: episode: 216, duration: 0.351s, episode steps: 121, steps per second: 345, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.347 [0.000, 2.000], mean observation: -0.224 [-0.998, 0.508], loss: 2.151710, mean_absolute_error: 33.623978, mean_q: -49.421070\n",
            " 37177/50000: episode: 217, duration: 0.410s, episode steps: 143, steps per second: 349, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.219 [-1.200, 0.540], loss: 3.177351, mean_absolute_error: 33.696957, mean_q: -49.524872\n",
            " 37318/50000: episode: 218, duration: 0.405s, episode steps: 141, steps per second: 348, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.250 [-1.200, 0.537], loss: 3.903281, mean_absolute_error: 33.580437, mean_q: -49.296844\n",
            " 37462/50000: episode: 219, duration: 0.416s, episode steps: 144, steps per second: 346, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000], mean observation: -0.211 [-1.200, 0.509], loss: 3.139797, mean_absolute_error: 33.585590, mean_q: -49.276726\n",
            " 37637/50000: episode: 220, duration: 0.511s, episode steps: 175, steps per second: 342, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.971 [0.000, 2.000], mean observation: -0.201 [-1.200, 0.519], loss: 2.710928, mean_absolute_error: 33.341484, mean_q: -48.971897\n",
            " 37741/50000: episode: 221, duration: 0.303s, episode steps: 104, steps per second: 343, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.288 [0.000, 2.000], mean observation: -0.205 [-0.942, 0.502], loss: 2.392354, mean_absolute_error: 33.623062, mean_q: -49.320171\n",
            " 37867/50000: episode: 222, duration: 0.367s, episode steps: 126, steps per second: 343, episode reward: -126.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.317 [0.000, 2.000], mean observation: -0.241 [-1.119, 0.532], loss: 2.884191, mean_absolute_error: 33.533669, mean_q: -49.311131\n",
            " 38035/50000: episode: 223, duration: 0.481s, episode steps: 168, steps per second: 349, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.248 [-1.200, 0.537], loss: 4.113007, mean_absolute_error: 33.492676, mean_q: -49.144440\n",
            " 38153/50000: episode: 224, duration: 0.344s, episode steps: 118, steps per second: 343, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.407 [0.000, 2.000], mean observation: -0.236 [-1.200, 0.537], loss: 5.591875, mean_absolute_error: 33.317760, mean_q: -48.768372\n",
            " 38277/50000: episode: 225, duration: 0.358s, episode steps: 124, steps per second: 346, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.202 [0.000, 2.000], mean observation: -0.269 [-1.200, 0.507], loss: 2.676078, mean_absolute_error: 33.718296, mean_q: -49.572437\n",
            " 38442/50000: episode: 226, duration: 0.477s, episode steps: 165, steps per second: 346, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000], mean observation: -0.203 [-1.200, 0.526], loss: 3.491488, mean_absolute_error: 33.307930, mean_q: -48.931702\n",
            " 38572/50000: episode: 227, duration: 0.378s, episode steps: 130, steps per second: 344, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.250 [-1.200, 0.537], loss: 3.133377, mean_absolute_error: 33.233612, mean_q: -48.813568\n",
            " 38720/50000: episode: 228, duration: 0.429s, episode steps: 148, steps per second: 345, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.537], loss: 2.299369, mean_absolute_error: 33.076279, mean_q: -48.525616\n",
            " 38878/50000: episode: 229, duration: 0.455s, episode steps: 158, steps per second: 347, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000], mean observation: -0.252 [-1.200, 0.506], loss: 3.057935, mean_absolute_error: 33.313068, mean_q: -48.911972\n",
            " 39033/50000: episode: 230, duration: 0.446s, episode steps: 155, steps per second: 347, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.994 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.537], loss: 1.445377, mean_absolute_error: 33.113689, mean_q: -48.668095\n",
            " 39182/50000: episode: 231, duration: 0.430s, episode steps: 149, steps per second: 346, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.114 [0.000, 2.000], mean observation: -0.218 [-1.200, 0.509], loss: 2.905067, mean_absolute_error: 33.122810, mean_q: -48.596695\n",
            " 39325/50000: episode: 232, duration: 0.413s, episode steps: 143, steps per second: 346, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000], mean observation: -0.250 [-1.200, 0.537], loss: 3.650389, mean_absolute_error: 32.774933, mean_q: -48.058552\n",
            " 39466/50000: episode: 233, duration: 0.408s, episode steps: 141, steps per second: 346, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.092 [0.000, 2.000], mean observation: -0.239 [-1.200, 0.537], loss: 2.589077, mean_absolute_error: 32.622715, mean_q: -47.868477\n",
            " 39566/50000: episode: 234, duration: 0.292s, episode steps: 100, steps per second: 343, episode reward: -100.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000], mean observation: -0.213 [-0.962, 0.515], loss: 2.527137, mean_absolute_error: 32.855331, mean_q: -48.220177\n",
            " 39671/50000: episode: 235, duration: 0.306s, episode steps: 105, steps per second: 343, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.233 [-1.056, 0.510], loss: 2.533952, mean_absolute_error: 33.042030, mean_q: -48.490952\n",
            " 39807/50000: episode: 236, duration: 0.392s, episode steps: 136, steps per second: 347, episode reward: -136.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.243 [-1.200, 0.522], loss: 4.509325, mean_absolute_error: 32.933411, mean_q: -48.301319\n",
            " 39958/50000: episode: 237, duration: 0.431s, episode steps: 151, steps per second: 351, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000], mean observation: -0.214 [-1.200, 0.537], loss: 2.475454, mean_absolute_error: 32.892960, mean_q: -48.406651\n",
            " 40109/50000: episode: 238, duration: 0.444s, episode steps: 151, steps per second: 340, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.993 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.506], loss: 3.476767, mean_absolute_error: 32.424206, mean_q: -47.615589\n",
            " 40309/50000: episode: 239, duration: 0.572s, episode steps: 200, steps per second: 350, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000], mean observation: -0.191 [-1.200, 0.443], loss: 3.671329, mean_absolute_error: 32.742046, mean_q: -48.037392\n",
            " 40455/50000: episode: 240, duration: 0.422s, episode steps: 146, steps per second: 346, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000], mean observation: -0.213 [-1.200, 0.503], loss: 3.958499, mean_absolute_error: 32.748730, mean_q: -48.142597\n",
            " 40599/50000: episode: 241, duration: 0.415s, episode steps: 144, steps per second: 347, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.216 [-1.200, 0.537], loss: 2.653238, mean_absolute_error: 32.489361, mean_q: -47.793411\n",
            " 40757/50000: episode: 242, duration: 0.456s, episode steps: 158, steps per second: 346, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.089 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.537], loss: 4.246425, mean_absolute_error: 32.739792, mean_q: -48.064571\n",
            " 40899/50000: episode: 243, duration: 0.409s, episode steps: 142, steps per second: 347, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.127 [0.000, 2.000], mean observation: -0.232 [-1.200, 0.525], loss: 3.225616, mean_absolute_error: 32.638134, mean_q: -48.001621\n",
            " 41012/50000: episode: 244, duration: 0.326s, episode steps: 113, steps per second: 347, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000], mean observation: -0.222 [-0.979, 0.528], loss: 3.311892, mean_absolute_error: 32.313774, mean_q: -47.486805\n",
            " 41152/50000: episode: 245, duration: 0.406s, episode steps: 140, steps per second: 345, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.086 [0.000, 2.000], mean observation: -0.252 [-1.200, 0.537], loss: 2.149317, mean_absolute_error: 32.515400, mean_q: -47.782654\n",
            " 41293/50000: episode: 246, duration: 0.407s, episode steps: 141, steps per second: 347, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.239 [-1.200, 0.537], loss: 2.948397, mean_absolute_error: 32.299168, mean_q: -47.498371\n",
            " 41448/50000: episode: 247, duration: 0.453s, episode steps: 155, steps per second: 342, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.174 [0.000, 2.000], mean observation: -0.203 [-1.200, 0.537], loss: 1.661495, mean_absolute_error: 32.497936, mean_q: -47.849094\n",
            " 41593/50000: episode: 248, duration: 0.418s, episode steps: 145, steps per second: 347, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.207 [-1.200, 0.525], loss: 4.149872, mean_absolute_error: 32.251308, mean_q: -47.346451\n",
            " 41717/50000: episode: 249, duration: 0.355s, episode steps: 124, steps per second: 349, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.492 [0.000, 2.000], mean observation: -0.205 [-0.971, 0.516], loss: 3.512231, mean_absolute_error: 32.529541, mean_q: -47.697510\n",
            " 41888/50000: episode: 250, duration: 0.492s, episode steps: 171, steps per second: 348, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.427 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.537], loss: 2.374136, mean_absolute_error: 32.116482, mean_q: -47.289684\n",
            " 42043/50000: episode: 251, duration: 0.445s, episode steps: 155, steps per second: 348, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.116 [0.000, 2.000], mean observation: -0.201 [-1.200, 0.537], loss: 1.540632, mean_absolute_error: 31.871641, mean_q: -46.989037\n",
            " 42194/50000: episode: 252, duration: 0.441s, episode steps: 151, steps per second: 343, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.066 [0.000, 2.000], mean observation: -0.218 [-1.200, 0.537], loss: 1.989230, mean_absolute_error: 32.282978, mean_q: -47.514160\n",
            " 42334/50000: episode: 253, duration: 0.405s, episode steps: 140, steps per second: 345, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.164 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.509], loss: 2.886287, mean_absolute_error: 32.572762, mean_q: -47.893120\n",
            " 42452/50000: episode: 254, duration: 0.343s, episode steps: 118, steps per second: 344, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.458 [0.000, 2.000], mean observation: -0.247 [-1.200, 0.537], loss: 3.241735, mean_absolute_error: 32.408752, mean_q: -47.684093\n",
            " 42569/50000: episode: 255, duration: 0.343s, episode steps: 117, steps per second: 341, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.496 [0.000, 2.000], mean observation: -0.201 [-0.967, 0.524], loss: 3.595883, mean_absolute_error: 32.200264, mean_q: -47.287094\n",
            " 42678/50000: episode: 256, duration: 0.317s, episode steps: 109, steps per second: 344, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000], mean observation: -0.235 [-0.999, 0.500], loss: 2.550826, mean_absolute_error: 31.872704, mean_q: -46.943985\n",
            " 42860/50000: episode: 257, duration: 0.522s, episode steps: 182, steps per second: 349, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000], mean observation: -0.191 [-1.200, 0.531], loss: 5.020539, mean_absolute_error: 32.338799, mean_q: -47.523506\n",
            " 43014/50000: episode: 258, duration: 0.444s, episode steps: 154, steps per second: 347, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.026 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.537], loss: 1.391284, mean_absolute_error: 32.264488, mean_q: -47.532719\n",
            " 43168/50000: episode: 259, duration: 0.446s, episode steps: 154, steps per second: 345, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.537], loss: 2.554291, mean_absolute_error: 32.319748, mean_q: -47.590580\n",
            " 43271/50000: episode: 260, duration: 0.300s, episode steps: 103, steps per second: 344, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.214 [0.000, 2.000], mean observation: -0.209 [-0.940, 0.517], loss: 3.375008, mean_absolute_error: 32.170452, mean_q: -47.278633\n",
            " 43417/50000: episode: 261, duration: 0.423s, episode steps: 146, steps per second: 345, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000], mean observation: -0.251 [-1.200, 0.537], loss: 2.011861, mean_absolute_error: 32.260609, mean_q: -47.559410\n",
            " 43569/50000: episode: 262, duration: 0.442s, episode steps: 152, steps per second: 344, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.537], loss: 2.132951, mean_absolute_error: 32.254787, mean_q: -47.479851\n",
            " 43734/50000: episode: 263, duration: 0.480s, episode steps: 165, steps per second: 344, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.036 [0.000, 2.000], mean observation: -0.195 [-1.200, 0.537], loss: 1.918386, mean_absolute_error: 32.133629, mean_q: -47.300671\n",
            " 43884/50000: episode: 264, duration: 0.435s, episode steps: 150, steps per second: 345, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.107 [0.000, 2.000], mean observation: -0.228 [-1.200, 0.537], loss: 1.298016, mean_absolute_error: 32.268738, mean_q: -47.575134\n",
            " 44033/50000: episode: 265, duration: 0.429s, episode steps: 149, steps per second: 347, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.081 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.537], loss: 2.659178, mean_absolute_error: 32.157402, mean_q: -47.295914\n",
            " 44137/50000: episode: 266, duration: 0.304s, episode steps: 104, steps per second: 342, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.221 [0.000, 2.000], mean observation: -0.219 [-0.975, 0.526], loss: 3.037683, mean_absolute_error: 31.649090, mean_q: -46.475491\n",
            " 44293/50000: episode: 267, duration: 0.446s, episode steps: 156, steps per second: 350, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.167 [0.000, 2.000], mean observation: -0.216 [-1.200, 0.537], loss: 1.620511, mean_absolute_error: 32.464619, mean_q: -47.857971\n",
            " 44427/50000: episode: 268, duration: 0.397s, episode steps: 134, steps per second: 338, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000], mean observation: -0.256 [-1.200, 0.537], loss: 3.918051, mean_absolute_error: 32.251263, mean_q: -47.407509\n",
            " 44522/50000: episode: 269, duration: 0.278s, episode steps: 95, steps per second: 342, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000], mean observation: -0.146 [-0.871, 0.513], loss: 1.509885, mean_absolute_error: 31.937160, mean_q: -47.042484\n",
            " 44643/50000: episode: 270, duration: 0.348s, episode steps: 121, steps per second: 348, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.182 [0.000, 2.000], mean observation: -0.245 [-1.058, 0.517], loss: 4.645453, mean_absolute_error: 31.973198, mean_q: -46.871933\n",
            " 44736/50000: episode: 271, duration: 0.267s, episode steps: 93, steps per second: 348, episode reward: -93.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.333 [0.000, 2.000], mean observation: -0.142 [-0.884, 0.515], loss: 3.598220, mean_absolute_error: 32.573330, mean_q: -47.869392\n",
            " 44909/50000: episode: 272, duration: 0.500s, episode steps: 173, steps per second: 346, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.264 [-1.196, 0.522], loss: 5.013576, mean_absolute_error: 32.163555, mean_q: -47.101879\n",
            " 45005/50000: episode: 273, duration: 0.279s, episode steps: 96, steps per second: 344, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.094 [0.000, 2.000], mean observation: -0.187 [-0.899, 0.515], loss: 2.577173, mean_absolute_error: 32.115986, mean_q: -47.251465\n",
            " 45165/50000: episode: 274, duration: 0.462s, episode steps: 160, steps per second: 346, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.206 [0.000, 2.000], mean observation: -0.199 [-1.200, 0.537], loss: 2.030883, mean_absolute_error: 32.078209, mean_q: -47.220478\n",
            " 45327/50000: episode: 275, duration: 0.469s, episode steps: 162, steps per second: 345, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.093 [0.000, 2.000], mean observation: -0.247 [-1.200, 0.537], loss: 2.875093, mean_absolute_error: 32.196602, mean_q: -47.330574\n",
            " 45422/50000: episode: 276, duration: 0.279s, episode steps: 95, steps per second: 340, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.179 [-0.885, 0.502], loss: 3.069164, mean_absolute_error: 32.170719, mean_q: -47.300186\n",
            " 45571/50000: episode: 277, duration: 0.433s, episode steps: 149, steps per second: 344, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.204 [-1.200, 0.537], loss: 2.756550, mean_absolute_error: 31.700268, mean_q: -46.578335\n",
            " 45655/50000: episode: 278, duration: 0.251s, episode steps: 84, steps per second: 335, episode reward: -84.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.203 [-0.937, 0.511], loss: 2.002739, mean_absolute_error: 32.064716, mean_q: -47.244488\n",
            " 45834/50000: episode: 279, duration: 0.516s, episode steps: 179, steps per second: 347, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.274 [0.000, 2.000], mean observation: -0.168 [-1.200, 0.537], loss: 1.526653, mean_absolute_error: 31.796940, mean_q: -46.760715\n",
            " 46007/50000: episode: 280, duration: 0.492s, episode steps: 173, steps per second: 352, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.249 [0.000, 2.000], mean observation: -0.214 [-1.200, 0.537], loss: 2.984051, mean_absolute_error: 31.908461, mean_q: -46.950241\n",
            " 46158/50000: episode: 281, duration: 0.433s, episode steps: 151, steps per second: 349, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.927 [0.000, 2.000], mean observation: -0.252 [-1.200, 0.537], loss: 2.310385, mean_absolute_error: 32.117985, mean_q: -47.236736\n",
            " 46324/50000: episode: 282, duration: 0.479s, episode steps: 166, steps per second: 347, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.361 [0.000, 2.000], mean observation: -0.171 [-1.200, 0.537], loss: 2.169665, mean_absolute_error: 31.728174, mean_q: -46.670650\n",
            " 46466/50000: episode: 283, duration: 0.409s, episode steps: 142, steps per second: 347, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.197 [0.000, 2.000], mean observation: -0.244 [-1.200, 0.537], loss: 1.116652, mean_absolute_error: 32.054714, mean_q: -47.174580\n",
            " 46666/50000: episode: 284, duration: 0.576s, episode steps: 200, steps per second: 347, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000], mean observation: -0.254 [-1.200, 0.148], loss: 2.284276, mean_absolute_error: 32.195049, mean_q: -47.298206\n",
            " 46822/50000: episode: 285, duration: 0.446s, episode steps: 156, steps per second: 349, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.321 [0.000, 2.000], mean observation: -0.243 [-1.200, 0.537], loss: 2.762760, mean_absolute_error: 31.991236, mean_q: -47.024601\n",
            " 46937/50000: episode: 286, duration: 0.335s, episode steps: 115, steps per second: 344, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.261 [0.000, 2.000], mean observation: -0.122 [-0.858, 0.508], loss: 2.414050, mean_absolute_error: 32.220325, mean_q: -47.361710\n",
            " 47099/50000: episode: 287, duration: 0.468s, episode steps: 162, steps per second: 346, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.049 [0.000, 2.000], mean observation: -0.240 [-1.200, 0.537], loss: 2.430330, mean_absolute_error: 31.915600, mean_q: -46.852280\n",
            " 47271/50000: episode: 288, duration: 0.490s, episode steps: 172, steps per second: 351, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000], mean observation: -0.194 [-1.200, 0.537], loss: 2.127745, mean_absolute_error: 31.868368, mean_q: -46.861866\n",
            " 47366/50000: episode: 289, duration: 0.275s, episode steps: 95, steps per second: 345, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.201 [-0.912, 0.504], loss: 2.926204, mean_absolute_error: 32.435337, mean_q: -47.619656\n",
            " 47475/50000: episode: 290, duration: 0.318s, episode steps: 109, steps per second: 343, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000], mean observation: -0.184 [-0.893, 0.517], loss: 2.410152, mean_absolute_error: 32.252220, mean_q: -47.353905\n",
            " 47630/50000: episode: 291, duration: 0.447s, episode steps: 155, steps per second: 347, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.858 [0.000, 2.000], mean observation: -0.274 [-1.200, 0.537], loss: 2.800031, mean_absolute_error: 32.322121, mean_q: -47.431198\n",
            " 47791/50000: episode: 292, duration: 0.463s, episode steps: 161, steps per second: 347, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000], mean observation: -0.240 [-1.200, 0.537], loss: 2.424979, mean_absolute_error: 31.991077, mean_q: -46.916538\n",
            " 47944/50000: episode: 293, duration: 0.443s, episode steps: 153, steps per second: 346, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.537], loss: 2.794343, mean_absolute_error: 32.123360, mean_q: -47.143307\n",
            " 48039/50000: episode: 294, duration: 0.279s, episode steps: 95, steps per second: 340, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.157 [-0.876, 0.517], loss: 2.240023, mean_absolute_error: 32.246693, mean_q: -47.348957\n",
            " 48239/50000: episode: 295, duration: 0.575s, episode steps: 200, steps per second: 348, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000], mean observation: -0.220 [-1.200, 0.225], loss: 2.308597, mean_absolute_error: 32.289154, mean_q: -47.393970\n",
            " 48395/50000: episode: 296, duration: 0.447s, episode steps: 156, steps per second: 349, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.204 [-1.200, 0.506], loss: 2.730825, mean_absolute_error: 32.249252, mean_q: -47.324120\n",
            " 48493/50000: episode: 297, duration: 0.284s, episode steps: 98, steps per second: 345, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000], mean observation: -0.149 [-0.868, 0.503], loss: 1.305633, mean_absolute_error: 31.971994, mean_q: -47.003792\n",
            " 48644/50000: episode: 298, duration: 0.437s, episode steps: 151, steps per second: 346, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.263 [-1.087, 0.541], loss: 2.159049, mean_absolute_error: 32.180683, mean_q: -47.238461\n",
            " 48746/50000: episode: 299, duration: 0.296s, episode steps: 102, steps per second: 344, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.196 [0.000, 2.000], mean observation: -0.153 [-0.868, 0.507], loss: 1.074884, mean_absolute_error: 32.056534, mean_q: -47.107311\n",
            " 48909/50000: episode: 300, duration: 0.470s, episode steps: 163, steps per second: 346, episode reward: -163.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000], mean observation: -0.247 [-1.094, 0.525], loss: 3.011652, mean_absolute_error: 32.326275, mean_q: -47.352467\n",
            " 49057/50000: episode: 301, duration: 0.426s, episode steps: 148, steps per second: 347, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.257 [0.000, 2.000], mean observation: -0.223 [-1.200, 0.537], loss: 1.988184, mean_absolute_error: 32.332565, mean_q: -47.466949\n",
            " 49211/50000: episode: 302, duration: 0.443s, episode steps: 154, steps per second: 348, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.305 [0.000, 2.000], mean observation: -0.214 [-1.200, 0.537], loss: 2.544092, mean_absolute_error: 32.627003, mean_q: -47.879345\n",
            " 49393/50000: episode: 303, duration: 0.523s, episode steps: 182, steps per second: 348, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.537], loss: 2.816792, mean_absolute_error: 32.388550, mean_q: -47.557102\n",
            " 49539/50000: episode: 304, duration: 0.420s, episode steps: 146, steps per second: 347, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.432 [0.000, 2.000], mean observation: -0.224 [-1.079, 0.511], loss: 2.772890, mean_absolute_error: 32.694904, mean_q: -47.929195\n",
            " 49680/50000: episode: 305, duration: 0.412s, episode steps: 141, steps per second: 343, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.957 [0.000, 2.000], mean observation: -0.259 [-1.200, 0.537], loss: 2.190512, mean_absolute_error: 32.206898, mean_q: -47.280910\n",
            " 49789/50000: episode: 306, duration: 0.318s, episode steps: 109, steps per second: 343, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.147 [0.000, 2.000], mean observation: -0.164 [-0.883, 0.506], loss: 3.137798, mean_absolute_error: 32.345982, mean_q: -47.456203\n",
            " 49955/50000: episode: 307, duration: 0.483s, episode steps: 166, steps per second: 344, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.108 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.537], loss: 1.165707, mean_absolute_error: 32.148155, mean_q: -47.253689\n",
            "done, took 145.846 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h9Ur8kWrOpv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1093
        },
        "outputId": "bab7cc7d-63f0-489c-c382-ca2d0e5ac622"
      },
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes=1, visualize=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 1 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e32e7a73eb7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'on_action_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rl/callbacks.py\u001b[0m in \u001b[0;36mon_action_end\u001b[0;34m(self, action, logs)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_action_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;34m\"\"\" Render environment at the end of each action \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "w7tqxBNlSlsE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
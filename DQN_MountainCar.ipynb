{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_MountainCar.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junyamahira/ReinforcementLearning_Prac/blob/master/DQN_MountainCar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BbHY0Gc_tdz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# keras-rlのインストール"
      ]
    },
    {
      "metadata": {
        "id": "ap27433nXCn9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## git clone \n",
        "- これかpipのどちらかを実行すればよい (pipがおすすめ)\n",
        "- keras-rlの中に入り、setup.pyを実行する"
      ]
    },
    {
      "metadata": {
        "id": "xLXt4VtkXLwh",
        "colab_type": "code",
        "outputId": "52769e99-8b76-47a6-e1b8-3947166fbb41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/keras-rl/keras-rl.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-rl'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 1710 (delta 3), reused 7 (delta 2), pack-reused 1698\u001b[K\n",
            "Receiving objects: 100% (1710/1710), 1.38 MiB | 19.35 MiB/s, done.\n",
            "Resolving deltas: 100% (1056/1056), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IaD-jZAAXa8S",
        "colab_type": "code",
        "outputId": "92ca32d0-eb5f-4124-f1d2-509fc1d6b118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "!ls keras-rl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assets\t\t examples\t    mkdocs.yml\trl\t   tests\n",
            "CONTRIBUTING.md  ISSUE_TEMPLATE.md  pytest.ini\tsetup.cfg  utils\n",
            "docs\t\t LICENSE\t    README.md\tsetup.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8cw_CkyZFsd",
        "colab_type": "code",
        "outputId": "be46aaca-71ad-4f27-c88e-7971eeb08fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!cd keras-rl\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keras-rl  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-_L-L0I2a-uQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## pip"
      ]
    },
    {
      "metadata": {
        "id": "DUyHjkEPbBtS",
        "colab_type": "code",
        "outputId": "2fced276-51fb-427d-f0c3-ea9d628b4de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "pip install keras-rl"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\r\u001b[K    25% |████████                        | 10kB 15.1MB/s eta 0:00:01\r\u001b[K    50% |████████████████▏               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 30kB 4.7MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 40kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JUcjN-baSzPe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# その他のimport"
      ]
    },
    {
      "metadata": {
        "id": "ZG-NDV4AQIFS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "import gym\n",
        "from gym import wrappers\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K9I5EaaMHWYK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## visiual display"
      ]
    },
    {
      "metadata": {
        "id": "qxGA6tFdNNEa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#?\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdpKHXZ8HVee",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBv7HoHzHXJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#　"
      ]
    },
    {
      "metadata": {
        "id": "DGIdiFuXT1rk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "565572a0-882f-457b-aefb-3cdbee1899d2"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MountainCar-v0\")\n",
        "env = wrappers.Monitor(env, './', force=True)\n",
        "nb_actions = env.action_space.n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c2e10dd6383a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MountainCar-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnb_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wrappers' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "xsV75LWXtMLx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# モデル"
      ]
    },
    {
      "metadata": {
        "id": "vFGewr_orPkl",
        "colab_type": "code",
        "outputId": "76447ccc-96fb-4112-d6b5-00d365787bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oFQDZxpvf9PJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "2c5dcf96-9a47-48d2-ae41-a8534fc57551"
      },
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 643\n",
            "Trainable params: 643\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F_kWc71Kse2-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5833
        },
        "outputId": "2d6d4718-e9b8-4f08-d470-3d6d3747a46e"
      },
      "cell_type": "code",
      "source": [
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "# 方策\n",
        "policy = EpsGreedyQPolicy(eps=0.001)\n",
        "#　各パラメータをここで調整\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions,gamma=0.99, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "# 何設定してん？\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   200/50000: episode: 1, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.245 [-0.868, 0.029], loss: 0.195985, mean_absolute_error: 0.650357, mean_q: -0.647298\n",
            "   400/50000: episode: 2, duration: 0.620s, episode steps: 200, steps per second: 323, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 1.000], mean observation: -0.281 [-0.829, 0.020], loss: 0.011042, mean_absolute_error: 1.581398, mean_q: -2.291042\n",
            "   600/50000: episode: 3, duration: 0.621s, episode steps: 200, steps per second: 322, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.215 [0.000, 1.000], mean observation: -0.320 [-0.761, 0.009], loss: 0.017752, mean_absolute_error: 2.685251, mean_q: -3.964835\n",
            "   800/50000: episode: 4, duration: 0.636s, episode steps: 200, steps per second: 315, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.355 [0.000, 2.000], mean observation: -0.311 [-0.845, 0.023], loss: 0.047803, mean_absolute_error: 3.856971, mean_q: -5.691300\n",
            "  1000/50000: episode: 5, duration: 0.628s, episode steps: 200, steps per second: 318, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.170 [0.000, 1.000], mean observation: -0.322 [-0.794, 0.011], loss: 0.087227, mean_absolute_error: 5.011901, mean_q: -7.399600\n",
            "  1200/50000: episode: 6, duration: 0.637s, episode steps: 200, steps per second: 314, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.298 [-0.703, 0.014], loss: 0.137991, mean_absolute_error: 6.158036, mean_q: -9.088457\n",
            "  1400/50000: episode: 7, duration: 0.639s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.595 [0.000, 2.000], mean observation: -0.288 [-0.861, 0.019], loss: 0.211685, mean_absolute_error: 7.276439, mean_q: -10.766445\n",
            "  1600/50000: episode: 8, duration: 0.627s, episode steps: 200, steps per second: 319, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.510 [0.000, 2.000], mean observation: -0.295 [-0.723, 0.016], loss: 0.212775, mean_absolute_error: 8.347979, mean_q: -12.390738\n",
            "  1800/50000: episode: 9, duration: 0.636s, episode steps: 200, steps per second: 315, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.425 [0.000, 2.000], mean observation: -0.307 [-0.807, 0.021], loss: 0.534914, mean_absolute_error: 9.408372, mean_q: -13.901172\n",
            "  2000/50000: episode: 10, duration: 0.639s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.485 [0.000, 2.000], mean observation: -0.310 [-0.950, 0.023], loss: 0.406881, mean_absolute_error: 10.414740, mean_q: -15.446236\n",
            "  2200/50000: episode: 11, duration: 0.631s, episode steps: 200, steps per second: 317, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.056], loss: 0.660467, mean_absolute_error: 11.405377, mean_q: -16.868200\n",
            "  2357/50000: episode: 12, duration: 0.505s, episode steps: 157, steps per second: 311, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.250 [-1.088, 0.515], loss: 0.483676, mean_absolute_error: 12.104133, mean_q: -17.901751\n",
            "  2557/50000: episode: 13, duration: 0.639s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.254 [-1.200, 0.019], loss: 0.694941, mean_absolute_error: 12.881310, mean_q: -19.105444\n",
            "  2757/50000: episode: 14, duration: 0.624s, episode steps: 200, steps per second: 320, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.330 [0.000, 2.000], mean observation: -0.238 [-0.842, 0.027], loss: 1.018003, mean_absolute_error: 13.729388, mean_q: -20.302656\n",
            "  2957/50000: episode: 15, duration: 0.642s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000], mean observation: -0.258 [-1.145, 0.052], loss: 1.009578, mean_absolute_error: 14.616055, mean_q: -21.624449\n",
            "  3157/50000: episode: 16, duration: 0.620s, episode steps: 200, steps per second: 323, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000], mean observation: -0.219 [-0.986, 0.319], loss: 1.007759, mean_absolute_error: 15.457433, mean_q: -22.893616\n",
            "  3357/50000: episode: 17, duration: 0.625s, episode steps: 200, steps per second: 320, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000], mean observation: -0.254 [-0.708, 0.013], loss: 1.222158, mean_absolute_error: 16.286667, mean_q: -24.178635\n",
            "  3557/50000: episode: 18, duration: 0.630s, episode steps: 200, steps per second: 317, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000], mean observation: -0.292 [-1.189, 0.027], loss: 1.539350, mean_absolute_error: 17.102690, mean_q: -25.358868\n",
            "  3757/50000: episode: 19, duration: 0.628s, episode steps: 200, steps per second: 318, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000], mean observation: -0.268 [-1.093, 0.043], loss: 1.795114, mean_absolute_error: 17.820505, mean_q: -26.376450\n",
            "  3957/50000: episode: 20, duration: 0.640s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000], mean observation: -0.256 [-0.696, 0.017], loss: 2.096995, mean_absolute_error: 18.597681, mean_q: -27.538023\n",
            "  4157/50000: episode: 21, duration: 0.627s, episode steps: 200, steps per second: 319, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000], mean observation: -0.275 [-0.961, 0.030], loss: 2.598963, mean_absolute_error: 19.313745, mean_q: -28.577286\n",
            "  4297/50000: episode: 22, duration: 0.441s, episode steps: 140, steps per second: 317, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.493 [0.000, 2.000], mean observation: -0.209 [-0.980, 0.502], loss: 1.760230, mean_absolute_error: 19.848194, mean_q: -29.427847\n",
            "  4497/50000: episode: 23, duration: 0.633s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000], mean observation: -0.231 [-0.896, 0.145], loss: 1.615457, mean_absolute_error: 20.453175, mean_q: -30.377232\n",
            "  4674/50000: episode: 24, duration: 0.555s, episode steps: 177, steps per second: 319, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.062 [0.000, 2.000], mean observation: -0.249 [-1.200, 0.514], loss: 1.563961, mean_absolute_error: 21.163906, mean_q: -31.451134\n",
            "  4874/50000: episode: 25, duration: 0.630s, episode steps: 200, steps per second: 317, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.350 [0.000, 2.000], mean observation: -0.222 [-1.183, 0.159], loss: 1.722750, mean_absolute_error: 21.707685, mean_q: -32.175449\n",
            "  5074/50000: episode: 26, duration: 0.633s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000], mean observation: -0.274 [-0.928, 0.029], loss: 1.904461, mean_absolute_error: 22.366667, mean_q: -33.215427\n",
            "  5274/50000: episode: 27, duration: 0.629s, episode steps: 200, steps per second: 318, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 2.000], mean observation: -0.212 [-0.912, 0.032], loss: 2.737718, mean_absolute_error: 23.026911, mean_q: -34.111618\n",
            "  5474/50000: episode: 28, duration: 0.637s, episode steps: 200, steps per second: 314, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000], mean observation: -0.218 [-0.850, 0.035], loss: 2.813092, mean_absolute_error: 23.605108, mean_q: -34.966850\n",
            "  5674/50000: episode: 29, duration: 0.629s, episode steps: 200, steps per second: 318, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.340 [0.000, 2.000], mean observation: -0.224 [-1.200, 0.041], loss: 2.403297, mean_absolute_error: 24.116333, mean_q: -35.748753\n",
            "  5874/50000: episode: 30, duration: 0.625s, episode steps: 200, steps per second: 320, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.335 [0.000, 2.000], mean observation: -0.233 [-0.850, 0.022], loss: 1.868953, mean_absolute_error: 24.815065, mean_q: -36.842331\n",
            "  6040/50000: episode: 31, duration: 0.529s, episode steps: 166, steps per second: 314, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.416 [0.000, 2.000], mean observation: -0.195 [-0.916, 0.515], loss: 3.037278, mean_absolute_error: 25.327608, mean_q: -37.482151\n",
            "  6240/50000: episode: 32, duration: 0.631s, episode steps: 200, steps per second: 317, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.235 [0.000, 2.000], mean observation: -0.228 [-0.787, 0.032], loss: 4.181227, mean_absolute_error: 25.773540, mean_q: -38.115429\n",
            "  6440/50000: episode: 33, duration: 0.636s, episode steps: 200, steps per second: 315, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000], mean observation: -0.226 [-1.067, 0.327], loss: 2.220741, mean_absolute_error: 26.310328, mean_q: -39.076256\n",
            "  6640/50000: episode: 34, duration: 0.636s, episode steps: 200, steps per second: 314, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.250 [-0.831, 0.023], loss: 4.623747, mean_absolute_error: 26.812588, mean_q: -39.663651\n",
            "  6840/50000: episode: 35, duration: 0.634s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.270 [-1.119, 0.043], loss: 3.120218, mean_absolute_error: 27.242352, mean_q: -40.336449\n",
            "  7040/50000: episode: 36, duration: 0.638s, episode steps: 200, steps per second: 314, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.680 [0.000, 2.000], mean observation: -0.206 [-0.804, 0.020], loss: 2.549195, mean_absolute_error: 27.659571, mean_q: -41.048134\n",
            "  7190/50000: episode: 37, duration: 0.479s, episode steps: 150, steps per second: 313, episode reward: -150.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000], mean observation: -0.183 [-1.007, 0.505], loss: 2.515781, mean_absolute_error: 28.049335, mean_q: -41.577904\n",
            "  7390/50000: episode: 38, duration: 0.629s, episode steps: 200, steps per second: 318, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.500 [0.000, 2.000], mean observation: -0.219 [-1.104, 0.027], loss: 3.162198, mean_absolute_error: 28.471886, mean_q: -42.152569\n",
            "  7590/50000: episode: 39, duration: 0.636s, episode steps: 200, steps per second: 315, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000], mean observation: -0.254 [-0.853, 0.030], loss: 4.286418, mean_absolute_error: 28.870630, mean_q: -42.699703\n",
            "  7790/50000: episode: 40, duration: 0.629s, episode steps: 200, steps per second: 318, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.665 [0.000, 2.000], mean observation: -0.294 [-0.902, 0.018], loss: 3.871513, mean_absolute_error: 29.334335, mean_q: -43.380272\n",
            "  7990/50000: episode: 41, duration: 0.633s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000], mean observation: -0.207 [-0.907, 0.158], loss: 3.708614, mean_absolute_error: 29.792749, mean_q: -44.097881\n",
            "  8190/50000: episode: 42, duration: 0.631s, episode steps: 200, steps per second: 317, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.335 [0.000, 1.000], mean observation: -0.306 [-0.826, 0.020], loss: 4.171926, mean_absolute_error: 30.190048, mean_q: -44.679047\n",
            "  8390/50000: episode: 43, duration: 0.644s, episode steps: 200, steps per second: 310, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000], mean observation: -0.264 [-1.200, 0.102], loss: 3.568918, mean_absolute_error: 30.616051, mean_q: -45.315598\n",
            "  8590/50000: episode: 44, duration: 0.647s, episode steps: 200, steps per second: 309, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.243 [-0.905, 0.035], loss: 5.135259, mean_absolute_error: 30.938658, mean_q: -45.697365\n",
            "  8790/50000: episode: 45, duration: 0.649s, episode steps: 200, steps per second: 308, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.300 [0.000, 2.000], mean observation: -0.234 [-0.990, 0.037], loss: 5.867741, mean_absolute_error: 31.302973, mean_q: -46.230175\n",
            "  8990/50000: episode: 46, duration: 0.651s, episode steps: 200, steps per second: 307, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.470 [0.000, 2.000], mean observation: -0.194 [-0.724, 0.028], loss: 3.438949, mean_absolute_error: 31.631794, mean_q: -46.807098\n",
            "  9190/50000: episode: 47, duration: 0.641s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.380 [0.000, 2.000], mean observation: -0.316 [-1.062, 0.028], loss: 5.647086, mean_absolute_error: 31.966537, mean_q: -47.288700\n",
            "  9390/50000: episode: 48, duration: 0.643s, episode steps: 200, steps per second: 311, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.263 [-1.100, 0.067], loss: 4.995756, mean_absolute_error: 32.200626, mean_q: -47.639980\n",
            "  9590/50000: episode: 49, duration: 0.650s, episode steps: 200, steps per second: 308, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000], mean observation: -0.270 [-0.783, 0.026], loss: 4.692746, mean_absolute_error: 32.596695, mean_q: -48.270176\n",
            "  9790/50000: episode: 50, duration: 0.646s, episode steps: 200, steps per second: 310, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.555 [0.000, 2.000], mean observation: -0.221 [-0.757, 0.019], loss: 4.155242, mean_absolute_error: 32.973305, mean_q: -48.848011\n",
            "  9963/50000: episode: 51, duration: 0.560s, episode steps: 173, steps per second: 309, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.618 [0.000, 2.000], mean observation: -0.289 [-1.085, 0.502], loss: 3.868060, mean_absolute_error: 33.332470, mean_q: -49.400494\n",
            " 10163/50000: episode: 52, duration: 0.643s, episode steps: 200, steps per second: 311, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.375 [0.000, 2.000], mean observation: -0.302 [-0.779, 0.011], loss: 5.899716, mean_absolute_error: 33.455353, mean_q: -49.450249\n",
            " 10363/50000: episode: 53, duration: 0.648s, episode steps: 200, steps per second: 309, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.615 [0.000, 2.000], mean observation: -0.257 [-1.200, 0.388], loss: 7.344535, mean_absolute_error: 33.520397, mean_q: -49.351593\n",
            " 10563/50000: episode: 54, duration: 0.641s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000], mean observation: -0.234 [-1.200, 0.220], loss: 5.834185, mean_absolute_error: 33.821720, mean_q: -50.053905\n",
            " 10712/50000: episode: 55, duration: 0.479s, episode steps: 149, steps per second: 311, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.240 [-1.156, 0.506], loss: 4.816504, mean_absolute_error: 34.010021, mean_q: -50.309238\n",
            " 10912/50000: episode: 56, duration: 0.640s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000], mean observation: -0.287 [-0.991, 0.046], loss: 5.771681, mean_absolute_error: 34.137375, mean_q: -50.357906\n",
            " 11112/50000: episode: 57, duration: 0.640s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.485 [0.000, 2.000], mean observation: -0.293 [-1.200, 0.034], loss: 4.606407, mean_absolute_error: 34.323929, mean_q: -50.722099\n",
            " 11312/50000: episode: 58, duration: 0.647s, episode steps: 200, steps per second: 309, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000], mean observation: -0.285 [-0.922, 0.020], loss: 4.506209, mean_absolute_error: 34.721474, mean_q: -51.345978\n",
            " 11512/50000: episode: 59, duration: 0.650s, episode steps: 200, steps per second: 308, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.299], loss: 5.948231, mean_absolute_error: 35.096424, mean_q: -51.852905\n",
            " 11712/50000: episode: 60, duration: 0.657s, episode steps: 200, steps per second: 304, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.450 [0.000, 2.000], mean observation: -0.302 [-1.021, 0.026], loss: 5.451885, mean_absolute_error: 35.026600, mean_q: -51.703983\n",
            " 11912/50000: episode: 61, duration: 0.631s, episode steps: 200, steps per second: 317, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.660 [0.000, 2.000], mean observation: -0.281 [-0.894, 0.018], loss: 5.543056, mean_absolute_error: 35.319530, mean_q: -52.185497\n",
            " 12036/50000: episode: 62, duration: 0.394s, episode steps: 124, steps per second: 315, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000], mean observation: -0.154 [-0.902, 0.502], loss: 6.925714, mean_absolute_error: 35.496819, mean_q: -52.350117\n",
            " 12236/50000: episode: 63, duration: 0.629s, episode steps: 200, steps per second: 318, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000], mean observation: -0.195 [-1.200, 0.308], loss: 5.347995, mean_absolute_error: 35.495193, mean_q: -52.334015\n",
            " 12436/50000: episode: 64, duration: 0.633s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000], mean observation: -0.268 [-1.200, 0.118], loss: 5.437031, mean_absolute_error: 35.640060, mean_q: -52.547821\n",
            " 12634/50000: episode: 65, duration: 0.631s, episode steps: 198, steps per second: 314, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.879 [0.000, 2.000], mean observation: -0.229 [-1.200, 0.510], loss: 4.823073, mean_absolute_error: 35.889999, mean_q: -52.941357\n",
            " 12834/50000: episode: 66, duration: 0.638s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000], mean observation: -0.294 [-0.967, 0.023], loss: 4.900772, mean_absolute_error: 36.159336, mean_q: -53.281395\n",
            " 13034/50000: episode: 67, duration: 0.637s, episode steps: 200, steps per second: 314, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.655 [0.000, 2.000], mean observation: -0.289 [-1.040, 0.027], loss: 5.819345, mean_absolute_error: 36.214756, mean_q: -53.357265\n",
            " 13234/50000: episode: 68, duration: 0.628s, episode steps: 200, steps per second: 318, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.650 [0.000, 2.000], mean observation: -0.308 [-1.069, 0.027], loss: 5.004574, mean_absolute_error: 36.330795, mean_q: -53.548164\n",
            " 13434/50000: episode: 69, duration: 0.632s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.225 [0.000, 2.000], mean observation: -0.314 [-0.920, 0.020], loss: 5.409558, mean_absolute_error: 36.704117, mean_q: -54.157360\n",
            " 13594/50000: episode: 70, duration: 0.507s, episode steps: 160, steps per second: 316, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.981 [0.000, 2.000], mean observation: -0.243 [-0.958, 0.511], loss: 6.131798, mean_absolute_error: 36.824318, mean_q: -54.249489\n",
            " 13794/50000: episode: 71, duration: 0.646s, episode steps: 200, steps per second: 309, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000], mean observation: -0.288 [-1.200, 0.027], loss: 7.501524, mean_absolute_error: 36.995346, mean_q: -54.441906\n",
            " 13994/50000: episode: 72, duration: 0.656s, episode steps: 200, steps per second: 305, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.470 [0.000, 2.000], mean observation: -0.297 [-0.890, 0.018], loss: 5.614180, mean_absolute_error: 36.939995, mean_q: -54.386181\n",
            " 14111/50000: episode: 73, duration: 0.377s, episode steps: 117, steps per second: 310, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000], mean observation: -0.149 [-0.948, 0.504], loss: 4.784330, mean_absolute_error: 37.318222, mean_q: -55.073517\n",
            " 14311/50000: episode: 74, duration: 0.638s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.380 [0.000, 2.000], mean observation: -0.316 [-0.980, 0.024], loss: 4.794776, mean_absolute_error: 37.084084, mean_q: -54.636612\n",
            " 14503/50000: episode: 75, duration: 0.614s, episode steps: 192, steps per second: 313, episode reward: -192.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.297 [0.000, 2.000], mean observation: -0.244 [-1.200, 0.542], loss: 8.255775, mean_absolute_error: 37.242065, mean_q: -54.757626\n",
            " 14703/50000: episode: 76, duration: 0.652s, episode steps: 200, steps per second: 307, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.425 [0.000, 2.000], mean observation: -0.280 [-0.810, 0.026], loss: 6.489463, mean_absolute_error: 37.142689, mean_q: -54.679482\n",
            " 14903/50000: episode: 77, duration: 0.640s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.225 [0.000, 2.000], mean observation: -0.164 [-1.200, 0.336], loss: 8.474198, mean_absolute_error: 37.154171, mean_q: -54.624527\n",
            " 15103/50000: episode: 78, duration: 0.652s, episode steps: 200, steps per second: 307, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000], mean observation: -0.254 [-0.994, 0.038], loss: 6.576985, mean_absolute_error: 37.299225, mean_q: -54.988388\n",
            " 15227/50000: episode: 79, duration: 0.404s, episode steps: 124, steps per second: 307, episode reward: -124.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000], mean observation: -0.257 [-1.112, 0.542], loss: 6.877301, mean_absolute_error: 37.291691, mean_q: -54.854939\n",
            " 15427/50000: episode: 80, duration: 0.658s, episode steps: 200, steps per second: 304, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000], mean observation: -0.241 [-1.111, 0.112], loss: 5.337509, mean_absolute_error: 37.220959, mean_q: -54.904465\n",
            " 15627/50000: episode: 81, duration: 0.649s, episode steps: 200, steps per second: 308, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.780 [0.000, 2.000], mean observation: -0.273 [-0.872, 0.017], loss: 5.205178, mean_absolute_error: 37.388676, mean_q: -55.116924\n",
            " 15785/50000: episode: 82, duration: 0.513s, episode steps: 158, steps per second: 308, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.354 [0.000, 2.000], mean observation: -0.195 [-0.931, 0.504], loss: 6.489507, mean_absolute_error: 37.390060, mean_q: -55.070206\n",
            " 15930/50000: episode: 83, duration: 0.462s, episode steps: 145, steps per second: 314, episode reward: -145.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000], mean observation: -0.268 [-1.107, 0.521], loss: 7.279618, mean_absolute_error: 37.325077, mean_q: -54.847260\n",
            " 16130/50000: episode: 84, duration: 0.642s, episode steps: 200, steps per second: 311, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.165 [0.000, 2.000], mean observation: -0.329 [-0.766, 0.024], loss: 6.220233, mean_absolute_error: 37.400021, mean_q: -55.101437\n",
            " 16297/50000: episode: 85, duration: 0.550s, episode steps: 167, steps per second: 304, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.144 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.501], loss: 4.765439, mean_absolute_error: 37.189243, mean_q: -54.867004\n",
            " 16487/50000: episode: 86, duration: 0.620s, episode steps: 190, steps per second: 306, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.284 [0.000, 2.000], mean observation: -0.188 [-0.880, 0.512], loss: 7.085190, mean_absolute_error: 37.246025, mean_q: -54.824795\n",
            " 16651/50000: episode: 87, duration: 0.535s, episode steps: 164, steps per second: 307, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.006 [0.000, 2.000], mean observation: -0.277 [-1.187, 0.540], loss: 5.671376, mean_absolute_error: 37.252686, mean_q: -54.950832\n",
            " 16820/50000: episode: 88, duration: 0.542s, episode steps: 169, steps per second: 312, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.941 [0.000, 2.000], mean observation: -0.270 [-1.200, 0.506], loss: 6.490224, mean_absolute_error: 37.192127, mean_q: -54.827160\n",
            " 17020/50000: episode: 89, duration: 0.640s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.265 [0.000, 2.000], mean observation: -0.209 [-0.937, 0.097], loss: 6.465466, mean_absolute_error: 37.101761, mean_q: -54.740284\n",
            " 17172/50000: episode: 90, duration: 0.487s, episode steps: 152, steps per second: 312, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.178 [0.000, 2.000], mean observation: -0.236 [-1.002, 0.518], loss: 6.951085, mean_absolute_error: 37.003063, mean_q: -54.603825\n",
            " 17323/50000: episode: 91, duration: 0.481s, episode steps: 151, steps per second: 314, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.179 [0.000, 2.000], mean observation: -0.218 [-1.200, 0.537], loss: 6.192822, mean_absolute_error: 36.878922, mean_q: -54.416386\n",
            " 17507/50000: episode: 92, duration: 0.589s, episode steps: 184, steps per second: 312, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.685 [0.000, 2.000], mean observation: -0.193 [-0.973, 0.507], loss: 5.269830, mean_absolute_error: 36.990234, mean_q: -54.540455\n",
            " 17671/50000: episode: 93, duration: 0.526s, episode steps: 164, steps per second: 312, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.280 [0.000, 2.000], mean observation: -0.212 [-1.200, 0.537], loss: 5.649854, mean_absolute_error: 36.763382, mean_q: -54.262367\n",
            " 17860/50000: episode: 94, duration: 0.606s, episode steps: 189, steps per second: 312, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.778 [0.000, 2.000], mean observation: -0.260 [-0.961, 0.523], loss: 4.244027, mean_absolute_error: 36.638439, mean_q: -54.165165\n",
            " 18039/50000: episode: 95, duration: 0.575s, episode steps: 179, steps per second: 311, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.223 [0.000, 2.000], mean observation: -0.230 [-0.983, 0.516], loss: 4.395824, mean_absolute_error: 36.574444, mean_q: -54.065872\n",
            " 18239/50000: episode: 96, duration: 0.636s, episode steps: 200, steps per second: 314, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.438], loss: 5.802257, mean_absolute_error: 36.638462, mean_q: -54.090633\n",
            " 18345/50000: episode: 97, duration: 0.346s, episode steps: 106, steps per second: 306, episode reward: -106.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000], mean observation: -0.158 [-0.869, 0.502], loss: 7.797442, mean_absolute_error: 36.673233, mean_q: -54.088577\n",
            " 18459/50000: episode: 98, duration: 0.371s, episode steps: 114, steps per second: 307, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.456 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.537], loss: 3.687785, mean_absolute_error: 36.354572, mean_q: -53.648617\n",
            " 18643/50000: episode: 99, duration: 0.593s, episode steps: 184, steps per second: 310, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.908 [0.000, 2.000], mean observation: -0.243 [-1.200, 0.537], loss: 4.970115, mean_absolute_error: 36.205223, mean_q: -53.515331\n",
            " 18824/50000: episode: 100, duration: 0.584s, episode steps: 181, steps per second: 310, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.387 [0.000, 2.000], mean observation: -0.216 [-1.200, 0.537], loss: 5.686642, mean_absolute_error: 36.321198, mean_q: -53.634171\n",
            " 18984/50000: episode: 101, duration: 0.514s, episode steps: 160, steps per second: 311, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.894 [0.000, 2.000], mean observation: -0.275 [-1.139, 0.540], loss: 4.372611, mean_absolute_error: 36.099663, mean_q: -53.356873\n",
            " 19124/50000: episode: 102, duration: 0.458s, episode steps: 140, steps per second: 306, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000], mean observation: -0.242 [-0.992, 0.516], loss: 6.254283, mean_absolute_error: 36.197540, mean_q: -53.415096\n",
            " 19266/50000: episode: 103, duration: 0.458s, episode steps: 142, steps per second: 310, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.162 [0.000, 2.000], mean observation: -0.193 [-0.876, 0.509], loss: 4.424928, mean_absolute_error: 35.894283, mean_q: -53.027355\n",
            " 19361/50000: episode: 104, duration: 0.310s, episode steps: 95, steps per second: 306, episode reward: -95.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.242 [0.000, 2.000], mean observation: -0.158 [-0.874, 0.505], loss: 0.975776, mean_absolute_error: 35.983898, mean_q: -53.332996\n",
            " 19531/50000: episode: 105, duration: 0.551s, episode steps: 170, steps per second: 309, episode reward: -170.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000], mean observation: -0.177 [-1.200, 0.536], loss: 5.535291, mean_absolute_error: 36.127529, mean_q: -53.400734\n",
            " 19620/50000: episode: 106, duration: 0.291s, episode steps: 89, steps per second: 306, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000], mean observation: -0.215 [-0.939, 0.522], loss: 5.530180, mean_absolute_error: 36.196049, mean_q: -53.476753\n",
            " 19776/50000: episode: 107, duration: 0.506s, episode steps: 156, steps per second: 308, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.769 [0.000, 2.000], mean observation: -0.296 [-1.185, 0.520], loss: 5.347064, mean_absolute_error: 35.728199, mean_q: -52.676826\n",
            " 19868/50000: episode: 108, duration: 0.302s, episode steps: 92, steps per second: 304, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.283 [0.000, 2.000], mean observation: -0.157 [-0.878, 0.503], loss: 7.190439, mean_absolute_error: 35.309761, mean_q: -51.975815\n",
            " 20068/50000: episode: 109, duration: 0.664s, episode steps: 200, steps per second: 301, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.337 [-0.794, 0.011], loss: 3.047468, mean_absolute_error: 35.776230, mean_q: -52.907990\n",
            " 20223/50000: episode: 110, duration: 0.500s, episode steps: 155, steps per second: 310, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.800 [0.000, 2.000], mean observation: -0.274 [-1.068, 0.516], loss: 3.273107, mean_absolute_error: 35.805298, mean_q: -52.972870\n",
            " 20423/50000: episode: 111, duration: 0.652s, episode steps: 200, steps per second: 307, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.335 [-0.755, 0.008], loss: 4.092256, mean_absolute_error: 35.595314, mean_q: -52.555832\n",
            " 20623/50000: episode: 112, duration: 0.642s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.045 [0.000, 1.000], mean observation: -0.337 [-0.826, 0.013], loss: 2.797465, mean_absolute_error: 35.659775, mean_q: -52.694778\n",
            " 20823/50000: episode: 113, duration: 0.644s, episode steps: 200, steps per second: 311, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.140 [0.000, 1.000], mean observation: -0.320 [-0.866, 0.016], loss: 4.552054, mean_absolute_error: 35.683990, mean_q: -52.649776\n",
            " 20938/50000: episode: 114, duration: 0.372s, episode steps: 115, steps per second: 309, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.391 [0.000, 2.000], mean observation: -0.094 [-0.852, 0.506], loss: 6.715372, mean_absolute_error: 35.257477, mean_q: -51.960133\n",
            " 21138/50000: episode: 115, duration: 0.640s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.010 [0.000, 2.000], mean observation: -0.341 [-0.888, 0.018], loss: 3.257126, mean_absolute_error: 35.556385, mean_q: -52.552456\n",
            " 21284/50000: episode: 116, duration: 0.476s, episode steps: 146, steps per second: 307, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.973 [0.000, 2.000], mean observation: -0.262 [-1.084, 0.533], loss: 5.757928, mean_absolute_error: 35.600601, mean_q: -52.612232\n",
            " 21387/50000: episode: 117, duration: 0.339s, episode steps: 103, steps per second: 304, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000], mean observation: -0.208 [-0.920, 0.503], loss: 3.549395, mean_absolute_error: 35.751129, mean_q: -52.844353\n",
            " 21587/50000: episode: 118, duration: 0.649s, episode steps: 200, steps per second: 308, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.240 [0.000, 1.000], mean observation: -0.317 [-0.847, 0.015], loss: 5.308834, mean_absolute_error: 35.675926, mean_q: -52.660694\n",
            " 21787/50000: episode: 119, duration: 0.644s, episode steps: 200, steps per second: 311, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.050 [0.000, 2.000], mean observation: -0.330 [-0.745, 0.007], loss: 3.923991, mean_absolute_error: 35.602333, mean_q: -52.551056\n",
            " 21987/50000: episode: 120, duration: 0.645s, episode steps: 200, steps per second: 310, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 1.000], mean observation: -0.336 [-0.853, 0.015], loss: 3.622045, mean_absolute_error: 35.649345, mean_q: -52.670845\n",
            " 22187/50000: episode: 121, duration: 0.640s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.180 [0.000, 2.000], mean observation: -0.320 [-0.927, 0.016], loss: 3.686524, mean_absolute_error: 35.741699, mean_q: -52.714043\n",
            " 22387/50000: episode: 122, duration: 0.631s, episode steps: 200, steps per second: 317, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.010 [0.000, 2.000], mean observation: -0.336 [-0.789, 0.010], loss: 3.981745, mean_absolute_error: 35.624752, mean_q: -52.589283\n",
            " 22587/50000: episode: 123, duration: 0.642s, episode steps: 200, steps per second: 312, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000], mean observation: -0.307 [-1.198, 0.062], loss: 4.411754, mean_absolute_error: 35.760544, mean_q: -52.776039\n",
            " 22759/50000: episode: 124, duration: 0.550s, episode steps: 172, steps per second: 313, episode reward: -172.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.157 [0.000, 2.000], mean observation: -0.218 [-1.200, 0.534], loss: 4.357010, mean_absolute_error: 35.815567, mean_q: -52.865540\n",
            " 22959/50000: episode: 125, duration: 0.633s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000], mean observation: -0.332 [-0.816, 0.012], loss: 4.344353, mean_absolute_error: 35.708828, mean_q: -52.673206\n",
            " 23159/50000: episode: 126, duration: 0.632s, episode steps: 200, steps per second: 317, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.085 [0.000, 2.000], mean observation: -0.332 [-0.795, 0.016], loss: 3.749241, mean_absolute_error: 35.881626, mean_q: -52.968853\n",
            " 23359/50000: episode: 127, duration: 0.627s, episode steps: 200, steps per second: 319, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.040 [0.000, 1.000], mean observation: -0.337 [-0.899, 0.018], loss: 3.835189, mean_absolute_error: 36.024521, mean_q: -53.221249\n",
            " 23500/50000: episode: 128, duration: 0.445s, episode steps: 141, steps per second: 317, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.536], loss: 4.514674, mean_absolute_error: 35.780708, mean_q: -52.812790\n",
            " 23700/50000: episode: 129, duration: 0.637s, episode steps: 200, steps per second: 314, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.000 [0.000, 0.000], mean observation: -0.338 [-0.816, 0.012], loss: 5.908670, mean_absolute_error: 35.774387, mean_q: -52.705101\n",
            " 23862/50000: episode: 130, duration: 0.516s, episode steps: 162, steps per second: 314, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.253 [0.000, 2.000], mean observation: -0.217 [-1.200, 0.536], loss: 5.392893, mean_absolute_error: 35.850201, mean_q: -52.881775\n",
            " 24003/50000: episode: 131, duration: 0.446s, episode steps: 141, steps per second: 316, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000], mean observation: -0.241 [-1.200, 0.536], loss: 4.301816, mean_absolute_error: 35.658661, mean_q: -52.590588\n",
            " 24107/50000: episode: 132, duration: 0.335s, episode steps: 104, steps per second: 311, episode reward: -104.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.173 [0.000, 2.000], mean observation: -0.154 [-0.871, 0.511], loss: 5.234883, mean_absolute_error: 35.735794, mean_q: -52.673550\n",
            " 24241/50000: episode: 133, duration: 0.428s, episode steps: 134, steps per second: 313, episode reward: -134.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.216 [0.000, 2.000], mean observation: -0.244 [-1.200, 0.536], loss: 6.552021, mean_absolute_error: 35.604187, mean_q: -52.476765\n",
            " 24416/50000: episode: 134, duration: 0.572s, episode steps: 175, steps per second: 306, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.207 [-1.200, 0.534], loss: 5.351775, mean_absolute_error: 35.425117, mean_q: -52.256931\n",
            " 24592/50000: episode: 135, duration: 0.555s, episode steps: 176, steps per second: 317, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.159 [0.000, 2.000], mean observation: -0.189 [-1.200, 0.534], loss: 4.315964, mean_absolute_error: 35.457146, mean_q: -52.292679\n",
            " 24753/50000: episode: 136, duration: 0.525s, episode steps: 161, steps per second: 307, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000], mean observation: -0.210 [-0.915, 0.502], loss: 6.082504, mean_absolute_error: 35.380028, mean_q: -52.201664\n",
            " 24839/50000: episode: 137, duration: 0.280s, episode steps: 86, steps per second: 307, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.256 [0.000, 2.000], mean observation: -0.180 [-0.922, 0.512], loss: 3.895657, mean_absolute_error: 35.191685, mean_q: -51.932384\n",
            " 24990/50000: episode: 138, duration: 0.478s, episode steps: 151, steps per second: 316, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.531], loss: 3.503289, mean_absolute_error: 35.547146, mean_q: -52.533436\n",
            " 25078/50000: episode: 139, duration: 0.283s, episode steps: 88, steps per second: 311, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000], mean observation: -0.167 [-0.895, 0.519], loss: 4.263206, mean_absolute_error: 35.112408, mean_q: -51.841732\n",
            " 25278/50000: episode: 140, duration: 0.634s, episode steps: 200, steps per second: 316, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.775 [0.000, 2.000], mean observation: -0.289 [-1.200, 0.062], loss: 4.299478, mean_absolute_error: 35.485435, mean_q: -52.388760\n",
            " 25385/50000: episode: 141, duration: 0.350s, episode steps: 107, steps per second: 306, episode reward: -107.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.191 [-0.905, 0.500], loss: 3.438920, mean_absolute_error: 34.998905, mean_q: -51.753647\n",
            " 25531/50000: episode: 142, duration: 0.472s, episode steps: 146, steps per second: 309, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.897 [0.000, 2.000], mean observation: -0.260 [-1.041, 0.536], loss: 3.465443, mean_absolute_error: 35.231472, mean_q: -52.045227\n",
            " 25691/50000: episode: 143, duration: 0.519s, episode steps: 160, steps per second: 308, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000], mean observation: -0.252 [-0.976, 0.526], loss: 5.598979, mean_absolute_error: 35.652203, mean_q: -52.588585\n",
            " 25855/50000: episode: 144, duration: 0.531s, episode steps: 164, steps per second: 309, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.152 [0.000, 2.000], mean observation: -0.207 [-1.200, 0.531], loss: 3.156112, mean_absolute_error: 35.536560, mean_q: -52.508419\n",
            " 25940/50000: episode: 145, duration: 0.276s, episode steps: 85, steps per second: 308, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.197 [-0.926, 0.515], loss: 4.261484, mean_absolute_error: 35.438553, mean_q: -52.301598\n",
            " 26094/50000: episode: 146, duration: 0.507s, episode steps: 154, steps per second: 304, episode reward: -154.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000], mean observation: -0.230 [-1.200, 0.531], loss: 3.869671, mean_absolute_error: 35.404057, mean_q: -52.245258\n",
            " 26178/50000: episode: 147, duration: 0.274s, episode steps: 84, steps per second: 306, episode reward: -84.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000], mean observation: -0.198 [-0.931, 0.515], loss: 4.447555, mean_absolute_error: 35.228760, mean_q: -52.054890\n",
            " 26329/50000: episode: 148, duration: 0.480s, episode steps: 151, steps per second: 315, episode reward: -151.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.192 [0.000, 2.000], mean observation: -0.242 [-1.200, 0.531], loss: 3.953374, mean_absolute_error: 35.285717, mean_q: -52.141174\n",
            " 26513/50000: episode: 149, duration: 0.593s, episode steps: 184, steps per second: 310, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.359 [0.000, 2.000], mean observation: -0.134 [-1.200, 0.527], loss: 3.259198, mean_absolute_error: 35.394417, mean_q: -52.333633\n",
            " 26598/50000: episode: 150, duration: 0.277s, episode steps: 85, steps per second: 307, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.202 [-0.935, 0.522], loss: 5.502374, mean_absolute_error: 35.257553, mean_q: -52.074287\n",
            " 26708/50000: episode: 151, duration: 0.350s, episode steps: 110, steps per second: 314, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.382 [0.000, 2.000], mean observation: -0.104 [-0.853, 0.502], loss: 4.860378, mean_absolute_error: 35.168846, mean_q: -51.864765\n",
            " 26839/50000: episode: 152, duration: 0.425s, episode steps: 131, steps per second: 308, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.412 [0.000, 2.000], mean observation: -0.219 [-1.000, 0.505], loss: 3.932257, mean_absolute_error: 35.403416, mean_q: -52.272415\n",
            " 26924/50000: episode: 153, duration: 0.281s, episode steps: 85, steps per second: 303, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.207 [-0.942, 0.525], loss: 4.639668, mean_absolute_error: 35.241341, mean_q: -52.048069\n",
            " 27079/50000: episode: 154, duration: 0.499s, episode steps: 155, steps per second: 310, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.942 [0.000, 2.000], mean observation: -0.252 [-1.200, 0.527], loss: 3.976480, mean_absolute_error: 35.148048, mean_q: -51.889179\n",
            " 27222/50000: episode: 155, duration: 0.461s, episode steps: 143, steps per second: 310, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.112 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.527], loss: 4.567247, mean_absolute_error: 35.038502, mean_q: -51.655937\n",
            " 27422/50000: episode: 156, duration: 0.636s, episode steps: 200, steps per second: 314, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.210 [0.000, 2.000], mean observation: -0.323 [-0.772, 0.019], loss: 5.531841, mean_absolute_error: 35.216972, mean_q: -51.817982\n",
            " 27578/50000: episode: 157, duration: 0.506s, episode steps: 156, steps per second: 308, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.141 [0.000, 2.000], mean observation: -0.215 [-1.200, 0.531], loss: 6.798607, mean_absolute_error: 35.288982, mean_q: -51.902729\n",
            " 27768/50000: episode: 158, duration: 0.605s, episode steps: 190, steps per second: 314, episode reward: -190.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.208 [-1.200, 0.525], loss: 4.911688, mean_absolute_error: 35.022072, mean_q: -51.629463\n",
            " 27916/50000: episode: 159, duration: 0.473s, episode steps: 148, steps per second: 313, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.189 [0.000, 2.000], mean observation: -0.205 [-1.200, 0.531], loss: 2.907896, mean_absolute_error: 35.130825, mean_q: -51.822536\n",
            " 28116/50000: episode: 160, duration: 0.635s, episode steps: 200, steps per second: 315, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.250 [0.000, 2.000], mean observation: -0.318 [-0.751, 0.012], loss: 5.080164, mean_absolute_error: 35.122204, mean_q: -51.729717\n",
            " 28290/50000: episode: 161, duration: 0.556s, episode steps: 174, steps per second: 313, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.121 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.531], loss: 4.577632, mean_absolute_error: 34.980080, mean_q: -51.490547\n",
            " 28448/50000: episode: 162, duration: 0.511s, episode steps: 158, steps per second: 309, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.247 [0.000, 2.000], mean observation: -0.205 [-1.200, 0.531], loss: 8.115085, mean_absolute_error: 34.997925, mean_q: -51.391529\n",
            " 28567/50000: episode: 163, duration: 0.384s, episode steps: 119, steps per second: 310, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.622 [0.000, 2.000], mean observation: -0.192 [-0.977, 0.516], loss: 6.061989, mean_absolute_error: 34.914925, mean_q: -51.316875\n",
            " 28726/50000: episode: 164, duration: 0.506s, episode steps: 159, steps per second: 314, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.257 [-1.200, 0.527], loss: 5.322198, mean_absolute_error: 34.881603, mean_q: -51.323685\n",
            " 28838/50000: episode: 165, duration: 0.361s, episode steps: 112, steps per second: 310, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000], mean observation: -0.248 [-1.141, 0.534], loss: 3.128446, mean_absolute_error: 34.876530, mean_q: -51.423679\n",
            " 29038/50000: episode: 166, duration: 0.636s, episode steps: 200, steps per second: 315, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000], mean observation: -0.232 [-0.824, 0.215], loss: 3.466652, mean_absolute_error: 34.884491, mean_q: -51.406307\n",
            " 29203/50000: episode: 167, duration: 0.526s, episode steps: 165, steps per second: 313, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.897 [0.000, 2.000], mean observation: -0.254 [-1.200, 0.531], loss: 4.038701, mean_absolute_error: 35.367252, mean_q: -52.109779\n",
            " 29372/50000: episode: 168, duration: 0.535s, episode steps: 169, steps per second: 316, episode reward: -169.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000], mean observation: -0.244 [-1.200, 0.513], loss: 5.604905, mean_absolute_error: 34.890961, mean_q: -51.165936\n",
            " 29480/50000: episode: 169, duration: 0.348s, episode steps: 108, steps per second: 311, episode reward: -108.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.074 [0.000, 2.000], mean observation: -0.229 [-0.967, 0.523], loss: 5.876812, mean_absolute_error: 35.031105, mean_q: -51.495613\n",
            " 29658/50000: episode: 170, duration: 0.570s, episode steps: 178, steps per second: 312, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.989 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.525], loss: 5.630064, mean_absolute_error: 34.784405, mean_q: -51.170345\n",
            " 29805/50000: episode: 171, duration: 0.469s, episode steps: 147, steps per second: 313, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.109 [0.000, 2.000], mean observation: -0.262 [-1.200, 0.531], loss: 3.182319, mean_absolute_error: 34.358849, mean_q: -50.602802\n",
            " 29949/50000: episode: 172, duration: 0.465s, episode steps: 144, steps per second: 310, episode reward: -144.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.531], loss: 4.798755, mean_absolute_error: 34.556999, mean_q: -50.812443\n",
            " 30084/50000: episode: 173, duration: 0.425s, episode steps: 135, steps per second: 318, episode reward: -135.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.311 [0.000, 2.000], mean observation: -0.218 [-0.969, 0.515], loss: 3.496379, mean_absolute_error: 34.726456, mean_q: -51.195904\n",
            " 30195/50000: episode: 174, duration: 0.352s, episode steps: 111, steps per second: 315, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.171 [0.000, 2.000], mean observation: -0.273 [-1.173, 0.504], loss: 5.500022, mean_absolute_error: 34.794724, mean_q: -51.108295\n",
            " 30342/50000: episode: 175, duration: 0.486s, episode steps: 147, steps per second: 303, episode reward: -147.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.222 [-0.912, 0.519], loss: 4.840641, mean_absolute_error: 34.567760, mean_q: -50.954075\n",
            " 30542/50000: episode: 176, duration: 0.638s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000], mean observation: -0.268 [-1.200, 0.050], loss: 5.526203, mean_absolute_error: 34.977085, mean_q: -51.487370\n",
            " 30702/50000: episode: 177, duration: 0.514s, episode steps: 160, steps per second: 311, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000], mean observation: -0.222 [-1.200, 0.527], loss: 4.132407, mean_absolute_error: 34.753498, mean_q: -51.228050\n",
            " 30883/50000: episode: 178, duration: 0.583s, episode steps: 181, steps per second: 310, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.122 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.534], loss: 3.957315, mean_absolute_error: 34.931927, mean_q: -51.551350\n",
            " 31029/50000: episode: 179, duration: 0.470s, episode steps: 146, steps per second: 310, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.534], loss: 4.897337, mean_absolute_error: 34.776299, mean_q: -51.286442\n",
            " 31168/50000: episode: 180, duration: 0.443s, episode steps: 139, steps per second: 314, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.209 [0.000, 2.000], mean observation: -0.233 [-1.200, 0.531], loss: 3.506142, mean_absolute_error: 34.898579, mean_q: -51.480179\n",
            " 31309/50000: episode: 181, duration: 0.444s, episode steps: 141, steps per second: 317, episode reward: -141.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.235 [-1.200, 0.531], loss: 6.586249, mean_absolute_error: 34.699448, mean_q: -51.069691\n",
            " 31403/50000: episode: 182, duration: 0.299s, episode steps: 94, steps per second: 315, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000], mean observation: -0.191 [-0.899, 0.503], loss: 3.735344, mean_absolute_error: 34.475769, mean_q: -50.963505\n",
            " 31506/50000: episode: 183, duration: 0.336s, episode steps: 103, steps per second: 307, episode reward: -103.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.262 [0.000, 2.000], mean observation: -0.138 [-0.862, 0.508], loss: 5.816021, mean_absolute_error: 34.589188, mean_q: -51.030918\n",
            " 31608/50000: episode: 184, duration: 0.338s, episode steps: 102, steps per second: 302, episode reward: -102.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.176 [0.000, 2.000], mean observation: -0.218 [-0.951, 0.508], loss: 6.310797, mean_absolute_error: 34.649708, mean_q: -50.932648\n",
            " 31808/50000: episode: 185, duration: 0.639s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.605 [0.000, 2.000], mean observation: -0.193 [-0.766, 0.049], loss: 3.767928, mean_absolute_error: 34.231152, mean_q: -50.504990\n",
            " 31896/50000: episode: 186, duration: 0.288s, episode steps: 88, steps per second: 306, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.215 [-0.938, 0.514], loss: 1.930451, mean_absolute_error: 34.586132, mean_q: -51.136795\n",
            " 32007/50000: episode: 187, duration: 0.364s, episode steps: 111, steps per second: 305, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.342 [0.000, 2.000], mean observation: -0.237 [-1.081, 0.535], loss: 3.210126, mean_absolute_error: 34.698498, mean_q: -51.347206\n",
            " 32108/50000: episode: 188, duration: 0.329s, episode steps: 101, steps per second: 307, episode reward: -101.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.248 [0.000, 2.000], mean observation: -0.196 [-0.920, 0.520], loss: 1.759229, mean_absolute_error: 34.411766, mean_q: -50.926888\n",
            " 32276/50000: episode: 189, duration: 0.542s, episode steps: 168, steps per second: 310, episode reward: -168.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000], mean observation: -0.224 [-1.200, 0.504], loss: 4.885785, mean_absolute_error: 34.614388, mean_q: -51.054478\n",
            " 32424/50000: episode: 190, duration: 0.483s, episode steps: 148, steps per second: 307, episode reward: -148.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.034 [0.000, 2.000], mean observation: -0.221 [-1.200, 0.506], loss: 5.191047, mean_absolute_error: 34.494064, mean_q: -50.815403\n",
            " 32605/50000: episode: 191, duration: 0.578s, episode steps: 181, steps per second: 313, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000], mean observation: -0.127 [-1.200, 0.534], loss: 4.265642, mean_absolute_error: 34.491623, mean_q: -50.896744\n",
            " 32761/50000: episode: 192, duration: 0.508s, episode steps: 156, steps per second: 307, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.186 [0.000, 2.000], mean observation: -0.183 [-1.200, 0.534], loss: 4.539758, mean_absolute_error: 34.463928, mean_q: -50.860542\n",
            " 32946/50000: episode: 193, duration: 0.590s, episode steps: 185, steps per second: 314, episode reward: -185.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.432 [0.000, 2.000], mean observation: -0.220 [-1.200, 0.531], loss: 3.004760, mean_absolute_error: 34.544159, mean_q: -51.040298\n",
            " 33098/50000: episode: 194, duration: 0.487s, episode steps: 152, steps per second: 312, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.914 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.525], loss: 3.901982, mean_absolute_error: 34.483559, mean_q: -50.965942\n",
            " 33237/50000: episode: 195, duration: 0.448s, episode steps: 139, steps per second: 310, episode reward: -139.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.505], loss: 4.918343, mean_absolute_error: 34.478195, mean_q: -50.876972\n",
            " 33350/50000: episode: 196, duration: 0.367s, episode steps: 113, steps per second: 308, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000], mean observation: -0.252 [-1.102, 0.527], loss: 3.405660, mean_absolute_error: 34.523731, mean_q: -51.022137\n",
            " 33493/50000: episode: 197, duration: 0.465s, episode steps: 143, steps per second: 308, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.219 [-1.200, 0.539], loss: 3.563640, mean_absolute_error: 34.538055, mean_q: -51.058212\n",
            " 33614/50000: episode: 198, duration: 0.394s, episode steps: 121, steps per second: 307, episode reward: -121.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.091 [0.000, 2.000], mean observation: -0.265 [-1.200, 0.531], loss: 5.627556, mean_absolute_error: 34.294319, mean_q: -50.625443\n",
            " 33732/50000: episode: 199, duration: 0.385s, episode steps: 118, steps per second: 306, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.203 [0.000, 2.000], mean observation: -0.258 [-1.154, 0.529], loss: 1.363480, mean_absolute_error: 34.375488, mean_q: -50.931866\n",
            " 33817/50000: episode: 200, duration: 0.275s, episode steps: 85, steps per second: 309, episode reward: -85.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.047 [0.000, 2.000], mean observation: -0.215 [-0.946, 0.504], loss: 1.741338, mean_absolute_error: 34.172733, mean_q: -50.573071\n",
            " 33979/50000: episode: 201, duration: 0.516s, episode steps: 162, steps per second: 314, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.735 [0.000, 2.000], mean observation: -0.280 [-1.200, 0.540], loss: 4.261807, mean_absolute_error: 34.172832, mean_q: -50.508511\n",
            " 34143/50000: episode: 202, duration: 0.528s, episode steps: 164, steps per second: 311, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.957 [0.000, 2.000], mean observation: -0.188 [-1.200, 0.503], loss: 4.538732, mean_absolute_error: 33.904106, mean_q: -50.078476\n",
            " 34280/50000: episode: 203, duration: 0.453s, episode steps: 137, steps per second: 302, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000], mean observation: -0.242 [-1.200, 0.536], loss: 3.446241, mean_absolute_error: 34.035816, mean_q: -50.238987\n",
            " 34432/50000: episode: 204, duration: 0.493s, episode steps: 152, steps per second: 309, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.007 [0.000, 2.000], mean observation: -0.227 [-1.200, 0.521], loss: 4.966422, mean_absolute_error: 34.090057, mean_q: -50.331383\n",
            " 34542/50000: episode: 205, duration: 0.352s, episode steps: 110, steps per second: 312, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.236 [0.000, 2.000], mean observation: -0.259 [-1.200, 0.515], loss: 2.984577, mean_absolute_error: 33.814190, mean_q: -49.974277\n",
            " 34720/50000: episode: 206, duration: 0.568s, episode steps: 178, steps per second: 313, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.972 [0.000, 2.000], mean observation: -0.199 [-1.200, 0.503], loss: 3.726663, mean_absolute_error: 33.845154, mean_q: -49.976028\n",
            " 34860/50000: episode: 207, duration: 0.441s, episode steps: 140, steps per second: 317, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.136 [0.000, 2.000], mean observation: -0.253 [-1.200, 0.546], loss: 3.076315, mean_absolute_error: 33.674828, mean_q: -49.677197\n",
            " 35006/50000: episode: 208, duration: 0.475s, episode steps: 146, steps per second: 308, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.260 [-1.200, 0.531], loss: 3.842149, mean_absolute_error: 33.788548, mean_q: -49.863213\n",
            " 35158/50000: episode: 209, duration: 0.491s, episode steps: 152, steps per second: 309, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000], mean observation: -0.236 [-1.200, 0.527], loss: 2.628928, mean_absolute_error: 33.627575, mean_q: -49.685497\n",
            " 35267/50000: episode: 210, duration: 0.364s, episode steps: 109, steps per second: 299, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.239 [0.000, 2.000], mean observation: -0.258 [-1.137, 0.504], loss: 3.612667, mean_absolute_error: 33.683681, mean_q: -49.774578\n",
            " 35407/50000: episode: 211, duration: 0.447s, episode steps: 140, steps per second: 314, episode reward: -140.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.071 [0.000, 2.000], mean observation: -0.231 [-1.200, 0.522], loss: 3.714552, mean_absolute_error: 33.497593, mean_q: -49.434738\n",
            " 35563/50000: episode: 212, duration: 0.501s, episode steps: 156, steps per second: 312, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.846 [0.000, 2.000], mean observation: -0.251 [-1.200, 0.527], loss: 4.418799, mean_absolute_error: 33.448154, mean_q: -49.269337\n",
            " 35744/50000: episode: 213, duration: 0.584s, episode steps: 181, steps per second: 310, episode reward: -181.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.917 [0.000, 2.000], mean observation: -0.188 [-1.200, 0.515], loss: 3.715068, mean_absolute_error: 33.343987, mean_q: -49.192257\n",
            " 35897/50000: episode: 214, duration: 0.488s, episode steps: 153, steps per second: 314, episode reward: -153.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.908 [0.000, 2.000], mean observation: -0.247 [-1.200, 0.522], loss: 4.160304, mean_absolute_error: 33.019489, mean_q: -48.651310\n",
            " 36054/50000: episode: 215, duration: 0.497s, episode steps: 157, steps per second: 316, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.841 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.515], loss: 2.846592, mean_absolute_error: 33.223557, mean_q: -49.023464\n",
            " 36211/50000: episode: 216, duration: 0.504s, episode steps: 157, steps per second: 312, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.936 [0.000, 2.000], mean observation: -0.234 [-1.200, 0.515], loss: 1.831404, mean_absolute_error: 33.107380, mean_q: -48.943012\n",
            " 36354/50000: episode: 217, duration: 0.457s, episode steps: 143, steps per second: 313, episode reward: -143.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.881 [0.000, 2.000], mean observation: -0.256 [-1.200, 0.501], loss: 3.968745, mean_absolute_error: 33.231689, mean_q: -48.955551\n",
            " 36516/50000: episode: 218, duration: 0.524s, episode steps: 162, steps per second: 309, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.914 [0.000, 2.000], mean observation: -0.189 [-1.200, 0.521], loss: 4.472718, mean_absolute_error: 33.094013, mean_q: -48.711578\n",
            " 36654/50000: episode: 219, duration: 0.447s, episode steps: 138, steps per second: 308, episode reward: -138.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.964 [0.000, 2.000], mean observation: -0.248 [-1.200, 0.514], loss: 2.838238, mean_absolute_error: 32.849976, mean_q: -48.404423\n",
            " 36814/50000: episode: 220, duration: 0.529s, episode steps: 160, steps per second: 303, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000], mean observation: -0.236 [-1.200, 0.514], loss: 4.377647, mean_absolute_error: 32.888885, mean_q: -48.466770\n",
            " 36951/50000: episode: 221, duration: 0.444s, episode steps: 137, steps per second: 308, episode reward: -137.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.161 [0.000, 2.000], mean observation: -0.245 [-1.063, 0.509], loss: 3.541898, mean_absolute_error: 33.132160, mean_q: -48.782513\n",
            " 37110/50000: episode: 222, duration: 0.513s, episode steps: 159, steps per second: 310, episode reward: -159.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.881 [0.000, 2.000], mean observation: -0.245 [-1.200, 0.521], loss: 3.520310, mean_absolute_error: 33.242443, mean_q: -48.945248\n",
            " 37268/50000: episode: 223, duration: 0.509s, episode steps: 158, steps per second: 310, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.791 [0.000, 2.000], mean observation: -0.237 [-1.200, 0.514], loss: 3.130934, mean_absolute_error: 32.942013, mean_q: -48.493404\n",
            " 37387/50000: episode: 224, duration: 0.384s, episode steps: 119, steps per second: 310, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.378 [0.000, 2.000], mean observation: -0.244 [-1.139, 0.506], loss: 6.008804, mean_absolute_error: 32.417797, mean_q: -47.663250\n",
            " 37564/50000: episode: 225, duration: 0.571s, episode steps: 177, steps per second: 310, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000], mean observation: -0.279 [-1.107, 0.501], loss: 2.697307, mean_absolute_error: 32.677811, mean_q: -48.181068\n",
            " 37719/50000: episode: 226, duration: 0.495s, episode steps: 155, steps per second: 313, episode reward: -155.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.871 [0.000, 2.000], mean observation: -0.229 [-1.200, 0.514], loss: 3.844432, mean_absolute_error: 32.678490, mean_q: -48.114700\n",
            " 37886/50000: episode: 227, duration: 0.541s, episode steps: 167, steps per second: 309, episode reward: -167.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.114 [0.000, 2.000], mean observation: -0.265 [-1.149, 0.508], loss: 3.435918, mean_absolute_error: 32.461411, mean_q: -47.746994\n",
            " 38052/50000: episode: 228, duration: 0.566s, episode steps: 166, steps per second: 293, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.873 [0.000, 2.000], mean observation: -0.199 [-1.200, 0.519], loss: 3.776513, mean_absolute_error: 32.596336, mean_q: -47.918118\n",
            " 38164/50000: episode: 229, duration: 0.361s, episode steps: 112, steps per second: 311, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.241 [0.000, 2.000], mean observation: -0.257 [-1.200, 0.512], loss: 2.060761, mean_absolute_error: 32.606133, mean_q: -48.035217\n",
            " 38340/50000: episode: 230, duration: 0.553s, episode steps: 176, steps per second: 318, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000], mean observation: -0.228 [-1.200, 0.512], loss: 4.814023, mean_absolute_error: 32.402481, mean_q: -47.621632\n",
            " 38492/50000: episode: 231, duration: 0.473s, episode steps: 152, steps per second: 321, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.954 [0.000, 2.000], mean observation: -0.242 [-1.200, 0.514], loss: 3.132004, mean_absolute_error: 32.397339, mean_q: -47.687302\n",
            " 38650/50000: episode: 232, duration: 0.504s, episode steps: 158, steps per second: 313, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.918 [0.000, 2.000], mean observation: -0.196 [-1.200, 0.519], loss: 2.751069, mean_absolute_error: 32.535194, mean_q: -47.881508\n",
            " 38763/50000: episode: 233, duration: 0.357s, episode steps: 113, steps per second: 317, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.519], loss: 2.356311, mean_absolute_error: 32.397186, mean_q: -47.667896\n",
            " 38876/50000: episode: 234, duration: 0.378s, episode steps: 113, steps per second: 299, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000], mean observation: -0.258 [-1.143, 0.501], loss: 4.348361, mean_absolute_error: 32.699772, mean_q: -48.085770\n",
            " 39038/50000: episode: 235, duration: 0.516s, episode steps: 162, steps per second: 314, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.951 [0.000, 2.000], mean observation: -0.227 [-1.200, 0.521], loss: 3.091846, mean_absolute_error: 32.112427, mean_q: -47.163544\n",
            " 39195/50000: episode: 236, duration: 0.499s, episode steps: 157, steps per second: 315, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.962 [0.000, 2.000], mean observation: -0.194 [-1.200, 0.537], loss: 3.575019, mean_absolute_error: 32.335911, mean_q: -47.466537\n",
            " 39371/50000: episode: 237, duration: 0.553s, episode steps: 176, steps per second: 318, episode reward: -176.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.926 [0.000, 2.000], mean observation: -0.149 [-1.200, 0.521], loss: 3.367446, mean_absolute_error: 32.044621, mean_q: -47.117577\n",
            " 39483/50000: episode: 238, duration: 0.355s, episode steps: 112, steps per second: 316, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.366 [0.000, 2.000], mean observation: -0.236 [-1.086, 0.514], loss: 2.295081, mean_absolute_error: 32.478233, mean_q: -47.819160\n",
            " 39625/50000: episode: 239, duration: 0.452s, episode steps: 142, steps per second: 314, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.056 [0.000, 2.000], mean observation: -0.232 [-1.200, 0.521], loss: 2.084209, mean_absolute_error: 31.860916, mean_q: -46.887558\n",
            " 39712/50000: episode: 240, duration: 0.282s, episode steps: 87, steps per second: 309, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.126 [0.000, 2.000], mean observation: -0.201 [-0.926, 0.523], loss: 3.145422, mean_absolute_error: 31.778452, mean_q: -46.650352\n",
            " 39824/50000: episode: 241, duration: 0.377s, episode steps: 112, steps per second: 297, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000], mean observation: -0.225 [-0.994, 0.523], loss: 2.260668, mean_absolute_error: 31.664942, mean_q: -46.612148\n",
            " 39986/50000: episode: 242, duration: 0.520s, episode steps: 162, steps per second: 311, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.988 [0.000, 2.000], mean observation: -0.176 [-1.200, 0.501], loss: 2.039987, mean_absolute_error: 31.881721, mean_q: -46.992123\n",
            " 40135/50000: episode: 243, duration: 0.473s, episode steps: 149, steps per second: 315, episode reward: -149.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.087 [0.000, 2.000], mean observation: -0.227 [-1.200, 0.536], loss: 2.488873, mean_absolute_error: 31.775541, mean_q: -46.811707\n",
            " 40252/50000: episode: 244, duration: 0.390s, episode steps: 117, steps per second: 300, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.368 [0.000, 2.000], mean observation: -0.253 [-1.183, 0.502], loss: 2.247640, mean_absolute_error: 31.783648, mean_q: -46.828026\n",
            " 40370/50000: episode: 245, duration: 0.372s, episode steps: 118, steps per second: 317, episode reward: -118.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.398 [0.000, 2.000], mean observation: -0.241 [-1.200, 0.501], loss: 2.221992, mean_absolute_error: 31.709341, mean_q: -46.702644\n",
            " 40458/50000: episode: 246, duration: 0.282s, episode steps: 88, steps per second: 312, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000], mean observation: -0.215 [-0.943, 0.527], loss: 2.024463, mean_absolute_error: 31.856329, mean_q: -46.906494\n",
            " 40632/50000: episode: 247, duration: 0.550s, episode steps: 174, steps per second: 316, episode reward: -174.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.345 [0.000, 2.000], mean observation: -0.228 [-1.200, 0.502], loss: 3.732449, mean_absolute_error: 31.596476, mean_q: -46.379108\n",
            " 40744/50000: episode: 248, duration: 0.363s, episode steps: 112, steps per second: 308, episode reward: -112.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000], mean observation: -0.248 [-1.140, 0.531], loss: 2.888326, mean_absolute_error: 31.782309, mean_q: -46.814888\n",
            " 40902/50000: episode: 249, duration: 0.511s, episode steps: 158, steps per second: 309, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.848 [0.000, 2.000], mean observation: -0.261 [-1.200, 0.508], loss: 3.622428, mean_absolute_error: 31.668337, mean_q: -46.527790\n",
            " 41062/50000: episode: 250, duration: 0.499s, episode steps: 160, steps per second: 321, episode reward: -160.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000], mean observation: -0.197 [-1.200, 0.502], loss: 4.210946, mean_absolute_error: 32.072338, mean_q: -47.111504\n",
            " 41154/50000: episode: 251, duration: 0.299s, episode steps: 92, steps per second: 308, episode reward: -92.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000], mean observation: -0.169 [-0.884, 0.505], loss: 2.513225, mean_absolute_error: 31.049961, mean_q: -45.563965\n",
            " 41327/50000: episode: 252, duration: 0.544s, episode steps: 173, steps per second: 318, episode reward: -173.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.908 [0.000, 2.000], mean observation: -0.243 [-1.200, 0.515], loss: 2.611243, mean_absolute_error: 32.118385, mean_q: -47.337807\n",
            " 41493/50000: episode: 253, duration: 0.538s, episode steps: 166, steps per second: 309, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000], mean observation: -0.204 [-1.200, 0.536], loss: 1.795806, mean_absolute_error: 31.599638, mean_q: -46.572117\n",
            " 41623/50000: episode: 254, duration: 0.420s, episode steps: 130, steps per second: 309, episode reward: -130.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000], mean observation: -0.241 [-1.139, 0.515], loss: 1.155354, mean_absolute_error: 31.752310, mean_q: -46.847687\n",
            " 41740/50000: episode: 255, duration: 0.371s, episode steps: 117, steps per second: 315, episode reward: -117.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.453 [0.000, 2.000], mean observation: -0.235 [-1.123, 0.504], loss: 3.197751, mean_absolute_error: 31.822142, mean_q: -46.831722\n",
            " 41939/50000: episode: 256, duration: 0.670s, episode steps: 199, steps per second: 297, episode reward: -199.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000], mean observation: -0.233 [-1.200, 0.515], loss: 2.101402, mean_absolute_error: 31.785976, mean_q: -46.782722\n",
            " 42058/50000: episode: 257, duration: 0.417s, episode steps: 119, steps per second: 285, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.378 [0.000, 2.000], mean observation: -0.233 [-1.067, 0.504], loss: 1.970788, mean_absolute_error: 31.574072, mean_q: -46.484360\n",
            " 42236/50000: episode: 258, duration: 0.617s, episode steps: 178, steps per second: 289, episode reward: -178.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.944 [0.000, 2.000], mean observation: -0.225 [-1.200, 0.521], loss: 2.926332, mean_absolute_error: 31.848301, mean_q: -46.747730\n",
            " 42350/50000: episode: 259, duration: 0.398s, episode steps: 114, steps per second: 286, episode reward: -114.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.377 [0.000, 2.000], mean observation: -0.236 [-1.109, 0.531], loss: 1.103629, mean_absolute_error: 31.875051, mean_q: -46.950569\n",
            " 42463/50000: episode: 260, duration: 0.403s, episode steps: 113, steps per second: 280, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.327 [0.000, 2.000], mean observation: -0.241 [-1.104, 0.524], loss: 3.494107, mean_absolute_error: 31.645069, mean_q: -46.438549\n",
            " 42554/50000: episode: 261, duration: 0.322s, episode steps: 91, steps per second: 283, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.989 [0.000, 2.000], mean observation: -0.220 [-0.943, 0.508], loss: 2.625701, mean_absolute_error: 31.508604, mean_q: -46.290543\n",
            " 42710/50000: episode: 262, duration: 0.538s, episode steps: 156, steps per second: 290, episode reward: -156.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.019 [0.000, 2.000], mean observation: -0.210 [-1.200, 0.533], loss: 4.040304, mean_absolute_error: 31.906958, mean_q: -46.821857\n",
            " 42826/50000: episode: 263, duration: 0.404s, episode steps: 116, steps per second: 287, episode reward: -116.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000], mean observation: -0.240 [-1.107, 0.527], loss: 4.176396, mean_absolute_error: 31.162098, mean_q: -45.639893\n",
            " 43014/50000: episode: 264, duration: 0.643s, episode steps: 188, steps per second: 292, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.973 [0.000, 2.000], mean observation: -0.243 [-1.200, 0.515], loss: 2.781575, mean_absolute_error: 31.705500, mean_q: -46.578121\n",
            " 43129/50000: episode: 265, duration: 0.397s, episode steps: 115, steps per second: 290, episode reward: -115.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000], mean observation: -0.240 [-1.135, 0.508], loss: 1.907906, mean_absolute_error: 31.589312, mean_q: -46.411034\n",
            " 43216/50000: episode: 266, duration: 0.305s, episode steps: 87, steps per second: 285, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.057 [0.000, 2.000], mean observation: -0.216 [-0.936, 0.500], loss: 1.035912, mean_absolute_error: 31.560205, mean_q: -46.432854\n",
            " 43404/50000: episode: 267, duration: 0.644s, episode steps: 188, steps per second: 292, episode reward: -188.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000], mean observation: -0.178 [-1.200, 0.517], loss: 3.081988, mean_absolute_error: 31.747938, mean_q: -46.653522\n",
            " 43546/50000: episode: 268, duration: 0.489s, episode steps: 142, steps per second: 291, episode reward: -142.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.226 [-1.200, 0.541], loss: 4.387058, mean_absolute_error: 31.617229, mean_q: -46.302399\n",
            " 43735/50000: episode: 269, duration: 0.654s, episode steps: 189, steps per second: 289, episode reward: -189.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.153 [0.000, 2.000], mean observation: -0.169 [-1.200, 0.541], loss: 1.355479, mean_absolute_error: 31.333538, mean_q: -46.073277\n",
            " 43935/50000: episode: 270, duration: 0.683s, episode steps: 200, steps per second: 293, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000], mean observation: -0.240 [-1.200, 0.309], loss: 4.140331, mean_absolute_error: 31.489355, mean_q: -46.093407\n",
            " 44092/50000: episode: 271, duration: 0.544s, episode steps: 157, steps per second: 289, episode reward: -157.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000], mean observation: -0.232 [-1.200, 0.540], loss: 1.018432, mean_absolute_error: 31.729498, mean_q: -46.695625\n",
            " 44214/50000: episode: 272, duration: 0.421s, episode steps: 122, steps per second: 290, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.238 [0.000, 2.000], mean observation: -0.125 [-0.868, 0.507], loss: 1.313811, mean_absolute_error: 31.437647, mean_q: -46.273445\n",
            " 44319/50000: episode: 273, duration: 0.361s, episode steps: 105, steps per second: 291, episode reward: -105.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.143 [0.000, 2.000], mean observation: -0.154 [-0.875, 0.513], loss: 1.528524, mean_absolute_error: 31.690483, mean_q: -46.589573\n",
            " 44430/50000: episode: 274, duration: 0.399s, episode steps: 111, steps per second: 278, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.369 [0.000, 2.000], mean observation: -0.220 [-1.010, 0.524], loss: 1.317814, mean_absolute_error: 31.770359, mean_q: -46.728794\n",
            " 44555/50000: episode: 275, duration: 0.434s, episode steps: 125, steps per second: 288, episode reward: -125.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.352 [0.000, 2.000], mean observation: -0.245 [-1.144, 0.515], loss: 3.059639, mean_absolute_error: 31.819698, mean_q: -46.733006\n",
            " 44666/50000: episode: 276, duration: 0.383s, episode steps: 111, steps per second: 290, episode reward: -111.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.162 [0.000, 2.000], mean observation: -0.170 [-0.882, 0.501], loss: 2.710066, mean_absolute_error: 31.700989, mean_q: -46.505020\n",
            " 44756/50000: episode: 277, duration: 0.315s, episode steps: 90, steps per second: 286, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.222 [0.000, 2.000], mean observation: -0.173 [-0.891, 0.508], loss: 2.575949, mean_absolute_error: 31.912590, mean_q: -46.829437\n",
            " 44869/50000: episode: 278, duration: 0.362s, episode steps: 113, steps per second: 312, episode reward: -113.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.425 [0.000, 2.000], mean observation: -0.236 [-1.117, 0.508], loss: 1.368023, mean_absolute_error: 31.455357, mean_q: -46.216511\n",
            " 45069/50000: episode: 279, duration: 0.639s, episode steps: 200, steps per second: 313, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000], mean observation: -0.140 [-1.200, 0.353], loss: 1.856385, mean_absolute_error: 31.684820, mean_q: -46.515682\n",
            " 45227/50000: episode: 280, duration: 0.516s, episode steps: 158, steps per second: 306, episode reward: -158.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000], mean observation: -0.234 [-1.200, 0.515], loss: 1.756870, mean_absolute_error: 31.623455, mean_q: -46.443352\n",
            " 45404/50000: episode: 281, duration: 0.567s, episode steps: 177, steps per second: 312, episode reward: -177.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.158 [0.000, 2.000], mean observation: -0.166 [-1.200, 0.528], loss: 4.067710, mean_absolute_error: 31.590797, mean_q: -46.270325\n",
            " 45570/50000: episode: 282, duration: 0.537s, episode steps: 166, steps per second: 309, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.916 [0.000, 2.000], mean observation: -0.256 [-1.200, 0.524], loss: 1.907044, mean_absolute_error: 31.675125, mean_q: -46.563290\n",
            " 45664/50000: episode: 283, duration: 0.305s, episode steps: 94, steps per second: 309, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.188 [-0.903, 0.519], loss: 4.841549, mean_absolute_error: 31.838642, mean_q: -46.488651\n",
            " 45758/50000: episode: 284, duration: 0.299s, episode steps: 94, steps per second: 314, episode reward: -94.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.106 [0.000, 2.000], mean observation: -0.187 [-0.895, 0.512], loss: 3.350565, mean_absolute_error: 31.281109, mean_q: -45.795467\n",
            " 45958/50000: episode: 285, duration: 0.643s, episode steps: 200, steps per second: 311, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.010 [0.000, 2.000], mean observation: -0.334 [-0.742, 0.007], loss: 2.524649, mean_absolute_error: 31.456127, mean_q: -46.151211\n",
            " 46122/50000: episode: 286, duration: 0.526s, episode steps: 164, steps per second: 312, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.024 [0.000, 2.000], mean observation: -0.232 [-1.200, 0.522], loss: 2.776780, mean_absolute_error: 31.309105, mean_q: -45.890591\n",
            " 46231/50000: episode: 287, duration: 0.361s, episode steps: 109, steps per second: 302, episode reward: -109.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.385 [0.000, 2.000], mean observation: -0.234 [-1.089, 0.533], loss: 1.680235, mean_absolute_error: 31.370378, mean_q: -46.095940\n",
            " 46317/50000: episode: 288, duration: 0.280s, episode steps: 86, steps per second: 307, episode reward: -86.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.256 [0.000, 2.000], mean observation: -0.178 [-0.904, 0.510], loss: 3.441637, mean_absolute_error: 31.190294, mean_q: -45.734215\n",
            " 46436/50000: episode: 289, duration: 0.379s, episode steps: 119, steps per second: 314, episode reward: -119.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.294 [0.000, 2.000], mean observation: -0.108 [-0.855, 0.507], loss: 2.154905, mean_absolute_error: 31.170303, mean_q: -45.750271\n",
            " 46620/50000: episode: 290, duration: 0.591s, episode steps: 184, steps per second: 311, episode reward: -184.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.886 [0.000, 2.000], mean observation: -0.246 [-1.200, 0.529], loss: 3.013028, mean_absolute_error: 31.369406, mean_q: -46.034367\n",
            " 46802/50000: episode: 291, duration: 0.584s, episode steps: 182, steps per second: 312, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.984 [0.000, 2.000], mean observation: -0.232 [-1.200, 0.526], loss: 2.750170, mean_absolute_error: 31.133497, mean_q: -45.691170\n",
            " 46967/50000: episode: 292, duration: 0.536s, episode steps: 165, steps per second: 308, episode reward: -165.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.958 [0.000, 2.000], mean observation: -0.244 [-1.200, 0.542], loss: 2.852137, mean_absolute_error: 31.321047, mean_q: -46.002800\n",
            " 47098/50000: episode: 293, duration: 0.418s, episode steps: 131, steps per second: 313, episode reward: -131.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.229 [0.000, 2.000], mean observation: -0.177 [-0.872, 0.510], loss: 1.073120, mean_absolute_error: 31.233532, mean_q: -46.004955\n",
            " 47189/50000: episode: 294, duration: 0.302s, episode steps: 91, steps per second: 302, episode reward: -91.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.308 [0.000, 2.000], mean observation: -0.162 [-0.884, 0.503], loss: 0.887989, mean_absolute_error: 31.103748, mean_q: -45.796726\n",
            " 47276/50000: episode: 295, duration: 0.288s, episode steps: 87, steps per second: 302, episode reward: -87.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.264 [0.000, 2.000], mean observation: -0.174 [-0.896, 0.505], loss: 3.072301, mean_absolute_error: 31.485588, mean_q: -46.303673\n",
            " 47372/50000: episode: 296, duration: 0.309s, episode steps: 96, steps per second: 311, episode reward: -96.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.104 [0.000, 2.000], mean observation: -0.183 [-0.892, 0.515], loss: 3.233364, mean_absolute_error: 31.556028, mean_q: -46.334339\n",
            " 47572/50000: episode: 297, duration: 0.644s, episode steps: 200, steps per second: 310, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.745 [0.000, 2.000], mean observation: -0.246 [-0.870, 0.431], loss: 2.879961, mean_absolute_error: 31.409834, mean_q: -46.184425\n",
            " 47660/50000: episode: 298, duration: 0.291s, episode steps: 88, steps per second: 302, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000], mean observation: -0.165 [-0.891, 0.513], loss: 3.203953, mean_absolute_error: 31.316240, mean_q: -46.017162\n",
            " 47744/50000: episode: 299, duration: 0.274s, episode steps: 84, steps per second: 306, episode reward: -84.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.286 [0.000, 2.000], mean observation: -0.180 [-0.910, 0.509], loss: 2.441102, mean_absolute_error: 31.162064, mean_q: -45.840862\n",
            " 47842/50000: episode: 300, duration: 0.316s, episode steps: 98, steps per second: 310, episode reward: -98.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.306 [0.000, 2.000], mean observation: -0.139 [-0.865, 0.506], loss: 2.110668, mean_absolute_error: 31.081217, mean_q: -45.722801\n",
            " 48013/50000: episode: 301, duration: 0.539s, episode steps: 171, steps per second: 317, episode reward: -171.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.129 [0.000, 2.000], mean observation: -0.201 [-1.200, 0.521], loss: 2.015045, mean_absolute_error: 31.161823, mean_q: -45.887337\n",
            " 48192/50000: episode: 302, duration: 0.572s, episode steps: 179, steps per second: 313, episode reward: -179.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000], mean observation: -0.190 [-1.200, 0.537], loss: 3.453113, mean_absolute_error: 31.429079, mean_q: -46.180660\n",
            " 48356/50000: episode: 303, duration: 0.527s, episode steps: 164, steps per second: 311, episode reward: -164.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.043 [0.000, 2.000], mean observation: -0.220 [-1.200, 0.519], loss: 3.608327, mean_absolute_error: 31.155226, mean_q: -45.749573\n",
            " 48538/50000: episode: 304, duration: 0.577s, episode steps: 182, steps per second: 316, episode reward: -182.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.137 [0.000, 2.000], mean observation: -0.170 [-1.200, 0.514], loss: 1.911887, mean_absolute_error: 31.120686, mean_q: -45.860722\n",
            " 48626/50000: episode: 305, duration: 0.291s, episode steps: 88, steps per second: 303, episode reward: -88.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000], mean observation: -0.165 [-0.891, 0.514], loss: 1.570686, mean_absolute_error: 31.367882, mean_q: -46.220791\n",
            " 48715/50000: episode: 306, duration: 0.297s, episode steps: 89, steps per second: 299, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.348 [0.000, 2.000], mean observation: -0.153 [-0.883, 0.513], loss: 3.976398, mean_absolute_error: 31.017567, mean_q: -45.623219\n",
            " 48825/50000: episode: 307, duration: 0.353s, episode steps: 110, steps per second: 311, episode reward: -110.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.455 [0.000, 2.000], mean observation: -0.091 [-0.852, 0.503], loss: 1.469236, mean_absolute_error: 31.063599, mean_q: -45.801651\n",
            " 48947/50000: episode: 308, duration: 0.402s, episode steps: 122, steps per second: 304, episode reward: -122.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.213 [0.000, 2.000], mean observation: -0.187 [-0.884, 0.510], loss: 4.204261, mean_absolute_error: 30.927235, mean_q: -45.524403\n",
            " 49113/50000: episode: 309, duration: 0.543s, episode steps: 166, steps per second: 306, episode reward: -166.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.084 [0.000, 2.000], mean observation: -0.214 [-1.200, 0.520], loss: 2.566926, mean_absolute_error: 31.008198, mean_q: -45.702862\n",
            " 49203/50000: episode: 310, duration: 0.298s, episode steps: 90, steps per second: 302, episode reward: -90.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.289 [0.000, 2.000], mean observation: -0.161 [-0.882, 0.504], loss: 1.804812, mean_absolute_error: 30.906233, mean_q: -45.545910\n",
            " 49378/50000: episode: 311, duration: 0.559s, episode steps: 175, steps per second: 313, episode reward: -175.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.097 [0.000, 2.000], mean observation: -0.238 [-1.200, 0.535], loss: 2.407866, mean_absolute_error: 31.079861, mean_q: -45.808174\n",
            " 49524/50000: episode: 312, duration: 0.463s, episode steps: 146, steps per second: 315, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.397 [0.000, 2.000], mean observation: -0.233 [-1.102, 0.522], loss: 2.561343, mean_absolute_error: 30.913006, mean_q: -45.570210\n",
            " 49613/50000: episode: 313, duration: 0.296s, episode steps: 89, steps per second: 300, episode reward: -89.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.337 [0.000, 2.000], mean observation: -0.158 [-0.885, 0.510], loss: 2.465738, mean_absolute_error: 31.262903, mean_q: -46.052135\n",
            " 49765/50000: episode: 314, duration: 0.481s, episode steps: 152, steps per second: 316, episode reward: -152.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.217 [0.000, 2.000], mean observation: -0.232 [-1.200, 0.527], loss: 1.998652, mean_absolute_error: 31.106253, mean_q: -45.822472\n",
            " 49911/50000: episode: 315, duration: 0.475s, episode steps: 146, steps per second: 307, episode reward: -146.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.349 [0.000, 2.000], mean observation: -0.135 [-0.853, 0.503], loss: 2.311051, mean_absolute_error: 30.815247, mean_q: -45.361877\n",
            "done, took 162.445 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "46yjN9D0mhiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 実行"
      ]
    },
    {
      "metadata": {
        "id": "h9Ur8kWrOpv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d3248012-2bc7-4583-87bd-681bb7aa338f"
      },
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes=1, visualize=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 1 episodes ...\n",
            "Episode 1: reward: -84.000, steps: 84\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0cac61f7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "w7tqxBNlSlsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "90f18976-0eb4-44cd-a7b8-c1dd9b5835a2"
      },
      "cell_type": "code",
      "source": [
        "keras.callbacks.History()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d0b00a7e8c9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0x7f0cac61fef0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "QTavTeUHwZdG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bMgPDkNsw7ju",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 参考\n",
        "色々役に立ちそうなkerasのcallback[https://keras.io/ja/callbacks/]"
      ]
    },
    {
      "metadata": {
        "id": "v0l5nywYw9SZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}